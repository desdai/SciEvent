{
  "papers": [
    {
      "paper_code": "ACL_23_P_162",
      "abstract": "Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm. In this work, we propose a novel fine-tuning method DEER, which can make a single pre-trained model support Dynamic and Efficient infERence and achieve an adaptive trade-off between model performance and latency. In particular, our critical insight is to jointly utilize the non-autoregressive (NAR) generation and dynamic parameter pruning techniques, which can flexibly control the decoding iteration steps and model sizes according to memory and latency limitations. Besides, we also explore the effectiveness of the pre-trained MLMs (i.e., the BERT family) for text generation tasks since their bidirectional attention nature is more suitable for the NAR training objective. Extensive experiments on both monolingual and multilingual pre-trained MLMs demonstrate the effectiveness of our proposed DEER method by consistently achieving (1) higher BLEU scores than the strong autoregressive Transformer model on three neural machine translation tasks with 3 → 12 times speedup, (2) competitive performance (but with much faster inference speed) compared with the BART model on four GLGE benchmark tasks.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm.",
          "Main Action": "suffer from inefficient inference",
          "Arguments": {
            "Agent": [
              "Pre-trained Language Models"
            ],
            "Object": {
              "Primary Object": [
                "large-scale parameters",
                "universal autoregressive decoding paradigm"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Due to their large-scale parameters and the universal autoregressive decoding paradigm."
            ],
            "Purpose": [
              "Improving efficiency, reducing resource usage, and making model deployment more feasible."
            ],
            "Method": [
              "Optimizing computational requirements through efficient algorithms or alternative architectures.",
              "Rethinking the decoding process away from universality."
            ],
            "Results": [
              "Current methods lead to inefficiency, serving as a significant barrier."
            ],
            "Analysis": [
              "Recognizing this inefficiency as a critical limitation impacting practicality."
            ],
            "Challenge": [
              "The size of the models and the decoding mechanism introduce complexity."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Developing more efficient models is crucial for real-world applications.",
              "Enhancing practicality and scalability."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we propose a novel fine-tuning method DEER, which can make a single pre-trained model support Dynamic and Efficient infERence and achieve an adaptive trade-off between model performance and latency. In particular, our critical insight is to jointly utilize the non-autoregressive (NAR) generation and dynamic parameter pruning techniques, which can flexibly control the decoding iteration steps and model sizes according to memory and latency limitations. Besides, we also explore the effectiveness of the pre-trained MLMs (i.e., the BERT family) for text generation tasks since their bidirectional attention nature is more suitable for the NAR training objective.",
          "Main Action": "Propose a novel fine-tuning method",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "DEER"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Improving efficiency and adaptability in models",
              "Filling gaps in current methods"
            ],
            "Purpose": [
              "Providing a better balance and flexibility between model performance and latency"
            ],
            "Method": [
              "Jointly utilizing Non-Autoregressive (NAR) generation",
              "Dynamic parameter pruning techniques",
              "Testing the effectiveness of pre-trained Masked Language Models (MLMs)",
              "Adjusting decoding iteration steps and model sizes"
            ],
            "Results": [
              "Achieving an adaptive trade-off between performance and latency"
            ],
            "Analysis": [
              "Overcoming computational limits",
              "Insights into model design"
            ],
            "Challenge": [
              "Handling varying memory and latency limitations"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Future applications and research directions"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Extensive experiments on both monolingual and multilingual pre-trained MLMs demonstrate the effectiveness of our proposed DEER method by consistently achieving (1) higher BLEU scores than the strong autoregressive Transformer model on three neural machine translation tasks with 3 → 12 times speedup, (2) competitive performance (but with much faster inference speed) compared with the BART model on four GLGE benchmark tasks.",
          "Main Action": "demonstrate the effectiveness",
          "Arguments": {
            "Agent": [
              "extensive experiments"
            ],
            "Object": {
              "Primary Object": [
                "our proposed DEER method"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Extensive experiments on both monolingual and multilingual pre-trained MLMs"
            ],
            "Purpose": [
              "To evaluate the effectiveness of DEER against other models"
            ],
            "Method": [
              "using these pre-trained models and comparing against others"
            ],
            "Results": [
              "consistently achieving (1) higher BLEU scores",
              "(2) competitive performance"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Broader application potential in machine translation"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_82",
      "abstract": "Recent abstractive conversation summarization systems generally rely on large-scale datasets with annotated summaries. However, collecting and annotating these conversations can be a time-consuming and labor-intensive task. To address this issue, in this work, we present a sub-structure level compositional data augmentation method, Compo, for generating diverse and high-quality pairs of conversations and summaries. Specifically, Compo first extracts conversation structures like topic splits and action triples as basic units. Then we organize these semantically meaningful conversation snippets compositionally to create new training instances. Additionally, we explore noise-tolerant settings in both self-training and joint-training paradigms to make the most of these augmented samples. Our experiments on benchmark datasets, SAMSum and DialogSum, show that Compo substantially outperforms prior baseline methods by achieving a nearly 10% increase of ROUGE scores with limited data.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Recent abstractive conversation summarization systems generally rely on large-scale datasets with annotated summaries. However, collecting and annotating these conversations can be a time-consuming and labor-intensive task.",
          "Main Action": "Collecting and annotating these conversations",
          "Arguments": {
            "Agent": [
              "Human workers"
            ],
            "Object": {
              "Primary Object": [
                "These conversations"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To address this issue, in this work, we present a sub-structure level compositional data augmentation method, Compo, for generating diverse and high-quality pairs of conversations and summaries. Specifically, Compo first extracts conversation structures like topic splits and action triples as basic units. Then we organize these semantically meaningful conversation snippets compositionally to create new training instances. Additionally, we explore noise-tolerant settings in both self-training and joint-training paradigms to make the most of these augmented samples.",
          "Main Action": "present",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "Compo"
              ],
              "Secondary Object": [
                "diverse and high-quality pairs of conversations and summaries"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "generating diverse and high-quality pairs of conversations and summaries"
            ],
            "Method": [
              "extracting conversation structures like topic splits and action triples as basic units",
              "organizing these semantically meaningful conversation snippets compositionally to create new training instances"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "ERROR",
          "Text": "Our experiments on benchmark datasets, SAMSum and DialogSum, show that Compo substantially outperforms prior baseline methods by achieving a nearly 10% increase of ROUGE scores with limited data.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "RECONSTRUCTION_ERROR"
        }
      ]
    }
  ]
}