{
  "papers": [
    {
      "paper_code": "ACL_23_P_664",
      "abstract": "Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, MultiCapCLIP only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, MultiCapCLIP instead takes visual data as input directly to retrieve the concept prompts to generate the final visual descriptions. The extensive experiments on image and video captioning across four benchmarks and four languages (i.e., English, Chinese, German, and French) confirm the effectiveness of our approach. Compared with state-of-the-art zero-shot and weakly-supervised methods, our method achieves 4.8% and 21.5% absolute improvements in terms of BLEU@4 and CIDEr metrics.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available.",
          "Main Action": "typically require",
          "Arguments": {
            "Agent": [
              "supervised visual captioning models"
            ],
            "Object": {
              "Primary Object": [
                "images or videos paired with descriptions in a specific language"
              ],
              "Secondary Object": [
                "many scenarios and languages"
              ]
            },
            "Context": [
              "collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages"
            ],
            "Purpose": [
              "sufficient labeled pairs are usually not available"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "time-consuming and expensive for many scenarios and languages"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, MultiCapCLIP only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, MultiCapCLIP instead takes visual data as input directly to retrieve the concept prompts to generate the final visual descriptions.",
          "Main Action": "present",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "MultiCapCLIP"
              ],
              "Secondary Object": [
                "label shortage problem"
              ]
            },
            "Context": [
              "deal with the label shortage problem"
            ],
            "Purpose": [
              "eliminate reliance on labeled vision_caption pairs of downstream datasets"
            ],
            "Method": [
              "training stage: retrieving concept prompts and auto_encoding to learn writing styles",
              "testing stage: taking visual data to retrieve concept prompts"
            ],
            "Results": [
              "generate visual captions without labeled vision_caption pairs"
            ],
            "Analysis": [
              "preserve domain knowledge and adapt writing styles"
            ],
            "Challenge": [
              "only require text data for training"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "open up cross_modal understanding without reliant on labeled data"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The extensive experiments on image and video captioning across four benchmarks and four languages (i.e., English, Chinese, German, and French) confirm the effectiveness of our approach. Compared with state-of-the-art zero-shot and weakly-supervised methods, our method achieves 4.8% and 21.5% absolute improvements in terms of BLEU@4 and CIDEr metrics.",
          "Main Action": "Confirm",
          "Arguments": {
            "Agent": [
              "Our approach"
            ],
            "Object": {
              "Primary Object": [
                "Effectiveness of our approach"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "To evaluate the performance of our approach compared to others"
            ],
            "Method": [
              "BLEU@4",
              "CIDEr"
            ],
            "Results": [
              "4.8%",
              "21.5%"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Improving cross-language captioning systems"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_605",
      "abstract": "Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an FDISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our FDISTILL methods. We further derive step-wise decomposition for our FDISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models.",
          "Main Action": "transferring knowledge",
          "Arguments": {
            "Agent": [
              "a large model"
            ],
            "Object": {
              "Primary Object": [
                "a small model"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "natural language processing community",
              "ever-growing language models"
            ],
            "Purpose": [
              "compressing ever-growing language models"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "gaining increasing attention",
              "potential for future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we propose an FDISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our FDISTILL methods. We further derive step-wise decomposition for our FDISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner.",
          "Main Action": "Proposing",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "FDISTILL framework",
                "Generalized f-divergence function"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Knowledge distillation"
            ],
            "Purpose": [
              "Creating an effective framework for sequence-level knowledge distillation"
            ],
            "Method": [
              "Formulating sequence-level knowledge distillation as minimizing a generalized f-divergence function",
              "Proposing four distilling variants under our framework",
              "Deriving step-wise decomposition for our FDISTILL"
            ],
            "Results": [
              "Existing SeqKD and ENGINE approaches are approximations of our FDISTILL methods"
            ],
            "Analysis": [
              "This implies that existing methods do not fully capture the complexity of sequence-level divergence"
            ],
            "Challenge": [
              "Reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Our framework provides a more comprehensive approach to sequence-level knowledge distillation"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.",
          "Main Action": "our methods outperform existing KD approaches",
          "Arguments": {
            "Agent": [
              "us"
            ],
            "Object": {
              "Primary Object": [
                "existing KD approaches"
              ],
              "Secondary Object": [
                "student"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}