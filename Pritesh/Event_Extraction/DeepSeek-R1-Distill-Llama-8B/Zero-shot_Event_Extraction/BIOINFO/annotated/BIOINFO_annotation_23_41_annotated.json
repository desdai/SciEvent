{
  "papers": [
    {
      "paper_code": "bioinfo_23_P_742",
      "abstract": "There is a growing number of available protein sequences, but only a limited amount has been manually annotated. For example, only 0.25% of all entries of UniProtKB are reviewed by human annotators. Further developing automatic tools to infer protein function from sequence alone can alleviate part of this gap. In this article, we investigate the potential of Transformer deep neural networks on a specific case of functional sequence annotation: the prediction of enzymatic classes. We show that our EnzBert transformer models, trained to predict Enzyme Commission (EC) numbers by specialization of a protein language model, outperforms state-of-the-art tools for monofunctional enzyme class prediction based on sequences only. Accuracy is improved from 84% to 95% on the prediction of EC numbers at level two on the EC40 benchmark. To evaluate the prediction quality at level four, the most detailed level of EC numbers, we built two new time-based benchmarks for comparison with state-of-the-art methods ECPred and DeepEC: the macro-F1 score is respectively improved from 41% to 54% and from 20% to 26%. Finally, we also show that using a simple combination of attention maps is on par with, or better than, other classical interpretability methods on the EC prediction task. More specifically, important residues identified by attention maps tend to correspond to known catalytic sites. Quantitatively, we report a max F-Gain score of 96.05%, while classical interpretability methods reach 91.44% at best.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "There is a growing number of available protein sequences, but only a limited amount has been manually annotated. For example, only 0.25% of all entries of UniProtKB are reviewed by human annotators. Further developing automatic tools to infer protein function from sequence alone can alleviate part of this gap.",
          "Main Action": "Further developing",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "proteins"
              ],
              "Secondary Object": [
                "UniProtKB"
              ]
            },
            "Context": [
              "a growing number of available protein sequences, but only a limited amount has been manually annotated"
            ],
            "Purpose": [
              "alleviate part of this gap"
            ],
            "Method": [
              "automatic tools to infer protein function from sequence alone"
            ],
            "Results": [
              "reducing reliance on manual annotation"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "currently, only 0.25% are annotated",
              "complexity of protein function inference from sequences alone"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader impact on biological knowledge curation and access"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this article, we investigate the potential of Transformer deep neural networks on a specific case of functional sequence annotation: the prediction of enzymatic classes. We show that our EnzBert transformer models, trained to predict Enzyme Commission (EC) numbers by specialization of a protein language model, outperforms state-of-the-art tools for monofunctional enzyme class prediction based on sequences only.",
          "Main Action": "investigate",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "enzymatic classes"
              ],
              "Secondary Object": [
                "EnzBert transformer models"
              ]
            },
            "Context": [
              "functional sequence annotation"
            ],
            "Purpose": [
              "show that our EnzBert transformer models..."
            ],
            "Method": [
              "Transformer deep neural networks",
              "trained to predict Enzyme Commission (EC) numbers"
            ],
            "Results": [
              "outperforms state-of-the-art tools"
            ],
            "Analysis": [
              "improvements over existing tools"
            ],
            "Challenge": [
              "complexity of enzymatic classification",
              "data limitations"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "advancements in predictive modeling for biological functions"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Accuracy is improved from 84% to 95% on the prediction of EC numbers at level two on the EC40 benchmark. To evaluate the prediction quality at level four, the most detailed level of EC numbers, we built two new time-based benchmarks for comparison with state-of-the-art methods ECPred and DeepEC: the macro-F1 score is respectively improved from 41% to 54% and from 20% to 26%. Finally, we also show that using a simple combination of attention maps is on par with, or better than, other classical interpretability methods on the EC prediction task. More specifically, important residues identified by attention maps tend to correspond to known catalytic sites. Quantitatively, we report a max F-Gain score of 96.05%, while classical interpretability methods reach 91.44% at best.",
          "Main Action": "To evaluate the prediction quality at level four",
          "Arguments": {
            "Agent": [
              "Researchers"
            ],
            "Object": {
              "Primary Object": [
                "EC numbers at level two on the EC40 benchmark"
              ],
              "Secondary Object": [
                "EC numbers at level four"
              ]
            },
            "Context": [
              "We built two new time-based benchmarks for comparison with state-of-the-art methods ECPred and DeepEC"
            ],
            "Purpose": [
              "To assess the ability of our model to predict EC numbers at a deeper level"
            ],
            "Method": [
              "Building new benchmarks",
              "Using attention maps",
              "Comparing with state-of-the-art methods"
            ],
            "Results": [
              "The macro-F1 score is respectively improved from 41% to 54%",
              "From 20% to 26%"
            ],
            "Analysis": [
              "Important residues identified by attention maps tend to correspond to known catalytic sites"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Simpler combinations of attention maps can achieve comparable or better results than classical interpretability methods"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "bioinfo_23_P_277",
      "abstract": "Sequence alignment is a memory bound computation whose performance in modern systems is limited by the memory bandwidth bottleneck. Processing-in-memory (PIM) architectures alleviate this bottleneck by providing the memory with computing competencies. We propose Alignment-in-Memory (AIM), a framework for high-throughput sequence alignment using PIM, and evaluate it on UPMEM, the first publicly available general-purpose programmable PIM system. Our evaluation shows that a real PIM system can substantially outperform server-grade multi-threaded CPU systems running at full-scale when performing sequence alignment for a variety of algorithms, read lengths, and edit distance thresholds. We hope that our findings inspire more work on creating and accelerating bioinformatics algorithms for such real PIM systems.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Sequence alignment is a memory bound computation whose performance in modern systems is limited by the memory bandwidth bottleneck. Processing-in-memory (PIM) architectures alleviate this bottleneck by providing the memory with computing competencies.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We propose Alignment-in-Memory (AIM), a framework for high-throughput sequence alignment using PIM, and evaluate it on UPMEM, the first publicly available general-purpose programmable PIM system",
          "Main Action": "We propose Alignment-in-Memories (AIM)",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Alignment-in-Memories (AIM)"
              ],
              "Secondary Object": [
                "PIM"
              ]
            },
            "Context": [
              "UPMEM, the first publicly available general-purpose programmable PIM system"
            ],
            "Purpose": [
              "to develop a high-throughput sequence alignment framework"
            ],
            "Method": [
              "using PIM"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "for bioinformatics and computational biology"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our evaluation shows that a real PIM system can substantially outperform server-grade multi-threaded CPU systems running at full-scale when performing sequence alignment for a variety of algorithms, read lengths, and edit distance thresholds.",
          "Main Action": "outperforms",
          "Arguments": {
            "Agent": [
              "a real PIM system"
            ],
            "Object": {
              "Primary Object": [
                "server-grade multi-threaded CPU systems"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "to evaluate the performance differences"
            ],
            "Method": [
              "comparing performance between a real PIM system and server-grade multi-threaded CPU systems"
            ],
            "Results": [
              "that a real PIM system can substantially outperform server-grade multi-threaded CPU systems"
            ],
            "Analysis": [
              "This suggests that real PIM systems offer significant advantages over traditional server-grade systems"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "This finding highlights the potential for more efficient computational solutions in various applications"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "We hope that our findings inspire more work on creating and accelerating bioinformatics algorithms for such real PIM systems.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}