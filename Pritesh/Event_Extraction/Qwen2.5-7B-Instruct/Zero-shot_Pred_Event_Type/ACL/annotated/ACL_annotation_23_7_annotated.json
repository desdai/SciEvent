{
  "papers": [
    {
      "paper_code": "ACL_23_P_398",
      "abstract": "Many NLP pipelines split text into sentences as one of the crucial preprocessing steps. Prior sentence segmentation tools either rely on punctuation or require a considerable amount of sentence-segmented training data: both central assumptions might fail when porting sentence segmenters to diverse languages on a massive scale. In this work, we thus introduce a multilingual punctuation-agnostic sentence segmentation method, currently covering 85 languages, trained in a self-supervised fashion on unsegmented text, by making use of newline characters which implicitly perform segmentation into paragraphs. We further propose an approach that adapts our method to the segmentation in a given corpus by using only a small number (64-256) of sentence-segmented examples. The main results indicate that our method outperforms all the prior best sentence-segmentation tools by an average of 6.1% F1 points. Furthermore, we demonstrate that proper sentence segmentation has a point: the use of a (powerful) sentence segmenter makes a considerable difference for a downstream application such as machine translation (MT). By using our method to match sentence segmentation to the segmentation used during training of MT models, we achieve an average improvement of 2.3 BLEU points over the best prior segmentation tool, as well as massive gains over a trivial segmenter that splits text into equally-sized blocks.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Many NLP pipelines split text into sentences as one of the crucial preprocessing steps. Prior sentence segmentation tools either rely on punctuation or require a considerable amount of sentence-segmented training data: both central assumptions might fail when porting sentence segmenters to diverse languages on a massive scale.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "NLP pipelines"
            ],
            "Object": {
              "Primary Object": [
                "text"
              ],
              "Secondary Object": [
                "sentences"
              ]
            },
            "Context": [
              "One of the crucial preprocessing steps",
              "Prior sentence segmentation tools either rely on punctuation or require a considerable amount of sentence-segmented training data"
            ],
            "Purpose": [
              "None mentioned directly but implied through discussion of challenges"
            ],
            "Method": [
              "splitting text into sentences"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "central assumptions might fail when porting sentence segmenters to diverse languages on a massive scale"
            ],
            "Challenge": [
              "Both central assumptions might fail when porting sentence segmenters to diverse languages on a massive scale"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we thus introduce a multilingual punctuation-agnostic sentence segmentation method, currently covering 85 languages, trained in a self-supervised fashion on unsegmented text, by making use of newline characters which implicitly perform segmentation into paragraphs. We further propose an approach that adapts our method to the segmentation in a given corpus by using only a small number (64-256) of sentence-segmented examples.",
          "Main Action": "introduce",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a multilingual punctuation-agnostic sentence segmentation method"
              ],
              "Secondary Object": [
                "85 languages"
              ]
            },
            "Context": [
              "currently covering 85 languages, trained in a self-supervised fashion on unsegmented text"
            ],
            "Purpose": [
              "to adapt our method to the segmentation in a given corpus by using only a small number (64-256) of sentence-segmented examples"
            ],
            "Method": [
              "trained in a self-supervised fashion on unsegmented text, by making use of newline characters which implicitly perform segmentation into paragraphs"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The main results indicate that our method outperforms all the prior best sentence-segmentation tools by an average of 6.1% F1 points. Furthermore, we demonstrate that proper sentence segmentation has a point: the use of a (powerful) sentence segmenter makes a considerable difference for a downstream application such as machine translation (MT). By using our method to match sentence segmentation to the segmentation used during training of MT models, we achieve an average improvement of 2.3 BLEU points over the best prior segmentation tool, as well as massive gains over a trivial segmenter that splits text into equally-sized blocks.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "our method"
            ],
            "Object": {
              "Primary Object": [
                "all the prior best sentence-segmentation tools",
                "a downstream application such as machine translation (MT)",
                "trivial segmenter"
              ],
              "Secondary Object": [
                "an average of 6.1% F1 points",
                "average improvement of 2.3 BLEU points"
              ]
            },
            "Context": [
              "By using our method to match sentence segmentation to the segmentation used during training of MT models"
            ],
            "Purpose": [
              "to make a considerable difference for a downstream application such as machine translation (MT)",
              "to achieve an average improvement of 2.3 BLEU points over the best prior segmentation tool, as well as massive gains over a trivial segmenter that splits text into equally-sized blocks"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "outperforms all the prior best sentence-segmentation tools by an average of 6.1% F1 points",
              "achieve an average improvement of 2.3 BLEU points over the best prior segmentation tool, as well as massive gains over a trivial segmenter that splits text into equally-sized blocks"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "provides evidence that proper sentence segmentation improves performance in downstream tasks like machine translation"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_67",
      "abstract": "Unsupervised speech recognition (ASR-U) is the problem of learning automatic speech recognition (ASR) systems from unpaired speech-only and text-only corpora. While various algorithms exist to solve this problem, a theoretical framework is missing to study their properties and address such issues as sensitivity to hyperparameters and training instability. In this paper, we proposed a general theoretical framework to study the properties of ASR-U systems based on random matrix theory and the theory of neural tangent kernels. Such a framework allows us to prove various learnability conditions and sample complexity bounds of ASR-U. Extensive ASR-U experiments on synthetic languages with three classes of transition graphs provide strong empirical evidence for our theory.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Unsupervised speech recognition (ASR-U) is the problem of learning automatic speech recognition (ASR) systems from unpaired speech-only and text-only corpora. While various algorithms exist to solve this problem, a theoretical framework is missing to study their properties and address such issues as sensitivity to hyperparameters and training instability.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "unsupervised speech recognition (ASR-U)",
              "various algorithms"
            ],
            "Object": {
              "Primary Object": [
                "learning automatic speech recognition (ASR) systems",
                "unpaired speech-only and text-only corpora"
              ],
              "Secondary Object": [
                "sensitivity to hyperparameters",
                "training instability"
              ]
            },
            "Context": [
              "problem of unsupervised speech recognition (ASR-U) exists but lacks a theoretical framework to study its properties and address specific challenges like sensitivity to hyperparameters and training instability"
            ],
            "Purpose": [
              "to provide a theoretical framework for studying ASR-U methods and addressing associated problems"
            ],
            "Method": [
              "missing (<NONE>)"
            ],
            "Results": [
              "missing (<NONE>)"
            ],
            "Analysis": [
              "missing (<NONE>)"
            ],
            "Challenge": [
              "lack of a theoretical framework for understanding and improving ASR-U methods"
            ],
            "Ethical": [
              "missing (<NONE>)"
            ],
            "Implications": [
              "future work may benefit from developing a comprehensive theory for ASR-U which addresses practical limitations"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we proposed a general theoretical framework to study the properties of ASR-U systems based on random matrix theory and the theory of neural tangent kernels. Such a framework allows us to prove various learnability conditions and sample complexity bounds of ASR-U.",
          "Main Action": "proposed",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a general theoretical framework"
              ],
              "Secondary Object": [
                "ASR-U systems",
                "random matrix theory",
                "theory of neural tangent kernels"
              ]
            },
            "Context": [
              "to study the properties of ASR-U systems"
            ],
            "Purpose": [
              "prove various learnability conditions and sample complexity bounds of ASR-U"
            ],
            "Method": [
              "based on random matrix theory and the theory of neural tangent kernels"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Extensive ASR-U experiments on synthetic languages with three classes of transition graphs provide strong empirical evidence for our theory.",
          "Main Action": "provide",
          "Arguments": {
            "Agent": [
              "our theory"
            ],
            "Object": {
              "Primary Object": [
                "strong empirical evidence"
              ],
              "Secondary Object": [
                "ASR-U experiments on synthetic languages with three classes of transition graphs"
              ]
            },
            "Context": [
              "Extenstive ASR-U experiments on synthetic languages"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "provide strong empirical evidence for our theory"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}