{
  "papers": [
    {
      "paper_code": "ACL_23_P_829",
      "abstract": "Adaptive inference is a simple method for reducing inference costs. The method works by maintaining multiple classifiers of different capacities, and allocating resources to each test instance according to its difficulty. In this work, we compare the two main approaches for adaptive inference, Early-Exit and Multi-Model, when training data is limited. First, we observe that for models with the same architecture and size, individual Multi-Model classifiers outperform their Early-Exit counterparts by an average of 2.3%. We show that this gap is caused by Early-Exit classifiers sharing model parameters during training, resulting in conflicting gradient updates of model weights. We find that despite this gap, Early-Exit still provides a better speed-accuracy trade-off due to the overhead of the Multi-Model approach. To address these issues, we propose SWEET (Separating Weights for Early-Exit Transformers) an Early-Exit fine-tuning method that assigns each classifier its own set of unique model weights, not updated by other classifiers. We compare SWEET’s speed-accuracy curve to standard Early-Exit and Multi-Model baselines and find that it outperforms both methods at fast speeds while maintaining comparable scores to Early- Exit at slow speeds. Moreover, SWEET individual classifiers outperform Early-Exit ones by 1.1% on average. SWEET enjoys the benefits of both methods, paving the way for further reduction of inference costs in NLP.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Adaptive inference is a simple method for reducing inference costs. The method works by maintaining multiple classifiers of different capacities, and allocating resources to each test instance according to its difficulty.",
          "Main Action": "works",
          "Arguments": {
            "Agent": [
              "The method"
            ],
            "Object": {
              "Primary Object": [
                "by maintaining multiple classifiers of different capacities"
              ],
              "Secondary Object": [
                "and allocating resources to each test instance according to its difficulty"
              ]
            },
            "Context": [
              "Adaptive inference is a simple method for reducing inference costs."
            ],
            "Purpose": [
              "for reducing inference costs"
            ],
            "Method": [
              "by maintaining multiple classifiers of different capacities",
              "allocating resources to each test instance according to its difficulty"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we compare the two main approaches for adaptive inference, Early-Exit and Multi-Model, when training data is limited. First, we observe that for models with the same architecture and size, individual Multi-Model classifiers outperform their Early-Exit counterparts by an average of 2.3%. We show that this gap is caused by Early-Exit classifiers sharing model parameters during training, resulting in conflicting gradient updates of model weights. We find that despite this gap, Early-Exit still provides a better speed-accuracy trade-off due to the overhead of the Multi-Model approach. To address these issues, we propose SWEET (Separating Weights for Early-Exit Transformers) an Early-Exit fine-tuning method that assigns each classifier its own set of unique model weights, not updated by other classifiers.",
          "Main Action": "propose SWEET (Separating Weights for Early-Exit Transformers) an Early-Exit fine-tuning method that assigns each classifier its own set of unique model weights, not updated by other classifiers",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "SWEET (Separating Weights for Early-Exit Transformers) an Early-Exit fine-tuning method"
              ],
              "Secondary Object": [
                "each classifier"
              ]
            },
            "Context": [
              "In this work, we compare the two main approaches for adaptive inference, Early-Exit and Multi-Model, when training data is limited."
            ],
            "Purpose": [
              "To address these issues"
            ],
            "Method": [
              "SWEET (Separating Weights for Early-Exit Transformers) an Early-Exit fine-tuning method that assigns each classifier its own set of unique model weights, not updated by other classifiers"
            ],
            "Results": [
              "for models with the same architecture and size, individual Multi-Model classifiers outperform their Early-Exit counterparts by an average of 2.3%",
              "Early-Exit still provides a better speed-accuracy trade-off due to the overhead of the Multi-Model approach"
            ],
            "Analysis": [
              "this gap is caused by Early-Exit classifiers sharing model parameters during training, resulting in conflicting gradient updates of model weights"
            ],
            "Challenge": [
              "when training data is limited",
              "overhead of the Multi-Model approach"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We compare SWEET’s speed-accuracy curve to standard Early-Exit and Multi-Model baselines and find that it outperforms both methods at fast speeds while maintaining comparable scores to Early- Exit at slow speeds. Moreover, SWEET individual classifiers outperform Early-Exit ones by 1.1% on average. SWEET enjoys the benefits of both methods, paving the way for further reduction of inference costs in NLP.",
          "Main Action": "find that it outperforms both methods at fast speeds while maintaining comparable scores to Early- Exit at slow speeds",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "it"
              ],
              "Secondary Object": [
                "both methods",
                "Early- Exit"
              ]
            },
            "Context": [
              "We compare SWEET’s speed-accuracy curve to standard Early-Exit and Multi-Model baselines"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "compare SWEET’s speed-accuracy curve to standard Early-Exit and Multi-Model baselines"
            ],
            "Results": [
              "it outperforms both methods at fast speeds while maintaining comparable scores to Early- Exit at slow speeds",
              "SWEET individual classifiers outperform Early-Exit ones by 1.1% on average"
            ],
            "Analysis": [
              "SWEET enjoys the benefits of both methods"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "paving the way for further reduction of inference costs in NLP"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_64",
      "abstract": "Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users’ behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored. We show that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks. Also, leveraging additional task-agnostic user histories delivers significant performance benefits. We further demonstrate that our approach can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users’ behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored.",
          "Main Action": "have proposed",
          "Arguments": {
            "Agent": [
              "Recent studies"
            ],
            "Object": {
              "Primary Object": [
                "unified user modeling frameworks"
              ],
              "Secondary Object": [
                "user behavior data from various applications"
              ]
            },
            "Context": [
              "Many of them benefit from utilizing users’ behavior sequences as plain texts, representing rich information in any domain or system without losing generality."
            ],
            "Purpose": [
              "Can language modeling for user history corpus help improve recommender systems?"
            ],
            "Method": [
              "leveraging user behavior data from various applications",
              "utilizing users’ behavior sequences as plain texts"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored."
            ],
            "Challenge": [
              "its applications to recommender systems still remain underexplored"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We show that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks. Also, leveraging additional task-agnostic user histories delivers significant performance benefits. We further demonstrate that our approach can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "language modeling applied directly to task-specific user histories",
              "leveraging additional task-agnostic user histories"
            ],
            "Results": [
              "achieves excellent results on diverse recommendation tasks",
              "delivers significant performance benefits",
              "can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}