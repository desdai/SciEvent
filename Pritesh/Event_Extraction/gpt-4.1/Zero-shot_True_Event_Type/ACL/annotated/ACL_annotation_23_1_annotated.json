{
  "papers": [
    {
      "paper_code": "ACL_23_P_726",
      "abstract": "Languages differ in how they divide up the world into concepts and words; e.g., in contrast to English, Swahili has a single concept for ‘belly’ and ‘womb’. We investigate these differences in conceptualization across 1,335 languages by aligning concepts in a parallel corpus. To this end, we propose Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings. In a detailed linguistic analysis across all languages for one concept (‘bird’) and an evaluation on gold standard data for 32 Swadesh concepts, we show that Conceptualizer has good alignment accuracy. We demonstrate the potential of research on conceptualization in NLP with two experiments. (1) We define crosslingual stability of a concept as the degree to which it has 1-1 correspondences across languages, and show that concreteness predicts stability. (2) We represent each language by its conceptualization pattern for 83 concepts, and define a similarity measure on these representations. The resulting measure for the conceptual similarity between two languages is complementary to standard genealogical, typological, and surface similarity measures. For four out of six language families, we can assign languages to their correct family based on conceptual similarity with accuracies between 54% and 87%",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Languages differ in how they divide up the world into concepts and words; e.g., in contrast to English, Swahili has a single concept for ‘belly’ and ‘womb’. We investigate these differences in conceptualization across 1,335 languages by aligning concepts in a parallel corpus.",
          "Main Action": "investigate",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "these differences in conceptualization across 1,335 languages"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Languages differ in how they divide up the world into concepts and words",
              "in contrast to English, Swahili has a single concept for ‘belly’ and ‘womb’"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "by aligning concepts in a parallel corpus"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To this end, we propose Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings. In a detailed linguistic analysis across all languages for one concept (‘bird’) and an evaluation on gold standard data for 32 Swadesh concepts, we show that Conceptualizer has good alignment accuracy. We demonstrate the potential of research on conceptualization in NLP with two experiments. (1) We define crosslingual stability of a concept as the degree to which it has 1-1 correspondences across languages, and show that concreteness predicts stability. (2) We represent each language by its conceptualization pattern for 83 concepts, and define a similarity measure on these representations.",
          "Main Action": "propose Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "To this end"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The resulting measure for the conceptual similarity between two languages is complementary to standard genealogical, typological, and surface similarity measures. For four out of six language families, we can assign languages to their correct family based on conceptual similarity with accuracies between 54% and 87%.",
          "Main Action": "assign languages to their correct family based on conceptual similarity",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "languages"
              ],
              "Secondary Object": [
                "their correct family"
              ]
            },
            "Context": [
              "For four out of six language families"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "based on conceptual similarity"
            ],
            "Results": [
              "with accuracies between 54% and 87%"
            ],
            "Analysis": [
              "The resulting measure for the conceptual similarity between two languages is complementary to standard genealogical, typological, and surface similarity measures."
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_790",
      "abstract": "The out-of-vocabulary (OOV) words are difficult to represent while critical to the performance of embedding-based downstream models. Prior OOV word embedding learning methods failed to model complex word formation well. In this paper, we propose a novel graph-based relation mining method, namely GRM, for OOV word embedding learning. We first build a Word Relationship Graph (WRG) based on word formation and associate OOV words with their semantically relevant words, which can mine the relational information inside word structures. Subsequently, our GRM can infer high-quality embeddings for OOV words through passing and aggregating semantic attributes and relational information in the WRG, regardless of contextual richness. Extensive experiments demonstrate that our model significantly outperforms state-of-the-art baselines on both intrinsic and downstream tasks when faced with OOV words.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "The out-of-vocabulary (OOV) words are difficult to represent while critical to the performance of embedding-based downstream models. Prior OOV word embedding learning methods failed to model complex word formation well.",
          "Main Action": "failed to model",
          "Arguments": {
            "Agent": [
              "Prior OOV word embedding learning methods"
            ],
            "Object": {
              "Primary Object": [
                "complex word formation"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "The out-of-vocabulary (OOV) words are difficult to represent while critical to the performance of embedding-based downstream models."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "The out-of-vocabulary (OOV) words are difficult to represent",
              "Prior OOV word embedding learning methods failed to model complex word formation well."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we propose a novel graph-based relation mining method, namely GRM, for OOV word embedding learning. We first build a Word Relationship Graph (WRG) based on word formation and associate OOV words with their semantically relevant words, which can mine the relational information inside word structures. Subsequently, our GRM can infer high-quality embeddings for OOV words through passing and aggregating semantic attributes and relational information in the WRG, regardless of contextual richness.",
          "Main Action": "propose a novel graph-based relation mining method, namely GRM, for OOV word embedding learning",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a novel graph-based relation mining method, namely GRM"
              ],
              "Secondary Object": [
                "OOV word embedding learning"
              ]
            },
            "Context": [
              "In this paper"
            ],
            "Purpose": [
              "for OOV word embedding learning"
            ],
            "Method": [
              "graph-based relation mining method",
              "Word Relationship Graph (WRG) based on word formation",
              "associate OOV words with their semantically relevant words",
              "mine the relational information inside word structures",
              "GRM can infer high-quality embeddings for OOV words through passing and aggregating semantic attributes and relational information in the WRG"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "regardless of contextual richness"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Extensive experiments demonstrate that our model significantly outperforms state-of-the-art baselines on both intrinsic and downstream tasks when faced with OOV words.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "Extensive experiments"
            ],
            "Object": {
              "Primary Object": [
                "that our model significantly outperforms state-of-the-art baselines on both intrinsic and downstream tasks when faced with OOV words"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "our model significantly outperforms state-of-the-art baselines on both intrinsic and downstream tasks when faced with OOV words"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}