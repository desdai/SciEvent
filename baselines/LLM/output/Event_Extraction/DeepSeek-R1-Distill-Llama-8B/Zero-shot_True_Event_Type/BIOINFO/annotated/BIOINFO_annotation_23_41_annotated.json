{
  "papers": [
    {
      "paper_code": "bioinfo_23_P_742",
      "abstract": "There is a growing number of available protein sequences, but only a limited amount has been manually annotated. For example, only 0.25% of all entries of UniProtKB are reviewed by human annotators. Further developing automatic tools to infer protein function from sequence alone can alleviate part of this gap. In this article, we investigate the potential of Transformer deep neural networks on a specific case of functional sequence annotation: the prediction of enzymatic classes. We show that our EnzBert transformer models, trained to predict Enzyme Commission (EC) numbers by specialization of a protein language model, outperforms state-of-the-art tools for monofunctional enzyme class prediction based on sequences only. Accuracy is improved from 84% to 95% on the prediction of EC numbers at level two on the EC40 benchmark. To evaluate the prediction quality at level four, the most detailed level of EC numbers, we built two new time-based benchmarks for comparison with state-of-the-art methods ECPred and DeepEC: the macro-F1 score is respectively improved from 41% to 54% and from 20% to 26%. Finally, we also show that using a simple combination of attention maps is on par with, or better than, other classical interpretability methods on the EC prediction task. More specifically, important residues identified by attention maps tend to correspond to known catalytic sites. Quantitatively, we report a max F-Gain score of 96.05%, while classical interpretability methods reach 91.44% at best.",
      "events": [
        {
          "Background/Introduction": "ERROR",
          "Text": "There is a growing number of available protein sequences, but only a limited amount has been manually annotated. For example, only 0.25% of all entries of UniProtKB are reviewed by human annotators. Further developing automatic tools to infer protein function from sequence alone can alleviate part of this gap.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        },
        {
          "Methods/Approach": "",
          "Text": "In this article, we investigate the potential of Transformer deep neural networks on a specific case of functional sequence annotation: the prediction of enzymatic classes. We show that our EnzBert transformer models, trained to predict Enzyme Commission (EC) numbers by specialization of a protein language model, outperforms state-of-the-art tools for monofunctional enzyme class prediction based on sequences only.",
          "Main Action": "We investigate the potential...",
          "Arguments": {
            "Agent": [
              "Our EnzBert transformer models"
            ],
            "Object": {
              "Primary Object": [
                "enzymatic classes"
              ],
              "Secondary Object": [
                "state-of-the-art tools"
              ]
            },
            "Context": [
              "functional sequence annotation",
              "Enzyme Commission numbers",
              "monofunctional enzyme class prediction"
            ],
            "Purpose": [
              "improving predictions",
              "enhancing biological understanding"
            ],
            "Method": [
              "Transformer deep neural networks",
              "EnzBert transformer models",
              "protein language model"
            ],
            "Results": [
              "outperforms state-of-the-art tools"
            ],
            "Analysis": [
              "specialization of a protein language model"
            ],
            "Challenge": [
              "dataset size",
              "complexity"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "advances in enzyme classification",
              "potential for future applications"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Accuracy is improved from 84% to 95% on the prediction of EC numbers at level two on the EC40 benchmark. To evaluate the prediction quality at level four, the most detailed level of EC numbers, we built two new time-based benchmarks for comparison with state-of-the-art methods ECPred and DeepEC: the macro-F1 score is respectively improved from 41% to 54% and from 20% to 26%. Finally, we also show that using a simple combination of attention maps is on par with, or better than, other classical interpretability methods on the EC prediction task. More specifically, important residues identified by attention maps tend to correspond to known catalytic sites. Quantitatively, we report a max F-Gain score of 96.05%, while classical interpretability methods reach 91.44% at best.",
          "Main Action": "We evaluated the prediction quality and showed improvements.",
          "Arguments": {
            "Agent": [
              "Researchers"
            ],
            "Object": {
              "Primary Object": [
                "EC numbers"
              ],
              "Secondary Object": [
                "EC40 benchmark"
              ]
            },
            "Context": [
              "At level two and four of EC numbers"
            ],
            "Purpose": [
              "To improve prediction accuracy and assess quality"
            ],
            "Method": [
              "Macro-F1 score",
              "Attention maps",
              "F-Gain score"
            ],
            "Results": [
              "Improved accuracy from 84% to 95%",
              "Increased macro-F1 score from 41% to 54%",
              "Higher F-Gain score of 96.05%"
            ],
            "Analysis": [
              "Important residues identified by attention maps correspond to catalytic sites"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Broader application in EC prediction tasks"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "bioinfo_23_P_277",
      "abstract": "Sequence alignment is a memory bound computation whose performance in modern systems is limited by the memory bandwidth bottleneck. Processing-in-memory (PIM) architectures alleviate this bottleneck by providing the memory with computing competencies. We propose Alignment-in-Memory (AIM), a framework for high-throughput sequence alignment using PIM, and evaluate it on UPMEM, the first publicly available general-purpose programmable PIM system. Our evaluation shows that a real PIM system can substantially outperform server-grade multi-threaded CPU systems running at full-scale when performing sequence alignment for a variety of algorithms, read lengths, and edit distance thresholds. We hope that our findings inspire more work on creating and accelerating bioinformatics algorithms for such real PIM systems.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Sequence alignment is a memory bound computation whose performance in modern systems is limited by the memory bandwidth bottleneck. Processing-in-memory (PIM) architectures alleviate this bottleneck by providing the memory with computing competencies.",
          "Main Action": "Processing-in-memory (PIM) architectures alleviate this bottleneck",
          "Arguments": {
            "Agent": [
              "Processing-in-memory (PIM) architectures"
            ],
            "Object": {
              "Primary Object": [
                "this bottleneck"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Sequence alignment is a memory bound computation whose performance in modern systems is limited by the memory bandwidth bottleneck"
            ],
            "Purpose": [
              "To improve the performance of sequence alignment in modern systems"
            ],
            "Method": [
              "using processing-in-memory (PIM) architectures"
            ],
            "Results": [
              "addressing the memory bandwidth bottleneck"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Traditional methods are insufficient"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Improving system performance and efficiency in handling memory-intensive tasks"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "ERROR",
          "Text": "We propose Alignment-in-Memory (AIM), a framework for high-throughput sequence alignment using PIM, and evaluate it on UPMEM, the first publicly available general-purpose programmable PIM system",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        },
        {
          "Results/Findings": "",
          "Text": "Our evaluation shows that a real PIM system can substantially outperform server-grade multi-threaded CPU systems running at full-scale when performing sequence alignment for a variety of algorithms, read lengths, and edit distance thresholds.",
          "Main Action": "a real PIM system can substantially outperform",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "server-grade multi-threaded CPU systems"
              ],
              "Secondary Object": [
                "sequence alignment"
              ]
            },
            "Context": [
              "when performing sequence alignment for a variety of algorithms, read lengths, and edit distance thresholds."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "various algorithms, read lengths, and edit distance thresholds"
            ],
            "Results": [
              "substantially outperforming server-grade multi-threaded CPU systems running at full-scale"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "This result suggests that real PIM systems can offer significant advantages over traditional server-grade multi-threaded CPU systems in terms of processing speed and efficiency for sequence alignment tasks."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "We hope that our findings inspire more work on creating and accelerating bioinformatics algorithms for such real PIM systems.",
          "Main Action": "inspire",
          "Arguments": {
            "Agent": [
              "Our findings"
            ],
            "Object": {
              "Primary Object": [
                "more work"
              ],
              "Secondary Object": [
                "bioinformatics algorithms",
                "real PIM systems"
              ]
            },
            "Context": [
              "Creating and accelerating bioinformatics algorithms for such real PIM systems"
            ],
            "Purpose": [
              "To encourage more work on creating and accelerating bioinformatics algorithms for such real PIM systems"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Our findings"
            ],
            "Analysis": [
              "This suggests that our findings have the potential to inspire advancements in bioinformatics technology"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "These results highlight the importance of continued investment in bioinformatics technologies for real-time processing systems"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}