{
  "papers": [
    {
      "paper_code": "ACL_23_P_511",
      "abstract": "Pre-trained large language models (PLMs) underly most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM and associated techniques like few-shot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks–while they can be used to compare systems at a high level–relate to the real world use cases for which people have been adopting them. In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input and output language. Our results show that PLMs differ in their applicability to different data regimes and their generalization to multiple languages. They further inform practitioners as to which PLMs to use for a given generation task setup. We share best practices to be taken into consideration when benchmarking generation capabilities during the development of upcoming PLMs.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Pre-trained large language models (PLMs) underly most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM and associated techniques like few-shot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks–while they can be used to compare systems at a high level–relate to the real-world use cases for which people have been adopting them.",
          "Main Action": "Despite their ubiquitous use, the generation quality of language models is rarely evaluated",
          "Arguments": {
            "Agent": [
              "pre-trained large language models (PLMs)"
            ],
            "Object": {
              "Primary Object": [
                "generation quality of language models"
              ],
              "Secondary Object": [
                "existing generation tasks"
              ]
            },
            "Context": [
              "Pre-trained large language models (PLMs) underly most new developments in natural language processing.",
              "They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks.",
              "Autoregressive PLMs like GPT-3 or PaLM and associated techniques like few-shot learning, have additionally shifted the output modality to generation instead of classification or regression."
            ],
            "Purpose": [
              "It is unclear how existing generation tasks–while they can be used to compare systems at a high level–relate to the real-world use cases for which people have been adopting them."
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "While they can be used to compare systems at a high level"
            ],
            "Analysis": [
              "people have been adopting them"
            ],
            "Challenge": [
              "it is unclear how existing generation tasks–while they can be used to compare systems at a high level–relate to the real-world use cases for which people have been adopting them"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "improving model evaluation to better align with actual user needs"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input and output language.",
          "Main Action": "Adapt",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "existing application-specific generation benchmarks"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "natural language generation tasks along dimensions such as scale, architecture, input and output language"
            ],
            "Purpose": [
              "To empirically investigate the limitations and capabilities of PLMs in natural language generation tasks"
            ],
            "Method": [
              "Empirical investigation across dimensions including scale, architecture, input, and output language"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "By demonstrating the effectiveness of adapting existing benchmarks to PLMs, this work highlights their potential for improving natural language processing systems and informs future research directions"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our results show that PLMs differ in their applicability to different data regimes and their generalization to multiple languages. They further inform practitioners as to which PLMs to use for a given generation task setup.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "Our results"
            ],
            "Object": {
              "Primary Object": [
                "Pre-trained Language Models (PLMs)"
              ],
              "Secondary Object": [
                "different data regimes",
                "multiple languages"
              ]
            },
            "Context": [
              "comparing PLMs across different settings"
            ],
            "Purpose": [
              "informing practitioners"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "differ in their applicability to different data regimes",
              "generalize to multiple languages"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "guiding choices for generation tasks"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "We share best practices to be taken into consideration when benchmarking generation capabilities during the development of upcoming PLMs.",
          "Main Action": "sharing best practices",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "benchmarking generation capabilities"
              ],
              "Secondary Object": [
                "development of upcoming PLMs"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "providing guidance for benchmarking"
            ],
            "Method": [
              "sharing best practices"
            ],
            "Results": [
              "improved benchmarking processes"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader impact on AI model development standards"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_388",
      "abstract": "Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typology from the source language or when pre-training data is limited in size. In this paper, we propose XLM-P, a method that contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our space-efficient and model-agnostic XLM-P approach enables (1) lightweight modeling of language-invariant and language-specific knowledge across languages, and (2) easy integration with other multilingual pre-training methods. On the tasks of XTREME, which include text classification, sequence labeling, question answering, and sentence retrieval, both base- and large-size language models pre-trained with our proposed method exhibit consistent performance improvement. Furthermore, it provides substantial advantages for low-resource languages in unsupervised sentence retrieval and for target languages that differ greatly from the source language in cross-lingual transfer.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typology from the source language or when pre-training data is limited in size.",
          "Main Action": "Their performance is hindered",
          "Arguments": {
            "Agent": [
              "Multilingual pre-trained language models"
            ],
            "Object": {
              "Primary Object": [
                "Target language",
                "Pre-training data"
              ],
              "Secondary Object": [
                "Source language"
              ]
            },
            "Context": [
              "Cross-lingual transfer abilities",
              "Zero-shot capabilities"
            ],
            "Purpose": [
              "Importance of overcoming limitations"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Distant typology between source and target languages",
              "Limited pre-training data"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Potential for future applications",
              "Significance in NLP research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we propose XLM-P, a method that contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our space-efficient and model-agnostic XLM-P approach enables (1) lightweight modeling of language-invariant and language-specific knowledge across languages, and (2) easy integration with other multilingual pre-training methods.",
          "Main Action": "We propose XLM-P",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "lightweight modeling of language-invariant and language-specific knowledge across languages",
                "easy integration with other multilingual pre-training methods"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "handling of multilingual models inefficiently"
            ],
            "Purpose": [
              "to address inefficiencies in handling multilingual models"
            ],
            "Method": [
              "contextual prompt retrieval",
              "enabling lightweight modeling",
              "model-agnostic approach",
              "integration with other multilingual pre-training methods"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "On the tasks of XTREME, which include text classification, sequence labeling, question answering, and sentence retrieval, both base- and large-size language models pre-trained with our proposed method exhibit consistent performance improvement. Furthermore, it provides substantial advantages for low-resource languages in unsupervised sentence retrieval and for target languages that differ greatly from the source language in cross-lingual transfer.",
          "Main Action": "exhibit consistent performance improvement",
          "Arguments": {
            "Agent": [
              "base-and-large-size language models"
            ],
            "Object": {
              "Primary Object": [
                "text classification",
                "sequence labeling",
                "question answering",
                "sentence retrieval"
              ],
              "Secondary Object": [
                "low-resource languages",
                "cross-lingual transfer"
              ]
            },
            "Context": [
              "tasks of XTREME",
              "unsupervised sentence retrieval",
              "target languages"
            ],
            "Purpose": [
              "evaluate performance improvement",
              "assess robustness and effectiveness",
              "consider resource limitations and cross-language transfer"
            ],
            "Method": [
              "training models with proposed method",
              "evaluation against standard benchmarks",
              "testing across different language sizes and settings"
            ],
            "Results": [
              "consistent performance improvement across all tasks",
              "substantial advantages for low-resource languages",
              "benefits in cross-lingual transfer"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}