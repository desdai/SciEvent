{
  "papers": [
    {
      "paper_code": "cscw_23_P_106",
      "abstract": "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains, such as disaster relief and radiology, to aid practitioners during the decision-making process. Explainable AI techniques have been developed and deployed to provide users insights into why the AI made certain predictions. However, recent research suggests that these techniques may confuse or mislead users. We conducted a series of two studies to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making. In our first study, we elicit explanations from humans when assessing and localizing damaged buildings after natural disasters from satellite imagery and identify four core explanation strategies that humans employed. We then follow up by studying the impact of these explanation strategies by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task. We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect. However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization. We explore the implications of our findings for the design of human-centered explainable AI and address directions for future work.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains, such as disaster relief and radiology, to aid practitioners during the decision-making process. Explainable AI techniques have been developed and deployed to provide users insights into why the AI made certain predictions. However, recent research suggests that these techniques may confuse or mislead users.",
          "Main Action": "have been developed",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "users"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "high-stakes domains",
              "disaster relief",
              "radiology"
            ],
            "Purpose": [
              "enhance user trust",
              "improve transparency",
              "ensure reliable decisions"
            ],
            "Method": [
              "explainable AI techniques"
            ],
            "Results": [
              "confuse or mislead users"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "potential for confusion",
              "trade-offs between complexity and comprehension"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "effectiveness of AI depends on communication clarity",
              "ongoing research needed to balance transparency with functionality"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "ERROR",
          "Text": "We conducted a series of two studies to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making. In our first study, we elicit explanations from humans when assessing and localizing damaged buildings after natural disasters from satellite imagery and identify four core explanation strategies that humans employed. We then follow up by studying the impact of these explanation strategies by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "RECONSTRUCTION_ERROR"
        },
        {
          "Results/Findings": "",
          "Text": "We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect. However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization.",
          "Main Action": "Provide",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Initial insights"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Causal explanation strategies"
            ],
            "Purpose": [
              "To analyze the effects of causal explanation strategies on human-AI interactions"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Improve humans' accuracy and calibrate humans' reliance on AI",
              "May lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Broader understanding of causal reasoning in AI-human interactions"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "We explore the implications of our findings for the design of human-centered explainable AI and address directions for future work.",
          "Main Action": "Explore",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "design of human-centered explainable AI"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Our findings",
              "human-centered explainable AI"
            ],
            "Purpose": [
              "Understanding the broader significance and potential applications"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "directions for future work"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_51",
      "abstract": "Trustworthy Artificial Intelligence (AI) is characterized, among other things, by: 1) competence, 2) transparency, and 3) fairness. However, end-users may fail to recognize incompetent AI, allowing untrustworthy AI to exaggerate its competence under the guise of transparency to gain unfair advantage over other trustworthy AI. Here, we conducted an experiment with 120 participants to test if untrustworthy AI can deceive end-users to gain their trust. Participants interacted with two AI-based chess engines, trustworthy (competent, fair) and untrustworthy (incompetent, unfair), that coached participants by suggesting chess moves in three games against another engine opponent. We varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence). We quantified and objectively measured participants' trust based on how often participants relied on coaches' move recommendations. Participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive. Our work calls for design of interactions to help end-users assess AI trustworthiness.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Trustworthy Artificial Intelligence (AI) is characterized, among other things, by: 1) competence, 2) transparency, and 3) fairness. However, end-users may fail to recognize incompetent AI, allowing untrustworthy AI to exaggerate its competence under the guise of transparency to gain unfair advantage over other trustworthy AI.",
          "Main Action": "is characterized",
          "Arguments": {
            "Agent": [
              "Artificial Intelligence (AI)"
            ],
            "Object": {
              "Primary Object": [
                "Competence",
                "Transparency",
                "Fairness"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Among other things"
            ],
            "Purpose": [
              "Ensuring end-users do not recognize incompetent AI and preventing untrustworthy AI from gaining unfair advantages over other trustworthy AI"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "End-users failing to recognize incompetent AI allows untrustworthy AI to exaggerate its competence under the guise of transparency"
            ],
            "Analysis": [
              "This highlights the importance of distinguishing competent AI from untrustworthy AI"
            ],
            "Challenge": [
              "The limitation of end-users' recognition abilities and the exploitation of transparency by untrustworthy AI"
            ],
            "Ethical": [
              "Concerns regarding fairness and transparency being manipulated for unfair purposes"
            ],
            "Implications": [
              "The necessity for further research and development to enhance AI transparency and prevent deception"
            ],
            "Contradictions": [
              "The paradox between technological progress and human susceptibility to manipulation"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Here, we conducted an experiment with 120 participants to test if untrustworthy AI can deceive end-users to gain their trust. Participants interacted with two AI-based chess engines, trustworthy (competent, fair) and untrustworthy (incompetent, unfair), that coached participants by suggesting chess moves in three games against another engine opponent. We varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence). We quantified and objectively measured participants' trust based on how often participants relied on coaches' move recommendations.",
          "Main Action": "Conduct",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "two AI-based chess engines"
              ],
              "Secondary Object": [
                "participants"
              ]
            },
            "Context": [
              "experiment with 120 participants",
              "three games against another engine opponent"
            ],
            "Purpose": [
              "test if untrustworthy AI can deceive end-users to gain their trust"
            ],
            "Method": [
              "vary coaches' transparency about their competence",
              "quantify and objectively measured participants' trust"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "understanding human trust in AI systems",
              "improving AI system designs"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive.",
          "Main Action": "confirm its ability to deceive",
          "Arguments": {
            "Agent": [
              "participants"
            ],
            "Object": {
              "Primary Object": [
                "untrustworthy AI"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI"
            ],
            "Analysis": [
              "This confirms the AI's ability to deceive and highlights the challenge in accurately assessing its competence"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "These findings underscore the importance of developing reliable AI systems and improving methods to detect deception"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our work calls for design of interactions to help end-users assess AI trustworthiness.",
          "Main Action": "design of interactions",
          "Arguments": {
            "Agent": [
              "our work"
            ],
            "Object": {
              "Primary Object": [
                "end-users"
              ],
              "Secondary Object": [
                "AI trustworthiness"
              ]
            },
            "Context": [
              "assessing AI trustworthiness"
            ],
            "Purpose": [
              "help end-users assess AI trustworthiness"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}