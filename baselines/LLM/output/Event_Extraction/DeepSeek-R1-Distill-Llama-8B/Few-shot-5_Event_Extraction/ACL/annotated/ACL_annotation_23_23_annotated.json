{
  "papers": [
    {
      "paper_code": "ACL_23_P_37",
      "abstract": "Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias. In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore, we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels. Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias.",
          "Main Action": "are published and spread",
          "Arguments": {
            "Agent": [
              "most of today’s fake news"
            ],
            "Object": {
              "Primary Object": [
                "in a multi-modal form"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "due to the rapid upgrade of social platforms"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Label-specific features learned from the training set cannot generalize well to the testing set"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore, we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels.",
          "Main Action": "analyze and identify",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "psycholinguistic bias in the text and the bias of inferring news label based on only image features"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "effects of such transfer can be positive (facilitating acquisition) or negative (impeding acquisition)",
              "language family distance predicts more negative transfer",
              "conversational speech data shows greater facilitation for language acquisition than scripted speech data"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "causal intervention",
              "counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image"
            ],
            "Results": [
              "removing the spurious correlations between text features and news label",
              "estimating the direct effect of the image",
              "eliminating the image-only bias by deducting the direct effect of the image from the total effect on labels"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "risk of losing haplotype-specific information"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "extensive experiments on two real-world benchmark datasets"
            ],
            "Object": {
              "Primary Object": [
                "effectiveness of our framework"
              ],
              "Secondary Object": [
                "improving multi-modal fake news detection"
              ]
            },
            "Context": [
              "extending beyond traditional approaches"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "significant improvements in performance metrics"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "challenges posed by diverse benchmarks and complex tasks"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "advancing state-of-the-art methods for detecting misinformation"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_507",
      "abstract": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community.",
          "Main Action": "present",
          "Arguments": {
            "Agent": [
              "WinoQueer: a benchmark specifically designed to measure..."
            ],
            "Object": {
              "Primary Object": [
                "whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias.",
          "Main Action": "benchmark is community-sourced",
          "Arguments": {
            "Agent": [
              "community"
            ],
            "Object": {
              "Primary Object": [
                "bias benchmark"
              ],
              "Secondary Object": [
                "via application of a novel method"
              ]
            },
            "Context": [
              "from a community survey"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "novel method that generates a bias benchmark from a community survey"
            ],
            "Results": [
              "off-the-shelf models generally do exhibit considerable anti-queer bias"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "fine-tuned neural networks can effectively handle out-of-distribution detection"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Fine-tuning neural networks on specific distributions allows them to better generalize to similar but unseen distributions.",
              "Neural networks exhibit strong memorization biases toward training distribution characteristics."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "Using fine-tuning technique to adapt neural network models for different distributions"
            ],
            "Results": [
              "Improved generalizability to unseen distributions",
              "Reduced memorization of training distribution specifics"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Complexity in tuning parameters for optimal performance",
              "Potential trade-offs between accuracy and computational efficiency"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Widely applicable to various real-world scenarios requiring robust modeling",
              "Promotes efficient resource utilization in machine learning tasks"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}