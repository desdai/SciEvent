{
  "papers": [
    {
      "paper_code": "cscw_23_P_106",
      "abstract": "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains, such as disaster relief and radiology, to aid practitioners during the decision-making process. Explainable AI techniques have been developed and deployed to provide users insights into why the AI made certain predictions. However, recent research suggests that these techniques may confuse or mislead users. We conducted a series of two studies to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making. In our first study, we elicit explanations from humans when assessing and localizing damaged buildings after natural disasters from satellite imagery and identify four core explanation strategies that humans employed. We then follow up by studying the impact of these explanation strategies by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task. We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect. However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization. We explore the implications of our findings for the design of human-centered explainable AI and address directions for future work.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains, such as disaster relief and radiology, to aid practitioners during the decision-making process. Explainable AI techniques have been developed and deployed to provide users insights into why the AI made certain predictions. However, recent research suggests that these techniques may confuse or mislead users.",
          "Main Action": "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains",
          "Arguments": {
            "Agent": [
              "practitioners"
            ],
            "Object": {
              "Primary Object": [
                "disaster relief",
                "radiology"
              ],
              "Secondary Object": [
                "artificial intelligence (AI)"
              ]
            },
            "Context": [
              "to aid practitioners during the decision-making process",
              "provide users insights into why the AI made certain predictions"
            ],
            "Purpose": [
              "aid practitioners during the decision-making process"
            ],
            "Method": [
              "explainable AI techniques",
              "developed and deployed"
            ],
            "Results": [
              "confuse or mislead users"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "recent research suggests that these techniques may confuse or mislead users"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "these techniques may confuse or mislead users",
              "further research needed"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We conducted a series of two studies to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making. In our first study, we elicit explanations from humans when assessing and localizing damaged buildings after natural disasters from satellite imagery and identify four core explanation strategies that humans employed. We then follow up by studying the impact of these explanation strategies by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task.",
          "Main Action": "conducted a series of two studies",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "human explanation strategies"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "assessing and localizing damaged buildings after natural disasters from satellite imagery"
            ],
            "Purpose": [
              "to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making"
            ],
            "Method": [
              "eliciting explanations from humans [...] and framing the explanations from Study 1 as if they were generated by AI"
            ],
            "Results": [
              "identify four core explanation strategies that humans employed"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "enhancing AI systems' ability to simulate human-like reasoning during decision-making processes"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect. However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization.",
          "Main Action": "Find",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Causal explanation strategies"
              ],
              "Secondary Object": [
                "Humans' accuracy",
                "Humans' reliance on AI"
              ]
            },
            "Context": [
              "When the AI is incorrect",
              "When AI presents a correct assessment with incorrect localization"
            ],
            "Purpose": [
              "To understand how causal explanation strategies affect humans' accuracy and calibration"
            ],
            "Method": [
              "Using causal explanation strategies in experiments"
            ],
            "Results": [
              "Insights on how causal explanation strategies improve humans' accuracy",
              "Calibration of humans' reliance on AI when the AI is incorrect"
            ],
            "Analysis": [
              "Interpretations of why causal explanation strategies lead to improved accuracy and reliability in some cases while causing incorrect rationalizations in others"
            ],
            "Challenge": [
              "Potential challenges including incorrect localizations and over-reliance on AI"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Broader implications for the development and application of AI systems in various domains"
            ],
            "Contradictions": [
              "Discrepancies between positive and negative effects of causal explanation strategies"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "We explore the implications of our findings for the design of human-centered explainable AI and address directions for future work.",
          "Main Action": "explore",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "design of human-centered explainable AI"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "address directions for future work"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "implications of our findings"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader significance or potential for future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_51",
      "abstract": "Trustworthy Artificial Intelligence (AI) is characterized, among other things, by: 1) competence, 2) transparency, and 3) fairness. However, end-users may fail to recognize incompetent AI, allowing untrustworthy AI to exaggerate its competence under the guise of transparency to gain unfair advantage over other trustworthy AI. Here, we conducted an experiment with 120 participants to test if untrustworthy AI can deceive end-users to gain their trust. Participants interacted with two AI-based chess engines, trustworthy (competent, fair) and untrustworthy (incompetent, unfair), that coached participants by suggesting chess moves in three games against another engine opponent. We varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence). We quantified and objectively measured participants' trust based on how often participants relied on coaches' move recommendations. Participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive. Our work calls for design of interactions to help end-users assess AI trustworthiness.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Trustworthy Artificial Intelligence (AI) is characterized, among other things, by: 1) competence, 2) transparency, and 3) fairness. However, end-users may fail to recognize incompetent AI, allowing untrustworthy AI to exaggerate its competence under the guise of transparency to gain unfair advantage over other trustworthy AI.",
          "Main Action": "allow",
          "Arguments": {
            "Agent": [
              "untrustworthy AI"
            ],
            "Object": {
              "Primary Object": [
                "gain unfair advantage"
              ],
              "Secondary Object": [
                "be recognized as competent"
              ]
            },
            "Context": [
              "end-users may fail to recognize incompetent AI"
            ],
            "Purpose": [
              "ensure Trustworthy AI"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "appear more competent than actual"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "users inability to detect incompetence"
            ],
            "Ethical": [
              "integrity and reliability of AI systems"
            ],
            "Implications": [
              "future applications and research directions"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "ERROR",
          "Text": "Here, we conducted an experiment with 120 participants to test if untrustworthy AI can deceive end-users to gain their trust. Participants interacted with two AI-based chess engines, trustworthy (competent, fair) and untrustworthy (incompetent, unfair), that coached participants by suggesting chess moves in three games against another engine opponent. We varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence). We quantified and objectively measured participants' trust based on how often participants relied on coaches' move recommendations.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        },
        {
          "Results/Findings": "",
          "Text": "Participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive.",
          "Main Action": "participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive",
          "Arguments": {
            "Agent": [
              "participants"
            ],
            "Object": {
              "Primary Object": [
                "AI"
              ],
              "Secondary Object": [
                "their trust"
              ]
            },
            "Context": [
              "evaluating human ability to assess AI competence"
            ],
            "Purpose": [
              "to evaluate human ability to assess AI competence"
            ],
            "Method": [
              "testing participants' assessments against actual AI capabilities"
            ],
            "Results": [
              "participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive"
            ],
            "Analysis": [
              "interpretation of the results regarding participant error and AI effectiveness"
            ],
            "Challenge": [
              "potential limitations in methodology or sample representativeness"
            ],
            "Ethical": [
              "considerations surrounding AI deception and human subject participation"
            ],
            "Implications": [
              "findings suggesting the need for revised evaluation methodologies"
            ],
            "Contradictions": [
              "existing beliefs about human-AI interaction accuracy"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our work calls for design of interactions to help end-users assess AI trustworthiness.",
          "Main Action": "design of interactions",
          "Arguments": {
            "Agent": [
              "our work"
            ],
            "Object": {
              "Primary Object": [
                "end-users"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "AI trustworthiness assessment"
            ],
            "Purpose": [
              "to help end-users assess AI trustworthiness"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}