{
  "papers": [
    {
      "paper_code": "ACL_23_P_505",
      "abstract": "Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries. We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved.",
          "Main Action": "Design biases in NLP systems often stem from their creator’s positionality",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "Design biases"
              ],
              "Secondary Object": [
                "Creator’s positionality"
              ]
            },
            "Context": [
              "Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "researcher, system, and dataset positionality is often unobserved"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "understanding where biases come from for improving NLP systems"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social acceptability and hate speech detection.",
          "Main Action": "Introduce",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "NLPositionality"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "characterizing design biases",
              "quantifying the positionality of NLP datasets and models"
            ],
            "Purpose": [
              "Addressing design biases in NLP datasets"
            ],
            "Method": [
              "collecting annotations from a diverse pool of volunteer participants on LabintheWild",
              "statistically quantifying alignment with dataset labels and model predictions"
            ],
            "Results": [
              "alignment between participant annotations and dataset/model outputs",
              "application to social acceptability and hate speech detection"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Potential for evaluating and improving NLP datasets and models"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries. We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "16,299 annotations"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "to date",
              "in over a year",
              "for 600 instances",
              "from 1,096 annotators",
              "across 87 countries"
            ],
            "Purpose": [
              "understanding dataset alignment"
            ],
            "Method": [
              "collecting annotations",
              "gathering data",
              "obtaining inputs"
            ],
            "Results": [
              "datasets and models align predominantly with Western, White, college-educated, and younger populations",
              "non-binary people and non-native English speakers are further marginalized by datasets and models"
            ],
            "Analysis": [
              "interpretation of findings regarding population alignment",
              "explanation of disparities among groups"
            ],
            "Challenge": [
              "bias in datasets affecting certain groups"
            ],
            "Ethical": [
              "marginalization of non-binary people and non-native English speakers",
              "ethical concerns regarding inclusivity"
            ],
            "Implications": [
              "need for more inclusive datasets",
              "potential consequences of biased models"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.",
          "Main Action": "discuss",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "their own positionality"
              ],
              "Secondary Object": [
                "datasets and models"
              ]
            },
            "Context": [
              "drawing from prior literature"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "more inclusive NLP systems"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "opening the door for more inclusive NLP systems"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_215",
      "abstract": "In this paper, we propose DiffusionNER, which formulates the named entity recognition task as a boundary-denoising diffusion process and thus generates named entities from noisy spans. During training, DiffusionNER gradually adds noises to the golden entity boundaries by a fixed forward diffusion process and learns a reverse diffusion process to recover the entity boundaries. In inference, DiffusionNER first randomly samples some noisy spans from a standard Gaussian distribution and then generates the named entities by denoising them with the learned reverse diffusion process. The proposed boundary-denoising diffusion process allows progressive refinement and dynamic sampling of entities, empowering DiffusionNER with efficient and flexible entity generation capability. Experiments on multiple flat and nested NER datasets demonstrate that DiffusionNER achieves comparable or even better performance than previous state-of-the-art models.",
      "events": [
        {
          "Methods/Approach": "",
          "Text": "In this paper, we propose DiffusionNER, which formulates the named entity recognition task as a boundary-denoising diffusion process and thus generates named entities from noisy spans. During training, DiffusionNER gradually adds noises to the golden entity boundaries by a fixed forward diffusion process and learns a reverse diffusion process to recover the entity boundaries. In inference, DiffusionNER first randomly samples some noisy spans from a standard Gaussian distribution and then generates the named entities by denoising them with the learned reverse diffusion process.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "DiffusionNER"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "named entity recognition task",
              "boundary-denoising diffusion process"
            ],
            "Purpose": [
              "generating named entities from noisy spans"
            ],
            "Method": [
              "gradually adds noises to the golden entity boundaries by a fixed forward diffusion process",
              "learns a reverse diffusion process to recover the entity boundaries"
            ],
            "Results": [
              "samples some noisy spans from a standard Gaussian distribution",
              "generates the named entities by denoising them with the learned reverse diffusion process"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "state-of-the-art models"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The proposed boundary-denoising diffusion process allows progressive refinement and dynamic sampling of entities, empowering DiffusionNER with efficient and flexible entity generation capability. Experiments on multiple flat and nested NER datasets demonstrate that DiffusionNER achieves comparable or even better performance than previous state-of-the-art models.",
          "Main Action": "allows",
          "Arguments": {
            "Agent": [
              "DiffusionNER"
            ],
            "Object": {
              "Primary Object": [
                "progressive refinement and dynamic sampling of entities"
              ],
              "Secondary Object": [
                "efficient and flexible entity generation capability"
              ]
            },
            "Context": [
              "The proposed boundary-denoising diffusion process"
            ],
            "Purpose": [
              "Empowering DiffusionNER with efficient and flexible entity generation capability"
            ],
            "Method": [
              "boundary-denoising diffusion process"
            ],
            "Results": [
              "Experiments on multiple flat and nested NER datasets demonstrate that DiffusionNER achieves comparable or even better performance than previous state-of-the-art models"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Enabling advanced entity generation capabilities for natural language processing tasks"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}