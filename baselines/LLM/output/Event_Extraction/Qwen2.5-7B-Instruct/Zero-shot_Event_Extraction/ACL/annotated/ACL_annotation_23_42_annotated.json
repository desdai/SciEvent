{
  "papers": [
    {
      "paper_code": "ACL_23_P_709",
      "abstract": "Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "recent works"
            ],
            "Object": {
              "Primary Object": [
                "pre-trained language models"
              ],
              "Secondary Object": [
                "training data"
              ]
            },
            "Context": [
              "impressive results across many tasks"
            ],
            "Purpose": [
              "point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "privacy risk of information leakage"
            ],
            "Ethical": [
              "information leakage"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a method named Ethicist",
                "a smoothing loss",
                "a calibrated confidence estimation method"
              ],
              "Secondary Object": [
                "soft prompt embeddings",
                "suffix tokens",
                "generated suffixes"
              ]
            },
            "Context": [
              "in this paper"
            ],
            "Purpose": [
              "to recover the suffix in the training data when given a prefix",
              "to elicit memorization in the attacked model",
              "to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence"
            ],
            "Method": [
              "through loss smoothed soft prompting and calibrated confidence estimation",
              "tune soft prompt embeddings while keeping the model fixed",
              "proposing a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix",
              "proposing a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "that Ethicist significantly improves the extraction performance on a recently proposed public benchmark"
              ],
              "Secondary Object": [
                "several factors influencing the data extraction performance"
              ]
            },
            "Context": [
              "on a recently proposed public benchmark"
            ],
            "Purpose": [
              "investigate"
            ],
            "Method": [
              "including decoding strategy, model scale, prefix length, and suffix length"
            ],
            "Results": [
              "significantly improves the extraction performance",
              "investigate several factors influencing the data extraction performance"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_373",
      "abstract": "Text-based reinforcement learning agents have predominantly been neural network-based models with embeddings-based representation, learning uninterpretable policies that often do not generalize well to unseen games. On the other hand, neuro-symbolic methods, specifically those that leverage an intermediate formal representation, are gaining significant attention in language understanding tasks. This is because of their advantages ranging from inherent interpretability, the lesser requirement of training data, and being generalizable in scenarios with unseen data. Therefore, in this paper, we propose a modular, NEuro-Symbolic Textual Agent (NESTA) that combines a generic semantic parser with a rule induction system to learn abstract interpretable rules as policies. Our experiments on established text-based game benchmarks show that the proposed NESTA method outperforms deep reinforcement learning-based techniques by achieving better generalization to unseen test games and learning from fewer training interactions.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Text-based reinforcement learning agents have predominantly been neural network-based models with embeddings-based representation, learning uninterpretable policies that often do not generalize well to unseen games. On the other hand, neuro-symbolic methods, specifically those that leverage an intermediate formal representation, are gaining significant attention in language understanding tasks. This is because of their advantages ranging from inherent interpretability, the lesser requirement of training data, and being generalizable in scenarios with unseen data.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "neural network-based models",
              "neuro-symbolic methods"
            ],
            "Object": {
              "Primary Object": [
                "uninterpretable policies",
                "language understanding tasks"
              ],
              "Secondary Object": [
                "seen games",
                "scenarios with unseen data"
              ]
            },
            "Context": [
              "text-based reinforcement learning agents",
              "advantages ranging from inherent interpretability, the lesser requirement of training data, and being generalizable"
            ],
            "Purpose": [
              "gaining significant attention",
              "improving policy interpretability, reducing data requirements, enhancing generalizability"
            ],
            "Method": [
              "using embeddings-based representation",
              "leveraging an intermediate formal representation"
            ],
            "Results": [
              "do not generalize well to unseen games",
              "generalizable in scenarios with unseen data"
            ],
            "Analysis": [
              "because of their advantages ranging from inherent interpretability, the lesser requirement of training data, and being generalizable in scenarios with unseen data"
            ],
            "Challenge": [
              "none explicitly mentioned"
            ],
            "Ethical": [
              "none explicitly mentioned"
            ],
            "Implications": [
              "none explicitly mentioned"
            ],
            "Contradictions": [
              "none explicitly mentioned"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Therefore, in this paper, we propose a modular, NEuro-Symbolic Textual Agent (NESTA) that combines a generic semantic parser with a rule induction system to learn abstract interpretable rules as policies.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a modular, NEuro-Symbolic Textual Agent (NESTA)"
              ],
              "Secondary Object": [
                "that combines a generic semantic parser with a rule induction system"
              ]
            },
            "Context": [
              "in this paper"
            ],
            "Purpose": [
              "to learn abstract interpretable rules as policies"
            ],
            "Method": [
              "combines a generic semantic parser with a rule induction system"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our experiments on established text-based game benchmarks show that the proposed NESTA method outperforms deep reinforcement learning-based techniques by achieving better generalization to unseen test games and learning from fewer training interactions.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "our experiments"
            ],
            "Object": {
              "Primary Object": [
                "established text-based game benchmarks"
              ],
              "Secondary Object": [
                "proposed NESTA method",
                "deep reinforcement learning-based techniques"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "outperforming"
            ],
            "Method": [
              "using"
            ],
            "Results": [
              "better generalization to unseen test games",
              "learning from fewer training interactions"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}