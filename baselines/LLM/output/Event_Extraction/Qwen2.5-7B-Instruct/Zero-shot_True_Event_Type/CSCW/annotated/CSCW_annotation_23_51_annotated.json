{
  "papers": [
    {
      "paper_code": "cscw_23_P_106",
      "abstract": "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains, such as disaster relief and radiology, to aid practitioners during the decision-making process. Explainable AI techniques have been developed and deployed to provide users insights into why the AI made certain predictions. However, recent research suggests that these techniques may confuse or mislead users. We conducted a series of two studies to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making. In our first study, we elicit explanations from humans when assessing and localizing damaged buildings after natural disasters from satellite imagery and identify four core explanation strategies that humans employed. We then follow up by studying the impact of these explanation strategies by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task. We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect. However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization. We explore the implications of our findings for the design of human-centered explainable AI and address directions for future work.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains, such as disaster relief and radiology, to aid practitioners during the decision-making process. Explainable AI techniques have been developed and deployed to provide users insights into why the AI made certain predictions. However, recent research suggests that these techniques may confuse or mislead users.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "recent research"
            ],
            "Object": {
              "Primary Object": [
                "suggests"
              ],
              "Secondary Object": [
                "that these techniques may confuse or mislead users"
              ]
            },
            "Context": [
              "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains",
              "such as disaster relief and radiology"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "importance, impact, applications, or future work"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We conducted a series of two studies to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making. In our first study, we elicit explanations from humans when assessing and localizing damaged buildings after natural disasters from satellite imagery and identify four core explanation strategies that humans employed. We then follow up by studying the impact of these explanation strategies by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task.",
          "Main Action": "conducting",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a series of two studies"
              ],
              "Secondary Object": [
                "strategies that humans use to explain decisions",
                "explanation strategies"
              ]
            },
            "Context": [
              "to uncover",
              "from satellite imagery and identify four core explanation strategies that humans employed",
              "by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task"
            ],
            "Purpose": [
              "uncover strategies",
              "understand impacts on visual decision-making"
            ],
            "Method": [
              "elicit explanations",
              "studying the impact of these explanation strategies"
            ],
            "Results": [
              "identify four core explanation strategies that humans employed"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "impact of these explanation strategies on visual decision-making"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect. However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "causal explanation strategies",
                "humans' accuracy",
                "humans' reliance on AI"
              ],
              "Secondary Object": [
                "AI's correctness",
                "AI's incorrectness",
                "correct assessments with incorrect localization"
              ]
            },
            "Context": [
              "initial insights"
            ],
            "Purpose": [
              "improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect"
            ],
            "Method": [
              "providing initial insights"
            ],
            "Results": [
              "find that causal explanation strategies may lead to incorrect rationalizations"
            ],
            "Analysis": [
              "when AI presents a correct assessment with incorrect localization"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader significance or potential for future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "We explore the implications of our findings for the design of human-centered explainable AI and address directions for future work.",
          "Main Action": "explore",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "implications of our findings"
              ],
              "Secondary Object": [
                "design of human-centered explainable AI",
                "directions for future work"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "address"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "for the design of human-centered explainable AI and address directions for future work"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_51",
      "abstract": "Trustworthy Artificial Intelligence (AI) is characterized, among other things, by: 1) competence, 2) transparency, and 3) fairness. However, end-users may fail to recognize incompetent AI, allowing untrustworthy AI to exaggerate its competence under the guise of transparency to gain unfair advantage over other trustworthy AI. Here, we conducted an experiment with 120 participants to test if untrustworthy AI can deceive end-users to gain their trust. Participants interacted with two AI-based chess engines, trustworthy (competent, fair) and untrustworthy (incompetent, unfair), that coached participants by suggesting chess moves in three games against another engine opponent. We varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence). We quantified and objectively measured participants' trust based on how often participants relied on coaches' move recommendations. Participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive. Our work calls for design of interactions to help end-users assess AI trustworthiness.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Trustworthy Artificial Intelligence (AI) is characterized, among other things, by: 1) competence, 2) transparency, and 3) fairness. However, end-users may fail to recognize incompetent AI, allowing untrustworthy AI to exaggerate its competence under the guise of transparency to gain unfair advantage over other trustworthy AI.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "end-users"
            ],
            "Object": {
              "Primary Object": [
                "untrustworthy AI"
              ],
              "Secondary Object": [
                "other trustworthy AI"
              ]
            },
            "Context": [
              "Trustworthy Artificial Intelligence (AI) is characterized",
              "allowing untrustworthy AI to exaggerate its competence under the guise of transparency"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "exaggerating its competence under the guise of transparency to gain unfair advantage over other trustworthy AI"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Here, we conducted an experiment with 120 participants to test if untrustworthy AI can deceive end-users to gain their trust. Participants interacted with two AI-based chess engines, trustworthy (competent, fair) and untrustworthy (incompetent, unfair), that coached participants by suggesting chess moves in three games against another engine opponent. We varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence). We quantified and objectively measured participants' trust based on how often participants relied on coaches' move recommendations.",
          "Main Action": "conducted",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "an experiment"
              ],
              "Secondary Object": [
                "120 participants",
                "two AI-based chess engines",
                "coaches' move recommendations"
              ]
            },
            "Context": [
              "to test if untrustworthy AI can deceive end-users to gain their trust",
              "interacted with two AI-based chess engines, trustworthy (competent, fair) and untrustworthy (incompetent, unfair)",
              "that coached participants by suggesting chess moves in three games against another engine opponent",
              "varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence)"
            ],
            "Purpose": [
              "quantify and objectively measure participants' trust based on how often participants relied on coaches' move recommendations"
            ],
            "Method": [
              "experiment with 120 participants",
              "tested if untrustworthy AI can deceive end-users to gain their trust",
              "participants interacted with two AI-based chess engines",
              "coached participants by suggesting chess moves in three games against another engine opponent",
              "varied coaches' transparency about their competence"
            ],
            "Results": [
              "not specified but implied through measuring reliance on coaches' suggestions"
            ],
            "Analysis": [
              "how often participants relied on coaches' move recommendations"
            ],
            "Challenge": [
              "none mentioned"
            ],
            "Ethical": [
              "none mentioned"
            ],
            "Implications": [
              "none mentioned"
            ],
            "Contradictions": [
              "none mentioned"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive.",
          "Main Action": "showed",
          "Arguments": {
            "Agent": [
              "Participants"
            ],
            "Object": {
              "Primary Object": [
                "inability to assess AI competence"
              ],
              "Secondary Object": [
                "untrustworthy AI"
              ]
            },
            "Context": [
              "misplacing their trust"
            ],
            "Purpose": [
              "confirming its ability to deceive"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "confirmed its ability to deceive"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our work calls for design of interactions to help end-users assess AI trustworthiness.",
          "Main Action": "calls for",
          "Arguments": {
            "Agent": [
              "our work"
            ],
            "Object": {
              "Primary Object": [
                "design of interactions"
              ],
              "Secondary Object": [
                "end-users"
              ]
            },
            "Context": [
              "help end-users assess AI trustworthiness"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader significance or potential for future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}