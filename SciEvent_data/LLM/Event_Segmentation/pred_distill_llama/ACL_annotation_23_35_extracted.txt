Paper Code: ACL_23_P_314

[Background]: Massively multilingual language models have displayed strong performance in zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer setups, where models fine-tuned on task data in a source language are transferred without any or with only a few annotated instances to the target language(s). However, current work typically overestimates model performance as fine-tuned models are frequently evaluated at model checkpoints that generalize best to validation instances in the target languages. This effectively violates the main assumptions of 'true' ZS-XLT and FS-XLT. Such XLT setups require robust methods that do not depend on labeled target language data for validation and model selection.

[Method]: Aiming to improve the robustness of 'true' ZS-XLT and FS-XLT, we propose a simple and effective method that averages different checkpoints (i.e., model snapshots) during task fine-tuning. We conduct exhaustive ZS-XLT and FS-XLT experiments across higher-level semantic tasks (NLI, extractive QA) and lower-level token classification tasks (NER, POS).

[Results]: The results indicate that averaging model checkpoints yields systematic and consistent performance gains across diverse target languages in all tasks. Importantly, it simultaneously substantially desensitizes XLT to varying hyperparameter choices in the absence of target language validation. We also show that checkpoint averaging benefits performance when further combined with run averaging (i.e., averaging the parameters of models fine-tuned over independent runs).

[Implications]: These findings suggest that our approach provides a more reliable framework for evaluating and comparing cross-lingual transfer learning methods without relying on costly target language resources. Additionally, the combination of checkpoint and run averaging offers practical insights into improving model robustness and generalization capabilities in multilingual settings.


Paper Code: ACL_23_P_96

[Background]: Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one's mental state. Existing works focus on generating grounded responses and responding strategies (e.g., questioning), which overlook the effects on ES and lack explicit goals to guide emotional positive transitions.

[Method]: We introduce S supporter, a mixture-of-experts based reinforcement learning model, designed with carefully crafted emotional support (ES) and dialogue coherence rewards to guide the policy's learning for effective responding.

[Results]: Our experiments demonstrate that S supporter outperforms existing approaches in achieving positive emotion elicitation during conversations while maintaining essential conversational goals such as coherence.

[Implications]: This advancement provides a novel framework for developing emotionally supportive AI systems capable of effectively guiding users toward improved mental states through structured, goal-oriented interactions.

