Paper Code: ACL_23_P_34

[Background]: Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging.

[Method]: We explored two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability.

[Results]: We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain).

[Implications]: With oracle studies, we found that label replacement (LR), correcting misaligned labels, increased the absolute accuracy of models trained with diversified datasets by 14.4%. Additionally, some models trained with data generated using LR interventions outperformed LLM-based few-shot classification. In contrast, out-of-scope filtering (OOSF) did not effectively increase model accuracy, suggesting the need for future work in human-in-the-loop text data generation.


Paper Code: ACL_23_P_823

[Background]: Existing research on multimodal relation extraction (MRE) faces two co-existing challenges, internal-information over-utilization and external-information under-exploitation. To combat that, we propose a novel framework that simultaneously implements the idea of internal-information screening and external-information exploiting.

[Method]: We represent the fine-grained semantic structures of the input image and text with the visual and textual scene graphs, which are further fused into a unified cross-modal graph (CMG). Based on CMG, we perform structure refinement with the guidance of the graph information bottleneck principle, actively denoising the less-informative features. Next, we perform topic modeling over the input image and text, incorporating latent multimodal topic features to enrich the contexts.

[Results]: On the benchmark MRE dataset, our system outperforms the current best model significantly.

[Implications]: With further in-depth analyses, we reveal the great potential of our method for the MRE task.

