Paper Code: ACL_23_P_511

[Background]: Pre-trained large language models (PLMs) underlie most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM and associated techniques like few-shot learning have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks—while they can be used to compare systems at a high level—relate to the real-world use cases for which people have been adopting them.

[Method]: This work discusses adapting existing application-specific generation benchmarks to PLMs and provides an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input, and output language. The results highlight differences in applicability across various data regimes and generalize across multiple languages. Best practices for benchmarking generation capabilities are shared to guide the development of upcoming PLMs.

[Results]: The empirical study reveals that PLMs differ significantly in their suitability for diverse data environments and their ability to generalize across linguistic boundaries. Specific findings include variations in performance based on dataset size, architectural choices, and language diversity, with implications for selecting appropriate PLMs for particular generation tasks.

[Implications]: These findings help practitioners make informed decisions about which PLMs to utilize based on specific generation task requirements. Furthermore, the insights provided offer guidance for developers working on new PLMs, ensuring that their generation capabilities are thoroughly tested and optimized against relevant benchmarks.


Paper Code: ACL_23_P_388

[Background]: Multilingual pre-trained language models have demonstrated impressive zero-shot cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typology from the source language or when pre-training data is limited in size.

[Method]: We propose XLM-P, a method that contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our space-efficient and model-agnostic XLM-P approach enables (1) lightweight modeling of language-invariant and language-specific knowledge across languages, and (2) easy integration with other multilingual pre-training methods.

[Results]: On the tasks of XTREME, which include text classification, sequence labeling, question answering, and sentence retrieval, both base- and large-size language models pre-trained with our proposed method exhibit consistent performance improvement. Furthermore, it provides substantial advantages for low-resource languages in unsupervised sentence retrieval and for target languages that differ greatly from the source language in cross-lingual transfer.

[Implications]: The proposed method offers significant potential for enhancing the effectiveness of multilingual models, particularly in scenarios involving linguistic diversity and resource constraints.

