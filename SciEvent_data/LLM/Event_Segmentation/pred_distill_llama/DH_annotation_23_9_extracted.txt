Paper Code: dh_23_P_05

[Background]: We present a case study on quality criteria for the robustness of categories in pragmalinguistic tagset development.

[Method]: We model classification tasks for linguistic routines of discourse referencing in plenary minutes of the German Bundestag. We focus on segmentation, granularity, and interpretation depth. Experiments test how well statistical measures show if interpretative classifications can be reliably reproduced by machines using fine-tuned BERT models compared to Naive Bayes baselines.

[Results]: Experiments reveal that certain segmentations and granularities lead to more accurate and reproducible categorizations when using BERT compared to Naive Bayes.

[Implications]: These findings enhance understanding of category system design for computational linguistics applications, suggesting improved methods for developing interpretable and robust tagging schemes.


Paper Code: dh_23_P_38

[Background]: Sound studies in general, and voice studies in particular, present particular challenges for digital humanities scholarship. The software tools available to digital humanists who want to study performative speech are less familiar and less developed for our uses, and the user base is also much smaller than for text mining or network analysis.

[Method]: This article provides a critical narrative of our research and an outline of our methodology, in applying, developing and refining tools for the analysis of pitch and timing patterns in recorded performances of literary texts. The primary texts and audio considered are poetry readings, but the tools and methods can and have been applied more widely to podcasts, talking books, political speeches, etc.

[Results]: <NONE>

[Implications]: <NONE>

