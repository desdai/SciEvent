Paper Code: ACL_23_P_509

[Background]: Fine-tuning has been proven to be a simple and effective technique to transfer the learned knowledge of Pre-trained Language Models (PLMs) to downstream tasks. However, vanilla fine-tuning easily overfits the target data and degrades the generalization ability. Most existing studies attribute it to catastrophic forgetting, and they retain the pre-trained knowledge indiscriminately without identifying what knowledge is transferable. Motivated by this, we frame fine-tuning into a causal graph and discover that the crux of catastrophic forgetting lies in the missing causal effects from the pre-trained data. Based on the causal view, we propose a unified objective for fine-tuning to retrieve the causality back. Intriguingly, the unified objective can be seen as the sum of the vanilla fine-tuning objective, which learns new knowledge from target data, and the causal objective, which preserves old knowledge from PLMs. Therefore, our method is flexible and can mitigate negative transfer while preserving knowledge.

[Method]: We implement our method on commonsense QA with a proposed heuristic estimation to verify its effectiveness. In the experiments, our method outperforms state-of-the-art fine-tuning methods on all six commonsense QA datasets and can be implemented as a plug-in module to inflate the performance of existing QA models.

[Results]: The results show that our method achieves superior performance compared to existing approaches across multiple benchmarks, demonstrating the effectiveness of integrating causal reasoning into fine-tuning strategies.

[Implications]: Our findings suggest that enhancing fine-tuning techniques with causal understanding could lead to more robust language models capable of handling diverse tasks effectively. This approach not only improves performance on specific domains like commonsense QA but also provides a framework for better knowledge retention and utilization in broader NLP applications.


Paper Code: ACL_23_P_316

[Background]: We study grammar induction with mildly context-sensitive grammars for unsupervised discontinuous parsing.

[Method]: Using the probabilistic linear context-free rewriting system (LCFRS) formalism, our approach fixes the rule structure in advance and focuses on parameter learning with maximum likelihood. To reduce the computational complexity of both parsing and parameter estimation, we restrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two) and further discard rules that require O(l⁶) time to parse, reducing inference to O(l⁵). We also employ tensor decomposition-based rank-space dynamic programming with an embedding-based parameterization of rule probabilities to scale up the number of nonterminals.

[Results]: Experiments on German and Dutch demonstrate that our approach successfully induces linguistically meaningful trees with both continuous and discontinuous structures.

[Implications]: Our findings suggest that mildy context-sensitive grammars can effectively capture linguistic patterns, enabling robust unsupervised parsers capable of handling complex syntactic phenomena across different languages.

