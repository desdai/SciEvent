Paper Code: ACL_23_P_312

[Background]: <NONE>

[Method]: The proposed system uses Incremental Prompting and Verification (IncPrompt) consisting of three stages: event skeleton construction, event expansion, and event-event relation verification. It leverages large language models to generate complex schemas efficiently while maintaining high performance in capturing hierarchical and temporal relations between events.

[Results]: The framework achieves a 7.2% improvement in F1 score for temporal relations and a 31.0% improvement in F1 score for hierarchical relations compared to directly using LLMs for generating linearized graphs. Human assessments show that the translated schemas cover approximately 10% more events and receive a 1.3-point higher rating on readability compared to previous state-of-the-art models.

[Implications]: This approach represents a significant advancement in event schema induction by integrating common sense knowledge from large language models, enabling efficient generation of complex schemas with improved understanding of event hierarchies and temporal sequences. The increased comprehensibility and readability of the generated schemas further enhance their utility in various applications requiring structured event representation.


Paper Code: ACL_23_P_550

[Background]: Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \"lions don't live in the ocean,\" is also ubiquitous in the world but rarely mentioned explicitly in text.

[Method]: We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.

[Results]: Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions.

[Implications]: We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.

