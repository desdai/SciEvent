Paper Code: ACL_23_P_725

[Background]: Backdoor attacks have become an emerging threat to NLP systems.

[Method]: By providing poisoned training data, the adversary can embed a \"backdoor\" into the victim model, which allows input instances satisfying certain textual patterns (e.g., containing a keyword) to be predicted as a target label of the adversary's choice. We propose BITE, a backdoor attack that poisons the training data to establish strong correlations between the target label and a set of \"trigger words\". These trigger words are iteratively identified and injected into the target-label instances through natural word-level perturbations. The poisoned training data instruct the victim model to predict the target label on inputs containing trigger words, forming the backdoor.

[Results]: Experiments on four text classification datasets show that our proposed attack is significantly more effective than baseline methods while maintaining decent stealthiness.

[Implications]: <NONE>


Paper Code: ACL_23_P_702

[Background]: <NONE>

[Method]: <NONE>

[Results]: <NONE>

[Implications]: <NONE>

