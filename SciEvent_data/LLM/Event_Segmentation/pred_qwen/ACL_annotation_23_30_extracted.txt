Paper Code: ACL_23_P_03

[Background]: While the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little. Indeed, recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard sequence log-probability is more informative. It means that internal characteristics of the model can give much more information than we expect, and before using external models and measures, we first need to ask: how far can we go if we use nothing but the translation model itself?

[Method]: We propose to use a method that evaluates the percentage of the source contribution to a generated translation. Intuitively, hallucinations are translations \"detached\" from the source, hence they can be identified by low source contribution.

[Results]: This method improves detection accuracy for the most severe hallucinations by a factor of 2 and is able to alleviate hallucinations at test time on par with the previous best approach that relies on external models.

[Implications]: <NONE>


Paper Code: ACL_23_P_809

[Background]: The wide accessibility of social media has provided linguistically under-represented communities with an extraordinary opportunity to create content in their native languages.

[Method]: Using synthetic data with various levels of noise and a transformer-based model, we demonstrate that the problem can be effectively remediated.

[Results]: We conduct a small-scale evaluation of real data as well.

[Implications]: <NONE>

