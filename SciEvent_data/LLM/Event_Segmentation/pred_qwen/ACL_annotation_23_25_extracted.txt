Paper Code: ACL_23_P_783

[Background]: Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model's predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact.

[Method]: In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).

[Results]: Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks. The gain is substantial on tasks with large domain-label bias (up to 37% in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL.

[Implications]: <NONE>


Paper Code: ACL_23_P_139

[Background]: Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains.

[Method]: To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, restricting their ability to adjust to varied image-text pairs. Inspired by routing-based dynamic networks, we introduce the Dynamic Routing Transformer Network (DynRT-Net), utilizing dynamic paths to engage different routing transformer modules via hierarchical co-attention tailored to cross-modal discrepancies.

[Results]: Experiments on a public dataset show that our approach surpasses state-of-the-art methodologies.

[Implications]: <NONE> <|END_OF_CHUNK|> Based on the updated requirements and the provided abstract, here is the precise extraction adhering to the word counts:

