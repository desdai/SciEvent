Paper Code: cscw_23_P_109

[Background]: Prior work has identified a resilient phenomenon that threatens the performance of human-AI decision-making teams: overreliance, when people agree with an AI, even when it is incorrect. Surprisingly, overreliance does not reduce when the AI produces explanations for its predictions, compared to only providing predictions. Some have argued that overreliance results from cognitive biases or uncalibrated trust, attributing overreliance to an inevitability of human cognition. By contrast, our paper argues that people strategically choose whether or not to engage with an AI explanation

[Method]: To achieve this, we formalize this strategic choice in a cost-benefit framework, where the costs and benefits of engaging with the task are weighed against the costs and benefits of relying on the AI. We manipulate the costs and benefits in a maze task, where participants collaborate with a simulated AI to find the exit of a maze.

[Results]: Through 5 studies (N = 731), we find that costs such as task difficulty (Study 1), explanation difficulty (Study 2, 3), and benefits such as monetary compensation (Study 4) affect overreliance. Finally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify the utility of different explanations, providing further support for our framework.

[Implications]: <NONE>


Paper Code: cscw_23_P_142

[Background]: For peer production communities to be sustainable, they must attract and retain new contributors.

[Method]: Through a large-scale controlled experiment spanning 27 non-English Wikipedia wikis, we evaluate the homepage

[Results]: and that having a positive effect on the newcomer experience depends on the newcomer's context.

[Implications]: <NONE>

