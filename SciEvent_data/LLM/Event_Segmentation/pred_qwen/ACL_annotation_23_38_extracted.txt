Paper Code: ACL_23_P_893

[Background]: Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning.

[Method]: Recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks.

[Results]: An empirical analysis shows that parameters of both pretrained and fine-tuned models can be interpreted in embedding space; two applications include aligning the parameters of different models that share a vocabulary and constructing a classifier without training by \"translating\" the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained.

[Implications]: <NONE>


Paper Code: ACL_23_P_431

[Background]: Previous coherence models focused on individual documents but ignored correlations between them.

[Method]: A GCN-based model was developed to capture structural similarities via mined subgraph patterns and constructed a heterogeneous graph linking documents sharing those patterns.

[Results]: Outperformed baselines in discourse coherence and automated essay scoring evaluations.

[Implications]: Improved understanding and handling of inter-document coherence could enhance natural language processing systems. Future work may explore broader applicability. <|END_OF_RESPONSE|> {\"summary\":\"Previous coherence models focused on individual documents but ignored correlations between them. A GCN-based model was developed to capture structural similarities via mined subgraph patterns and constructed a heterogeneous graph linking documents sharing those patterns. Outperformed bas

