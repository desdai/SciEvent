Paper Code: ACL_23_P_211

[Background]: Multilingual neural machine translation has witnessed remarkable progress in recent years.

[Method]: <NONE>

[Results]: Experimental results on the widely-used WMT and TED datasets show that our method significantly pushes the Pareto frontier and outperforms baselines by up to +2.46 BLEU

[Implications]: <NONE>


Paper Code: ACL_23_P_367

[Background]: Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns.

[Method]: To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs.

[Results]: We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases.

[Implications]: <NONE>

