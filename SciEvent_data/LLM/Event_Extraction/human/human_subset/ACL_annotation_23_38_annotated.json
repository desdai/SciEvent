{
  "papers": [
    {
      "paper_code": "ACL_23_P_893",
      "abstract": "Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a theoretical analysis where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on. We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity. First, an empirical analysis showing that parameters of both pretrained and fine-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) constructing a classifier without training by “translating” the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained. Overall, our findings open the door to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only.",
      "events": [
        {
          "Background/Introduction": "Feasibility of zero pass approach for transformer parameters and two layer attention networks",
          "Text": "Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks.",
          "Main Action": "rely on running",
          "Arguments": {
            "Agent": [
              "most interpretability methods"
            ],
            "Object": {
              "Primary Object": [
                "models over inputs"
              ],
              "Primary Modifier": [],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [
              "Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning"
            ],
            "Purpose": [],
            "Method": [
              "recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks."
            ],
            "Results": [],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        },
        {
          "Methods/Approach": "Presentation of a theoretical analysis resulting in a simple theoretical framework",
          "Text": "In this work, we present a theoretical analysis where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on. We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity.",
          "Main Action": "present",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a theoretical analysis"
              ],
              "Primary Modifier": [],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [],
            "Purpose": [],
            "Method": [
              "where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on"
            ],
            "Results": [
              "We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity."
            ],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        },
        {
          "Results/Findings": "An empirical analysis showing parameters of both pre-trained and fine tuned models",
          "Text": "First, an empirical analysis showing that parameters of both pretrained and fine-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) constructing a classifier without training by “translating” the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained.",
          "Main Action": "showing that",
          "Arguments": {
            "Agent": [
              "an empirical analysis"
            ],
            "Object": {
              "Primary Object": [],
              "Primary Modifier": [],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [],
            "Purpose": [],
            "Method": [],
            "Results": [
              "parameters of both pretrained and fine-tuned models can be interpreted in embedding space.",
              "(b) constructing a classifier without training by “translating” the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained",
              "(a) aligning the parameters of different models that share a vocabulary"
            ],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        },
        {
          "Conclusions/Implications": "Findings open the path for interpretation models that operate in embedding space",
          "Text": "Overall, our findings open the door to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only.",
          "Main Action": "open",
          "Arguments": {
            "Agent": [
              "our findings"
            ],
            "Object": {
              "Primary Object": [
                "the door"
              ],
              "Primary Modifier": [],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [],
            "Purpose": [],
            "Method": [],
            "Results": [
              "to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only."
            ],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_431",
      "abstract": "Coherence is an important aspect of text quality, and various approaches have been applied to coherence modeling. However, existing methods solely focus on a single document’s coherence patterns, ignoring the underlying correlation between documents. We investigate a GCN-based coherence model that is capable of capturing structural similarities between documents. Our model first creates a graph structure for each document, from where we mine different subgraph patterns. We then construct a heterogeneous graph for the training corpus, connecting documents based on their shared subgraphs. Finally, a GCN is applied to the heterogeneous graph to model the connectivity relationships. We evaluate our method on two tasks, assessing discourse coherence and automated essay scoring. Results show that our GCN-based model outperforms all baselines, achieving a new state-of-the-art on both tasks.",
      "events": [
        {
          "Background/Introduction": "Importance of coherence in text quality and existing methods",
          "Text": "Coherence is an important aspect of text quality, and various approaches have been applied to coherence modeling. However, existing methods solely focus on a single document’s coherence patterns, ignoring the underlying correlation between documents.",
          "Main Action": "solely focus",
          "Arguments": {
            "Agent": [
              "existing methods"
            ],
            "Object": {
              "Primary Object": [
                "on a single document’s coherence patterns"
              ],
              "Primary Modifier": [
                "ignoring the underlying correlation between documents."
              ],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [
              "Coherence is an important aspect of text quality, and various approaches have been applied to coherence modeling"
            ],
            "Purpose": [],
            "Method": [],
            "Results": [],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        },
        {
          "Methods/Approach": "Investigation of a GCN based coherence model",
          "Text": "We investigate a GCN-based coherence model that is capable of capturing structural similarities between documents. Our model first creates a graph structure for each document, from where we mine different subgraph patterns. We then construct a heterogeneous graph for the training corpus, connecting documents based on their shared subgraphs. Finally, a GCN is applied to the heterogeneous graph to model the connectivity relationships.",
          "Main Action": "investigate",
          "Arguments": {
            "Agent": [],
            "Object": {
              "Primary Object": [
                "a GCN-based"
              ],
              "Primary Modifier": [
                "coherence model"
              ],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [
              "that is capable of capturing structural similarities between documents."
            ],
            "Purpose": [],
            "Method": [
              "Our model first creates a graph structure for each document, from where we mine different subgraph patterns.",
              "We then construct a heterogeneous graph for the training corpus, connecting documents based on their shared subgraphs.",
              "a GCN is applied to the heterogeneous graph to model the connectivity relationships."
            ],
            "Results": [],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        },
        {
          "Results/Findings": "Evaluation of our method on two  tasks and results",
          "Text": "We evaluate our method on two tasks, assessing discourse coherence and automated essay scoring. Results show that our GCN-based model outperforms all baselines, achieving a new state-of-the-art on both tasks.",
          "Main Action": "evaluate",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "our method"
              ],
              "Primary Modifier": [
                "on two tasks"
              ],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [],
            "Purpose": [],
            "Method": [
              "assessing discourse coherence and automated essay scoring."
            ],
            "Results": [
              "Results show that our GCN-based model outperforms all baselines, achieving a new state-of-the-art on both tasks."
            ],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        }
      ]
    }
  ]
}