{
  "papers": [
    {
      "paper_code": "cscw_23_P_97",
      "abstract": "The spread of misinformation on social media is a pressing societal problem that platforms, policymakers, and researchers continue to grapple with. As a countermeasure, recent works have proposed to employ non-expert fact-checkers in the crowd to fact-check social media content. While experimental studies suggest that crowds might be able to accurately assess the veracity of social media content, an understanding of how crowd fact-checked (mis-)information spreads is missing. In this work, we empirically analyze the spread of misleading vs. not misleading community fact-checked posts on social media. For this purpose, we employ a dataset of community-created fact-checks from Twitter's 'Birdwatch' pilot and map them to resharing cascades on Twitter. Different from earlier studies analyzing the spread of misinformation listed on third-party fact-checking websites (e.g., snopes.com), we find that community fact-checked misinformation is less viral. Specifically, misleading posts are estimated to receive 36.62% fewer retweets than not misleading posts. A partial explanation may lie in differences in the fact-checking targets: community fact-checkers tend to fact-check posts from influential user accounts with many followers, while expert fact-checks tend to target posts that are shared by less influential users. We further find that there are significant differences in virality across different sub-types of misinformation (e.g., factual errors, missing context, manipulated media). Moreover, we conduct a user study to assess the perceived reliability of (real-world) community-created fact-checks. Here, we find that users, to a large extent, agree with community-created fact-checks. Altogether, our findings offer insights into how misleading vs. not misleading posts spread and highlight the crucial role of sample selection when studying misinformation on social media.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "The spread of misinformation on social media is a pressing societal problem that platforms, policymakers, and researchers continue to grapple with. As a countermeasure, recent works have proposed to employ non-expert fact-checkers in the crowd to fact-check social media content. While experimental studies suggest that crowds might be able to accurately assess the veracity of social media content, an understanding of how crowd fact-checked (mis-)information spreads is missing.",
          "Main Action": "to employ non-expert fact-checkers in the crowd to fact-check social media content",
          "Arguments": {
            "Agent": [
              "Recent works"
            ],
            "Object": {
              "Primary Object": [
                "non-expert fact-checkers"
              ],
              "Secondary Object": [
                "social media content"
              ]
            },
            "Context": [
              "The spread of misinformation on social media"
            ],
            "Purpose": [
              "To combat the spread of misinformation on social media"
            ],
            "Method": [
              "crowd fact-checkers"
            ],
            "Results": [
              "an understanding of how crowd fact-checked (mis-)information spreads is missing"
            ],
            "Analysis": [
              "challenges in tracking how misinformation moves post-fact-checking"
            ],
            "Challenge": [
              "difficulty in studying how verified information propagates further"
            ],
            "Ethical": [
              "questions about reliability and bias involving laypeople in fact-checking"
            ],
            "Implications": [
              "new directions for research and policy adjustments"
            ],
            "Contradictions": [
              "discrepancies between initial promising results and the recognition of deeper complexities"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we empirically analyze the spread of misleading vs. not misleading community fact-checked posts on social media. For this purpose, we employ a dataset of community-created fact-checks from Twitter's 'Birdwatch' pilot and map them to resharing cascades on Twitter. Different from earlier studies analyzing the spread of misinformation listed on third-party fact-checking websites (e.g., snopes.com), we find that community fact-checked misinformation is less viral.",
          "Main Action": "empirically analyze",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "spread of misleading vs. not misleading community fact-checked posts"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "community fact-checked posts",
              "social media"
            ],
            "Purpose": [
              "to compare the spread of misleading vs. not misleading community fact-checked posts"
            ],
            "Method": [
              "dataset of community-created fact-checks from Twitter's 'Birdwatch' pilot",
              "mapping them to resharing cascades on Twitter"
            ],
            "Results": [
              "find that community fact-checked misinformation is less viral"
            ],
            "Analysis": [
              "Different from earlier studies analyzing the spread of misinformation listed on third-party fact-checking websites (e.g., snopes.com)"
            ],
            "Challenge": [
              "Different from earlier studies analyzing the spread of misinformation listed on third-party fact-checking websites"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader significance or potential for future applications/research",
              "impacts on understanding community fact-checking dynamics"
            ],
            "Contradictions": [
              "disagreements with existing knowledge regarding misinformation spread"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Specifically, misleading posts are estimated to receive 36.62% fewer retweets than not misleading posts. A partial explanation may lie in differences in the fact-checking targets: community fact-checkers tend to fact-check posts from influential user accounts with many followers, while expert fact-checks tend to target posts that are shared by less influential users. We further find that there are significant differences in virality across different sub-types of misinformation (e.g., factual errors, missing context, manipulated media). Moreover, we conduct a user study to assess the perceived reliability of (real-world) community-created fact-checks. Here, we find that users, to a large extent, agree with community-created fact-checks.",
          "Main Action": "misleading posts are estimated to receive 36.62% fewer retweets than not misleading posts",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "misleading posts"
              ],
              "Secondary Object": [
                "not misleading posts"
              ]
            },
            "Context": [
              "social media platforms"
            ],
            "Purpose": [
              "to analyze the impact of misinformation on user engagement metrics"
            ],
            "Method": [
              "conducting a user study",
              "assessing the perceived reliability of community-created fact-checks"
            ],
            "Results": [
              "misleading posts receive 36.62% fewer retweets than not misleading posts"
            ],
            "Analysis": [
              "community fact-checkers tend to fact-check posts from influential user accounts... while expert fact-checks tend to target posts that are shared by less influential users"
            ],
            "Challenge": [
              "distinguishing between different subtypes of misinformation"
            ],
            "Ethical": [
              "potentially sensitive topics related to trust in online information"
            ],
            "Implications": [
              "better fact-checking strategies could reduce misinformation's harm"
            ],
            "Contradictions": [
              "conflicting theories about why certain fact-checking targets are chosen"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Altogether, our findings offer insights into how misleading vs. not misleading posts spread and highlight the crucial role of sample selection when studying misinformation on social media.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_29",
      "abstract": "Understanding the values that collaborators bring to a collaboration is important for the design of new systems. In collaborative systems, understanding differing values could help design solutions to mitigate conflicts and more effectively coordinate collaboration. We review prior studies of Commons-Based Peer Production (CBPP) identifying four common value dimensions previously noted as present in CBPP: usage value, social value, ideological value, and monetary value. We use this synthetic framework to analyze a dataset of 32 interviews with contributors to Wikimedia Commons and editors of Wikipedia who use Commons resources. Our analysis supports the prior values categories while expanding how some dimensions are expressed by participants. We also highlight four additional value dimensions that were not previously identified in CBPP: cultural heritage value, rarity value, aesthetic value, and administrative value. We discuss the implications of our findings for the design of collaborative systems.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Understanding the values that collaborators bring to a collaboration is important for the design of new systems. In collaborative systems, understanding differing values could help design solutions to mitigate conflicts and more effectively coordinate collaboration.",
          "Main Action": "Understanding",
          "Arguments": {
            "Agent": [
              "collaborators"
            ],
            "Object": {
              "Primary Object": [
                "values"
              ],
              "Secondary Object": [
                "systems"
              ]
            },
            "Context": [
              "In collaborative systems"
            ],
            "Purpose": [
              "designing new systems"
            ],
            "Method": [
              "analyzing differing values"
            ],
            "Results": [
              "insights"
            ],
            "Analysis": [
              "knowing the values allows for designing solutions that align with shared goals, reducing conflicts and enhancing coordination"
            ],
            "Challenge": [
              "ensuring that all parties' values are considered, particularly when conflicting interests arise"
            ],
            "Ethical": [
              "fairness and transparency in decision-making processes"
            ],
            "Implications": [
              "improving group dynamics and organizational effectiveness beyond individual project boundaries"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We review prior studies of Commons-Based Peer Production (CBPP) identifying four common value dimensions previously noted as present in CBPP: usage value, social value, ideological value, and monetary value. We use this synthetic framework to analyze a dataset of 32 interviews with contributors to Wikimedia Commons and editors of Wikipedia who use Commons resources.",
          "Main Action": "We use this synthetic framework to analyze",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "a dataset of 32 interviews with contributors to Wikimedia Commons and editors of Wikipedia who use Commons resources"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Commons-Based Peer Production (CBPP), identifying four common value dimensions previously noted as present in CBPP: usage value, social value, ideological value, and monetary value"
            ],
            "Purpose": [
              "to analyze how these value dimensions apply to CBPP"
            ],
            "Method": [
              "using this synthetic framework"
            ],
            "Results": [
              "such value dimensions exist among contributors to Wikimedia Commons and editors of Wikipedia"
            ],
            "Analysis": [
              "These value dimensions manifest in users' behaviors and motivations"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Understanding these value dimensions can improve management of collaborative platforms"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our analysis supports the prior values categories while expanding how some dimensions are expressed by participants. We also highlight four additional value dimensions that were not previously identified in CBPP: cultural heritage value, rarity value, aesthetic value, and administrative value.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "ERROR",
          "Text": "We discuss the implications of our findings for the design of collaborative systems.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        }
      ]
    }
  ]
}