{
  "papers": [
    {
      "paper_code": "ACL_23_P_312",
      "abstract": "Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ~10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances.",
          "Main Action": "Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances.",
          "Arguments": {
            "Agent": [
              "information extraction systems"
            ],
            "Object": {
              "Primary Object": [
                "event graph instances"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "to learn to generalize the schema"
            ],
            "Method": [
              "use information extraction systems to construct a large number of event graph instances from documents"
            ],
            "Results": [
              "a large number of event graph instances"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification",
          "Main Action": "We propose",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "event schemas"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "treating event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs)"
            ],
            "Purpose": [
              "This new paradigm greatly simplifies the schema induction process"
            ],
            "Method": [
              "an incremental prompting and verification method IncPrompt"
            ],
            "Results": [
              "allows us to handle both hierarchical relations and temporal relations between events in a straightforward way"
            ],
            "Analysis": [
              "Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "this allows us to better understand and utilize event-based knowledge in various applications"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ~10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.",
          "Main Action": "Compare",
          "Arguments": {
            "Agent": [
              "IncSchema"
            ],
            "Object": {
              "Primary Object": [
                "Large Language Models (LLMs)",
                "previous state-of-the-art closed-domain schema induction model"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "compared to directly using LLMs to generate a linearized graph"
            ],
            "Purpose": [
              "generate large and complex schemas"
            ],
            "Method": [
              "generate schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations",
              "translate the schemas into coherent stories"
            ],
            "Results": [
              "human assessors were able to cover ~10% more events",
              "rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "advancements in natural language processing tasks"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_550",
      "abstract": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as “lions don’t live in the ocean”, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs. Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as “lions don’t live in the ocean”, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge.",
          "Main Action": "What do LLMs know about negative knowledge?",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "negative knowledge"
              ],
              "Secondary Object": [
                "positive knowledge"
              ]
            },
            "Context": [
              "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge.",
              "Negative knowledge, such as 'lions don't live in the ocean', is also ubiquitous in the world but rarely mentioned explicitly in text."
            ],
            "Purpose": [
              "Examine the ability of LLMs on negative commonsense knowledge."
            ],
            "Method": [
              "Specific tasks or benchmarks to test comprehension of negative vs. positive knowledge"
            ],
            "Results": [
              "Findings on LLMs' effectiveness in handling negative knowledge"
            ],
            "Analysis": [
              "Reasoning on why some models perform better than others"
            ],
            "Challenge": [
              "Bias in dataset affecting evaluation",
              "Complexity of integrating negative knowledge into existing models"
            ],
            "Ethical": [
              "Ensuring testing does not reinforce harmful stereotypes or misinformation"
            ],
            "Implications": [
              "Broader significance for robust AI systems with nuanced reasoning"
            ],
            "Contradictions": [
              "Previous assumptions about LLM capabilities based solely on positive knowledge"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.",
          "Main Action": "Design",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "constrained keywords-to-sentence generation task (CG)",
                "Boolean question answering task (QA)"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "Probe LLMs"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Probe LLMs"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
          "Main Action": "Our experiments reveal",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "LLMs"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "negative commonsense knowledge",
              "polar yes-or-no questions"
            ],
            "Purpose": [
              "to investigate why LLMs fail to generate valid sentences"
            ],
            "Method": [
              "statistical shortcuts",
              "negation reporting bias",
              "language modeling pre-training"
            ],
            "Results": [
              "belief conflict of LLMs"
            ],
            "Analysis": [
              "statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "understanding how LLMs handle negative commonsense knowledge"
            ],
            "Contradictions": [
              "contradicting previous assumptions about model reliability"
            ]
          }
        }
      ]
    }
  ]
}