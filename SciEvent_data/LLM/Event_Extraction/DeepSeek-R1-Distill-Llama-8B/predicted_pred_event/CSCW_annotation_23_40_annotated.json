{
  "papers": [
    {
      "paper_code": "cscw_23_P_187",
      "abstract": "This paper combines design, machine learning and social computing to explore generative deep learning as both tool and probe for respiratory care. We first present GANspire, a deep learning tool that generates fine-grained breathing waveforms, which we crafted in collaboration with one respiratory physician, attending to joint materialities of human breathing data and deep generative models. We then relate a probe, produced with breathing waveforms generated with GANspire, and led with a group of ten respiratory care experts, responding to its material attributes. Qualitative annotations showed that respiratory care experts interpreted both realistic and ambiguous attributes of breathing waveforms generated with GANspire, according to subjective aspects of physiology, activity and emotion. Semi-structured interviews also revealed experts' broader perceptions, expectations and ethical concerns on AI technology, based on their clinical practice of respiratory care, and reflexive analysis of GANspire. These findings suggest design implications for technological aids in respiratory care, and show how ambiguity of deep generative models can be leveraged as a resource for qualitative inquiry, enabling socio-material research with generative deep learning. Our paper contributes to the CSCW community by broadening how generative deep learning may be approached not only as a tool to design human-computer interactions, but also as a probe to provoke open conversations with communities of practice about their current and speculative uses of AI technology.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "This paper combines design, machine learning and social computing to explore generative deep learning as both tool and probe for respiratory care.",
          "Main Action": "Combines",
          "Arguments": {
            "Agent": [
              "This paper"
            ],
            "Object": {
              "Primary Object": [
                "Generative deep learning"
              ],
              "Secondary Object": [
                "Respiratory care"
              ]
            },
            "Context": [
              "Design",
              "machine learning",
              "social computing"
            ],
            "Purpose": [
              "Explore"
            ],
            "Method": [
              "Design",
              "machine learning",
              "social computing"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We first present GANspire, a deep learning tool that generates fine-grained breathing waveforms, which we crafted in collaboration with one respiratory physician, attending to joint materialities of human breathing data and deep generative models. We then relate a probe, produced with breathing waveforms generated with GANspire, and led with a group of ten respiratory care experts, responding to its material attributes. Qualitative annotations showed that respiratory care experts interpreted both realistic and ambiguous attributes of breathing waveforms generated with GANspire, according to subjective aspects of physiology, activity and emotion. Semi-structured interviews also revealed experts' broader perceptions, expectations and ethical concerns on AI technology, based on their clinical practice of respiratory care, and reflexive analysis of GANspire.",
          "Main Action": "We first present",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "GANspire"
              ],
              "Secondary Object": [
                "breathing waveforms"
              ]
            },
            "Context": [
              "crafted in collaboration with one respiratory physician, addressing joint materialities of human breathing data and deep generative models"
            ],
            "Purpose": [
              "led with a group of ten respiratory care experts, responding to its material attributes"
            ],
            "Method": [
              "created GANspire",
              "produced a probe",
              "qualitative annotations",
              "semi-structured interviews"
            ],
            "Results": [
              "respiratory care experts interpreted both realistic and ambiguous attributes of breathing waveforms generated with GANspire",
              "subjective aspects of physiology, activity and emotion"
            ],
            "Analysis": [
              "experts' broader perceptions, expectations and ethical concerns on AI technology"
            ],
            "Challenge": [
              "reflexive analysis of GANspire"
            ],
            "Ethical": [
              "AI technology"
            ],
            "Implications": [
              "broader perceptions",
              "expectations",
              "ethical concerns",
              "clinical practice of respiratory care",
              "future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "These findings suggest design implications for technological aids in respiratory care, and show how ambiguity of deep generative models can be leveraged as a resource for qualitative inquiry, enabling socio-material research with generative deep learning.",
          "Main Action": "Suggest",
          "Arguments": {
            "Agent": [
              "Findings"
            ],
            "Object": {
              "Primary Object": [
                "Design implications"
              ],
              "Secondary Object": [
                "Ampiguity of deep generative models"
              ]
            },
            "Context": [
              "Technological aids in respiratory care"
            ],
            "Purpose": [
              "Enabling socio-material research with generative deep learning"
            ],
            "Method": [
              "Generative deep learning"
            ],
            "Results": [
              "Design implications for technological aids in respiratory care"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our paper contributes to the CSCW community by broadening how generative deep learning may be approached not only as a tool to design human-computer interactions, but also as a probe to provoke open conversations with communities of practice about their current and speculative uses of AI technology.",
          "Main Action": "Our paper contributes",
          "Arguments": {
            "Agent": [
              "our paper"
            ],
            "Object": {
              "Primary Object": [
                "the CSCW community"
              ],
              "Secondary Object": [
                "not only as a tool to design human-computer interactions, but also as a probe to provoke open conversations with communities of practice about their current and speculative uses of AI technology"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "enhancing interaction design and fostering community discussions about AI technology"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_65",
      "abstract": "Explainable AI (XAI) systems are sociotechnical in nature; thus, they are subject to the sociotechnical gap — divide between the technical affordances and the social needs. However, charting this gap is challenging. In the context of XAI, we argue that charting the gap improves our problem understanding, which can reflexively provide actionable insights to improve explainability. Utilizing two case studies in distinct domains, we empirically derive a framework that facilitates systematic charting of the sociotechnical gap by connecting AI guidelines in the context of XAI and elucidating how to use them to address the gap. We apply the framework to a third case in a new domain, showcasing its affordances. Finally, we discuss conceptual implications of the framework, share practical considerations in its operationalization, and offer guidance on transferring it to new contexts. By making conceptual and practical contributions to understanding the sociotechnical gap in XAI, the framework expands the XAI design space.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Explainable AI (XAI) systems are sociotechnical in nature; thus, they are subject to the sociotechnical gap — divide between the technical affordances and the social needs. However, charting this gap is challenging. In the context of XAI, we argue that charting the gap improves our problem understanding, which can reflexively provide actionable insights to improve explainability.",
          "Main Action": "Charting this gap",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "The sociotechnical gap"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Explainable AI (XAI) systems are sociotechnical in nature."
            ],
            "Purpose": [
              "To improve our problem understanding, which can reflexively provide actionable insights to improve explainability."
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Improved problem understanding and actionable insights."
            ],
            "Analysis": [
              "Reflexive provision of actionable insights."
            ],
            "Challenge": [
              "Challenging task."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Broader significance for improving explainability."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Utilizing two case studies in distinct domains, we empirically derive a framework that facilitates systematic charting of the sociotechnical gap by connecting AI guidelines in the context of XAI and elucidating how to use them to address the gap. We apply the framework to a third case in a new domain, showcasing its affordances.",
          "Main Action": "Applying the framework",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "AI guidelines in the context of XAI"
              ],
              "Secondary Object": [
                "a third case in a new domain"
              ]
            },
            "Context": [
              "The sociotechnical gap in XAI"
            ],
            "Purpose": [
              "Facilitating systematic charting of the sociotechnical gap by connecting AI guidelines"
            ],
            "Method": [
              "Using two case studies in distinct domains",
              "Empirically deriving the framework",
              "Applying the framework to a third case"
            ],
            "Results": [
              "Showcasing the affordances of the framework"
            ],
            "Analysis": [
              "Interpreting the results and their implications"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Generalizability of the framework across domains"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Finally, we discuss conceptual implications of the framework, share practical considerations in its operationalization, and offer guidance on transferring it to new contexts. By making conceptual and practical contributions to understanding the sociotechnical gap in XAI, the framework expands the XAI design space.",
          "Main Action": "Making conceptual and practical contributions",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Conceptual and practical contributions"
              ],
              "Secondary Object": [
                "Understanding the sociotechnical gap in XAI"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "To address the sociotechnical gap in XAI"
            ],
            "Method": [
              "Framework"
            ],
            "Results": [
              "Expanding the XAI design space"
            ],
            "Analysis": [
              "Interpretation of how the contributions affect the field"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Broader impact on AI development"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}