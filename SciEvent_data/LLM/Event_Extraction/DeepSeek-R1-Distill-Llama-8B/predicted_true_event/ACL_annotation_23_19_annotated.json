{
  "papers": [
    {
      "paper_code": "ACL_23_P_671",
      "abstract": "Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations. In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which, as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations. We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY’s supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks. TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations.",
          "Main Action": "Predicting temporal relations of event pairs",
          "Arguments": {
            "Agent": [
              "These systems"
            ],
            "Object": {
              "Primary Object": [
                "Event pairs"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations"
            ],
            "Purpose": [
              "Understanding human-like reasoning across domains"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "A challenge implied is the lack of available datasets that test model performance beyond their original domain"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "The broader implication is that improving dataset coverage could enhance our understanding of AI models' capabilities"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which, as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations.",
          "Main Action": "introduce",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "TODAY"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "bridges this gap with temporal differential analysis"
            ],
            "Purpose": [
              "evaluate whether systems can correctly understand the effect of incremental changes"
            ],
            "Method": [
              "slight contextual changes for given event pairs",
              "annotate human explanations"
            ],
            "Results": [
              "quantitative metrics",
              "qualitative data from human evaluations"
            ],
            "Analysis": [
              "interpret these results to assess strengths and challenges"
            ],
            "Challenge": [
              "dependency on domain-specific data",
              "potential biases"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "contribute to understanding NLP models",
              "open doors for future research into incremental reasoning"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY’s supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks.",
          "Main Action": "We show that existing models...",
          "Arguments": {
            "Agent": [
              "models"
            ],
            "Object": {
              "Primary Object": [
                "temporal predictions"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "existing models, including GPT-3.5, drop to random guessing on TODAY"
            ],
            "Purpose": [
              "evaluating how well the models handle temporal predictions"
            ],
            "Method": [
              "benchmark comparisons"
            ],
            "Results": [
              "On the other hand, we show that TODAY’s supervision style and explanation annotations can be used in joint learning"
            ],
            "Analysis": [
              "encouraging models to use more appropriate signals during training"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "improving model training with better supervision styles and explanations can enhance their effectiveness"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems.",
          "Main Action": "train models to solicit incidental supervision",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "GPT-3.5",
                "noisy sources"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "move us more toward the goal of generic temporal reasoning systems"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_65",
      "abstract": "Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations. To address this issue, we propose a novel continual extraction model for analogous relations. Specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. We also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations. Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations.",
          "Main Action": "We conduct an empirical study",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "existing works"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "continual relation extraction (RE)",
              "avoiding forgetting the learned relations"
            ],
            "Purpose": [
              "examining how existing works perform"
            ],
            "Method": [
              "empirical study",
              "observing performance degradation due to analogous relations"
            ],
            "Results": [
              "performance is severely affected by analogous relations"
            ],
            "Analysis": [
              "interpretations suggest the issue arises from similarities between tasks"
            ],
            "Challenge": [
              "overfitting problem",
              "repeatedly replaying samples may cause overfitting"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "importance of addressing these limitations for better model robustness"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To address this issue, we propose a novel continual extraction model for analogous relations. Specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. We also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations.",
          "Main Action": "Designing",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Memory-insensitive relation prototypes"
              ],
              "Secondary Object": [
                "Memory augmentation"
              ]
            },
            "Context": [
              "Addressing this issue"
            ],
            "Purpose": [
              "Improving performance on analogous relations"
            ],
            "Method": [
              "Memory-insensitive relation prototypes",
              "Memory augmentation",
              "Integrated training",
              "Focal knowledge distillation"
            ],
            "Results": [
              "Enhanced performance on analogous relations"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Overcoming the overfitting problem"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Broader significance for NLP tasks"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "ERROR",
          "Text": "Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        }
      ]
    }
  ]
}