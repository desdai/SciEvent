{
  "papers": [
    {
      "paper_code": "ACL_23_P_653",
      "abstract": "The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data, language adaptation can generalize well to diverse languages.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining.",
          "Main Action": "To adapt BLOOM",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "new languages not seen during pretraining"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "The BLOOM model is a large publicly available multilingual language model"
            ],
            "Purpose": [
              "To extend the benefits of BLOOM to other languages without incurring prohibitively large costs"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "incurring prohibitively large costs"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "making the model more usable and accessible across a wider range of languages"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data.",
          "Main Action": "apply",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "BLOOM"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data, language adaptation can generalize well to diverse languages.",
          "Main Action": "add",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "BLOOMZ"
              ],
              "Secondary Object": [
                "new languages"
              ]
            },
            "Context": [
              "which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot"
            ],
            "Purpose": [
              "to teach BLOOMZ a new language"
            ],
            "Method": [
              "including a new language in the multitask fine-tuning mixture"
            ],
            "Results": [
              "finding including a new language in the multitask fine-tuning mixture to be the most effective method"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "language adaptation can generalize well to diverse languages"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_23",
      "abstract": "As 3rd-person pronoun usage shifts to include novel forms, e.g., neopronouns, we need more research on identity-inclusive NLP. Exclusion is particularly harmful in one of the most popular NLP applications, machine translation (MT). Wrong pronoun translations can discriminate against marginalized groups, e.g., non-binary individuals (Dev et al., 2021). In this “reality check”, we study how three commercial MT systems translate 3rd-person pronouns. Concretely, we compare the translations of gendered vs. gender-neutral pronouns from English to five other languages (Danish, Farsi, French, German, Italian), and vice versa, from Danish to English. Our error analysis shows that the presence of a gender-neutral pronoun often leads to grammatical and semantic translation errors. Similarly, gender neutrality is often not preserved. By surveying the opinions of affected native speakers from diverse languages, we provide recommendations to address the issue in future MT research.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "As 3rd-person pronoun usage shifts to include novel forms, e.g., neopronouns, we need more research on identity-inclusive NLP. Exclusion is particularly harmful in one of the most popular NLP applications, machine translation (MT). Wrong pronoun translations can discriminate against marginalized groups, e.g., non-binary individuals (Dev et al., 2021).",
          "Main Action": "Shifts to include",
          "Arguments": {
            "Agent": [
              "Linguistic evolution"
            ],
            "Object": {
              "Primary Object": [
                "Natural Language Processing (NLP)"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Incorrect pronoun translations can discriminate against marginalized groups, e.g., non-binary individuals (Dev et al., 2021)"
            ],
            "Purpose": [
              "To address the lack of inclusivity in NLP, especially concerning gender-neutral pronouns and their proper handling in translation technologies"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Exclusion is particularly harmful in one of the most popular NLP applications, machine translation (MT). Wrong pronoun translations can discriminate against marginalized groups, e.g., non-binary individuals (Dev et al., 2021)"
            ],
            "Analysis": [
              "The existing system fails to accommodate newer pronouns adequately, pointing out gaps in current implementations"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "Mistreatment of marginalized groups is highlighted, making this a significant ethical issue in NLP development"
            ],
            "Implications": [
              "Improving NLP to include all genders will enhance accessibility and reduce bias, suggesting future applications in MT and similar areas"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this “reality check”, we study how three commercial MT systems translate 3rd-person pronouns. Concretely, we compare the translations of gendered vs. gender-neutral pronouns from English to five other languages (Danish, Farsi, French, German, Italian), and vice versa, from Danish to English.",
          "Main Action": "study",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "third-person pronouns"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "this 'reality check'",
              "comparing the translations of gendered vs. gender-neutral pronouns"
            ],
            "Purpose": [
              "evaluating differences in translation quality or accuracy"
            ],
            "Method": [
              "comparing translations from English to five other languages (Danish, Farsi, French, German, Italian)",
              "and vice versa, from Danish to English"
            ],
            "Results": [
              "findings about which system performs better",
              "whether gender affects translation quality"
            ],
            "Analysis": [
              "interpretation of results regarding why certain systems perform differently"
            ],
            "Challenge": [
              "potential issues faced during comparisons, like language-specific nuances"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader impacts on NLP research or practical applications"
            ],
            "Contradictions": [
              "discrepancies observed between expected and actual outputs"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our error analysis shows that the presence of a gender-neutral pronoun often leads to grammatical and semantic translation errors. Similarly, gender neutrality is often not preserved.",
          "Main Action": "Our error analysis shows that the presence of a gender-neutral pronoun often leads to grammatical and semantic translation errors",
          "Arguments": {
            "Agent": [
              "Our error analysis"
            ],
            "Object": {
              "Primary Object": [
                "a gender-neutral pronoun"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Similarly, gender neutrality is often not preserved"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "By surveying the opinions of affected native speakers from diverse languages, we provide recommendations to address the issue in future MT research.",
          "Main Action": "provide",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "address the issue in future MT research"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "affected native speakers from diverse languages"
            ],
            "Purpose": [
              "to address the issue in future MT research"
            ],
            "Method": [
              "surveys"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader significance or potential for future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}