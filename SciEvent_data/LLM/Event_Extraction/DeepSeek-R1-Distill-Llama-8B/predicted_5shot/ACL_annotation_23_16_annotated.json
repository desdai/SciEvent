{
  "papers": [
    {
      "paper_code": "ACL_23_P_404",
      "abstract": "Complaining is an illocutionary act in which the speaker communicates his/her dissatisfaction with a set of circumstances and holds the hearer (the complainee) answerable, directly or indirectly. Considering breakthroughs in machine learning approaches, the complaint detection task has piqued the interest of the natural language processing (NLP) community. Most of the earlier studies failed to justify their findings, necessitating the adoption of interpretable models that can explain the model’s output in real time. We introduce an explainable complaint dataset, X-CI, the first benchmark dataset for explainable complaint detection. Each instance in the X-CI dataset is annotated with five labels: complaint label, emotion label, polarity label, complaint severity level, and rationale (explainability), i.e., the causal span explaining the reason for the complaint/non-complaint label. We address the task of explainable complaint detection and propose a commonsense-aware unified generative framework by reframing the multitask problem as a text-to-text generation task. Our framework can predict the complaint cause, severity level, emotion, and polarity of the text in addition to detecting whether it is a complaint or not. We further establish the advantages of our proposed model on various evaluation metrics over the state-of-the-art models and other baselines when applied to the X-CI dataset in both full and few-shot settings.",
      "events": [
        {
          "Background/Introduction": "ERROR",
          "Text": "Complaining is an illocutionary act in which the speaker communicates his/her dissatisfaction with a set of circumstances and holds the hearer (the complainee) answerable, directly or indirectly. Considering breakthroughs in machine learning approaches, the complaint detection task has piqued the interest of the natural language processing (NLP) community. Most of the earlier studies failed to justify their findings, necessitating the adoption of interpretable models that can explain the model’s output in real time.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "RECONSTRUCTION_ERROR"
        },
        {
          "Methods/Approach": "",
          "Text": "We introduce an explainable complaint dataset, X-CI, the first benchmark dataset for explainable complaint detection. Each instance in the X-CI dataset is annotated with five labels: complaint label, emotion label, polarity label, complaint severity level, and rationale (explainability), i.e., the causal span explaining the reason for the complaint/non-complaint label. We address the task of explainable complaint detection and propose a commonsense-aware unified generative framework by reframing the multitask problem as a text-to-text generation task. Our framework can predict the complaint cause, severity level, emotion, and polarity of the text in addition to detecting whether it is a complaint or not.",
          "Main Action": "introduce",
          "Arguments": {
            "Agent": [
              "X-CI, the first benchmark dataset for explainable complaint detection"
            ],
            "Object": {
              "Primary Object": [
                "an explainable complaint dataset"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We further establish the advantages of our proposed model on various evaluation metrics over the state-of-the-art models and other baselines when applied to the X-CI dataset in both full and few-shot settings.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "over the state-of-the-art models and other baselines when applied to the X-CI dataset in both full and few-shot settings"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_532",
      "abstract": "Document-level event argument extraction aims to identify event arguments beyond sentence level, where a significant challenge is to model long-range dependencies. Focusing on this challenge, we present a new chain reasoning paradigm for the task, which can generate decomposable first-order logic rules for reasoning. This paradigm naturally captures long-range interdependence due to the chains’ compositional nature, which also improves interpretability by explicitly modeling the reasoning process. We introduce T-norm fuzzy logic for optimization, which permits end-to-end learning and shows promise for integrating the expressiveness of logical reasoning with the generalization of neural networks. In experiments, we show that our approach outperforms previous methods by a significant margin on two standard benchmarks (over 6 points in F1). Moreover, it is data-efficient in low-resource scenarios and robust enough to defend against adversarial attacks.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Document-level event argument extraction aims to identify event arguments beyond sentence level, where a significant challenge is to model long-range dependencies.",
          "Main Action": "aims to identify",
          "Arguments": {
            "Agent": [
              "document-level event argument extraction"
            ],
            "Object": {
              "Primary Object": [
                "event arguments beyond sentence level"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "where a significant challenge is to model long-range dependencies"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "a significant challenge is to model long-range dependencies"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Focusing on this challenge, we present a new chain reasoning paradigm for the task, which can generate decomposable first-order logic rules for reasoning. This paradigm naturally captures long-range interdependence due to the chains’ compositional nature, which also improves interpretability by explicitly modeling the reasoning process. We introduce T-norm fuzzy logic for optimization, which permits end-to-end learning and shows promise for integrating the expressiveness of logical reasoning with the generalization of neural networks.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "which can generate decomposable first-order logic rules for reasoning",
              "long-range interdependence due to the chains’ compositional nature",
              "improves interpretability by explicitly modeling the reasoning process"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "T-norm fuzzy logic for optimization"
            ],
            "Results": [
              "end-to-end learning",
              "expressiveness of logical reasoning",
              "generalization of neural networks"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "naturalness of the representation",
              "interpretability trade-offs"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "integration of logical reasoning with neural network generalization",
              "novelty in neural network architectures",
              "potential for creating more interpretable models"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "In experiments, we show that our approach outperforms previous methods by a significant margin on two standard benchmarks (over 6 points in F1). Moreover, it is data-efficient in low-resource scenarios and robust enough to defend against adversarial attacks.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "our approach outperforms previous methods"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "two standard benchmarks (over 6 points in F1)",
              "significant margin"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "over 6 points in F1"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "data efficiency in low-resource scenarios",
              "robustness against adversarial attacks"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader applicability",
              "potential for deployment in various settings"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}