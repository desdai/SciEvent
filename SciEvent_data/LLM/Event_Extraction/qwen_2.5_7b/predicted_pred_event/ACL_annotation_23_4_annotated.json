{
  "papers": [
    {
      "paper_code": "ACL_23_P_392",
      "abstract": "Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale’s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs’ ability to generate rationales with better human utility, while maintaining most of its task performance.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "human utility of existing rationales",
                "certain properties of rationales like conciseness and novelty"
              ],
              "Secondary Object": [
                "estimating human utility without human involvement"
              ]
            },
            "Context": [
              "among the remarkable emergent capabilities of large language models",
              "far from satisfactory and expensive to estimate with human studies"
            ],
            "Purpose": [
              "to understand whether machine-generated rationales have value for humans who might rely on these rationales to answer questions"
            ],
            "Method": [
              "observ[ing]",
              "correlat[ing]"
            ],
            "Results": [
              "existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility"
            ],
            "Analysis": [
              "while we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging"
            ],
            "Challenge": [
              "expensive to estimate with human studies",
              "challenging to estimate human utility without involving people"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "not specified but implied importance of understanding human utility of rationales"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We show that, by estimating a rationale’s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs’ ability to generate rationales with better human utility, while maintaining most of its task performance.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a rationale's helpfulness",
                "LMs' ability"
              ],
              "Secondary Object": [
                "Gen-U",
                "rationales with better human utility"
              ]
            },
            "Context": [
              "estimating a rationale’s helpfulness in answering similar unseen instances"
            ],
            "Purpose": [
              "improve LMs’ ability to generate rationales with better human utility, while maintaining most of its task performance"
            ],
            "Method": [
              "translating this finding into an automated score"
            ],
            "Results": [
              "measure its human utility to a better extent"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "helping improve LMs’ ability to generate rationales with better human utility"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_12",
      "abstract": "Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as ‘José le prometió/ordenó a María ser ordenado/a’ (‘Joseph promised/ordered Mary to be tidy’). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically-guided antecedent retrieval processes. Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity. Additional experiments on Galician reinforce these conclusions. Our findings are equally valuable for the evaluation of language models’ ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as ‘José le prometió/ordenó a María ser ordenado/a’ (‘Joseph promised/ordered Mary to be tidy’). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically-guided antecedent retrieval processes.",
          "Main Action": "compare",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "the ability of humans and several pre-trained masked language models"
              ],
              "Secondary Object": [
                "to correctly identify control dependencies in Spanish sentences"
              ]
            },
            "Context": [
              "such as 'José le prometió/ordenó a María ser ordenado/a' ('Joseph promised/ordered Mary to be tidy')"
            ],
            "Purpose": [
              "to study lexically-guided antecedent retrieval processes"
            ],
            "Method": [
              "psycholinguistic and computational experiments"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity. Additional experiments on Galician reinforce these conclusions.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "our results"
            ],
            "Object": {
              "Primary Object": [
                "show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity"
              ],
              "Secondary Object": [
                "Additional experiments on Galician reinforce these conclusions"
              ]
            },
            "Context": [
              "while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity"
            ],
            "Purpose": [
              "to compare human performance with machine learning model's ability to handle non-adjacency in linguistic structures"
            ],
            "Method": [
              "using additional experiments on Galician data"
            ],
            "Results": [
              "humans correctly identify the (un)acceptability of the strings, but language models struggle with identifying the right antecedents due to linear biases"
            ],
            "Analysis": [
              "language models rely heavily on linear cues rather than understanding deeper semantic relationships"
            ],
            "Challenge": [
              "none mentioned"
            ],
            "Ethical": [
              "none mentioned"
            ],
            "Implications": [
              "these findings suggest limitations in current NLP systems' capacity to process complex syntactic phenomena; further research needed into developing more sophisticated models capable of handling such complexities"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our findings are equally valuable for the evaluation of language models’ ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "our findings"
            ],
            "Object": {
              "Primary Object": [
                "language models' ability to capture linguistic generalizations",
                "psycholinguistic theories of anaphor resolution"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "equally valuable for...evaluation...and...for...anaphor resolution."
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader significance or potential for future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}