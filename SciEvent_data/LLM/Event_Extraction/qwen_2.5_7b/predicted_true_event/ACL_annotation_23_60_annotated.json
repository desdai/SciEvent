{
  "papers": [
    {
      "paper_code": "ACL_23_P_528",
      "abstract": "Text generation often involves producing coherent and grammatically correct texts that also satisfy a given set of semantic constraints. While most approaches for conditional text generation have primarily focused on lexical constraints, they often struggle to effectively incorporate syntactic constraints, which provide a richer language for approximating semantic constraints. We address this gap by introducing NeuroStructural Decoding, a new decoding algorithm that incorporates syntactic constraints to further improve the quality of the generated text. We build NeuroStructural Decoding on the NeuroLogic Decoding (Lu et al. 2021) algorithm, which enables language generation models to produce fluent text while satisfying complex lexical constraints. Our algorithm is powerful and scalable. It tracks lexico-syntactic constraints (e.g., we need to observe dog as subject and ball as object) during decoding by parsing the partial generations at each step. To this end, we adapt a dependency parser to generate parses for incomplete sentences. Our approach is evaluated on three different language generation tasks, and the results show improved performance in both lexical and syntactic metrics compared to previous methods. The results suggest this is a promising solution for integrating fine-grained controllable generation into the conventional beam search decoding.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Text generation often involves producing coherent and grammatically correct texts that also satisfy a given set of semantic constraints. While most approaches for conditional text generation have primarily focused on lexical constraints, they often struggle to effectively incorporate syntactic constraints, which provide a richer language for approximating semantic constraints.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "text generation"
            ],
            "Object": {
              "Primary Object": [
                "coherent and grammatically correct texts"
              ],
              "Secondary Object": [
                "a given set of semantic constraints"
              ]
            },
            "Context": [
              "While most approaches for conditional text generation have primarily focused on lexical constraints",
              "they often struggle to effectively incorporate syntactic constraints"
            ],
            "Purpose": [
              "satisfy a given set of semantic constraints"
            ],
            "Method": [
              "producing coherent and grammatically correct texts"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "struggle to effectively incorporate syntactic constraints"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "richer language for approximating semantic constraints"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We address this gap by introducing NeuroStructural Decoding, a new decoding algorithm that incorporates syntactic constraints to further improve the quality of the generated text. We build NeuroStructural Decoding on the NeuroLogic Decoding (Lu et al. 2021) algorithm, which enables language generation models to produce fluent text while satisfying complex lexical constraints. Our algorithm is powerful and scalable. It tracks lexico-syntactic constraints (e.g., we need to observe dog as subject and ball as object) during decoding by parsing the partial generations at each step. To this end, we adapt a dependency parser to generate parses for incomplete sentences.",
          "Main Action": "introducing",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "NeuroStructural Decoding"
              ],
              "Secondary Object": [
                "a new decoding algorithm"
              ]
            },
            "Context": [
              "by introducing NeuroStructural Decoding, a new decoding algorithm that incorporates syntactic constraints to further improve the quality of the generated text"
            ],
            "Purpose": [
              "to introduce a new decoding algorithm that improves the quality of generated text"
            ],
            "Method": [
              "incorporates syntactic constraints",
              "<builds> NeuroStructural Decoding on the NeuroLogic Decoding (Lu et al. 2021) algorithm",
              "tracks lexico-syntactic constraints (e.g., we need to observe dog as subject and ball as object) during decoding by parsing the partial generations at each step",
              "adapt[es] a dependency parser to generate parses for incomplete sentences"
            ],
            "Results": [
              "improves the quality of the generated text"
            ],
            "Analysis": [
              "none"
            ],
            "Challenge": [
              "none"
            ],
            "Ethical": [
              "none"
            ],
            "Implications": [
              "none"
            ],
            "Contradictions": [
              "none"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our approach is evaluated on three different language generation tasks, and the results show improved performance in both lexical and syntactic metrics compared to previous methods. The results suggest this is a promising solution for integrating fine-grained controllable generation into the conventional beam search decoding.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "results"
            ],
            "Object": {
              "Primary Object": [
                "improved performance"
              ],
              "Secondary Object": [
                "lexical and syntactic metrics",
                "previous methods"
              ]
            },
            "Context": [
              "three different language generation tasks"
            ],
            "Purpose": [
              "promising solution for integrating fine-grained controllable generation into the conventional beam search decoding"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "improved performance in both lexical and syntactic metrics compared to previous methods"
            ],
            "Analysis": [
              "suggests this is a promising solution"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "integrating fine-grained controllable generation into the conventional beam search decoding"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_872",
      "abstract": "Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "Direct speech-to-speech translation (S2ST)"
            ],
            "Object": {
              "Primary Object": [
                "all components can be optimized jointly"
              ],
              "Secondary Object": [
                "cascaded approaches"
              ]
            },
            "Context": [
              "in which",
              "is advantageous over"
            ],
            "Purpose": [
              "to achieve fast inference with a simplified pipeline"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task.",
          "Main Action": "present",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a novel two-pass direct S2ST architecture"
              ],
              "Secondary Object": [
                "UnitY"
              ]
            },
            "Context": [
              "to leverage large amounts of unlabeled text data"
            ],
            "Purpose": [
              "enhance the model performance"
            ],
            "Method": [
              "subword prediction in the first-pass decoder",
              "advanced two-pass decoder architecture design and search strategy",
              "better training regularization"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "leverage large amounts of unlabeled text data"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case.",
          "Main Action": "outperforms",
          "Arguments": {
            "Agent": [
              "UnitY"
            ],
            "Object": {
              "Primary Object": [
                "a single-pass speech-to-unit translation model"
              ],
              "Secondary Object": [
                "by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up"
              ]
            },
            "Context": [
              "on benchmark datasets at various data scales"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up"
            ],
            "Analysis": [
              "predicting spectrogram in the second pass boosts the performance but predicting discrete units achieves 2.51x decoding speed-up compared to that case"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "boosts the performance even when predicting spectrogram in the second pass; predicts discrete units achieves 2.51x decoding speed-up compared to that case"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}