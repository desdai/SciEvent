{
  "papers": [
    {
      "paper_code": "cscw_23_P_97",
      "abstract": "The spread of misinformation on social media is a pressing societal problem that platforms, policymakers, and researchers continue to grapple with. As a countermeasure, recent works have proposed to employ non-expert fact-checkers in the crowd to fact-check social media content. While experimental studies suggest that crowds might be able to accurately assess the veracity of social media content, an understanding of how crowd fact-checked (mis-)information spreads is missing. In this work, we empirically analyze the spread of misleading vs. not misleading community fact-checked posts on social media. For this purpose, we employ a dataset of community-created fact-checks from Twitter's 'Birdwatch' pilot and map them to resharing cascades on Twitter. Different from earlier studies analyzing the spread of misinformation listed on third-party fact-checking websites (e.g., snopes.com), we find that community fact-checked misinformation is less viral. Specifically, misleading posts are estimated to receive 36.62% fewer retweets than not misleading posts. A partial explanation may lie in differences in the fact-checking targets: community fact-checkers tend to fact-check posts from influential user accounts with many followers, while expert fact-checks tend to target posts that are shared by less influential users. We further find that there are significant differences in virality across different sub-types of misinformation (e.g., factual errors, missing context, manipulated media). Moreover, we conduct a user study to assess the perceived reliability of (real-world) community-created fact-checks. Here, we find that users, to a large extent, agree with community-created fact-checks. Altogether, our findings offer insights into how misleading vs. not misleading posts spread and highlight the crucial role of sample selection when studying misinformation on social media.",
      "events": [
        {
          "Background/Introduction": "Problems with spread of misinformation on social media",
          "Text": "The spread of misinformation on social media is a pressing societal problem that platforms, policymakers, and researchers continue to grapple with. As a countermeasure, recent works have proposed to employ non-expert fact-checkers in the crowd to fact-check social media content. While experimental studies suggest that crowds might be able to accurately assess the veracity of social media content, an understanding of how crowd fact-checked (mis-)information spreads is missing.",
          "Main Action": "have proposed to employ",
          "Arguments": {
            "Agent": [
              "recent works"
            ],
            "Object": {
              "Primary Object": [
                "non-expert fact-checkers in the crowd"
              ],
              "Primary Modifier": [],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [
              "recent works have proposed to employ non-expert fact-checkers in the crowd to fact-check social media content",
              "experimental studies suggest that crowds might be able to accurately assess the veracity of social media content"
            ],
            "Purpose": [],
            "Method": [],
            "Results": [],
            "Analysis": [],
            "Challenge": [
              "The spread of misinformation on social media is a pressing societal problem that platforms, policymakers, and researchers continue to grapple with",
              "an understanding of how crowd fact-checked (mis-)information spreads is missing"
            ],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        },
        {
          "Methods/Approach": "Empirical analysis of spread of misleading and not misleading community fact checked posts",
          "Text": "In this work, we empirically analyze the spread of misleading vs. not misleading community fact-checked posts on social media. For this purpose, we employ a dataset of community-created fact-checks from Twitter's 'Birdwatch' pilot and map them to resharing cascades on Twitter. Different from earlier studies analyzing the spread of misinformation listed on third-party fact-checking websites (e.g., snopes.com), we find that community fact-checked misinformation is less viral.",
          "Main Action": "analyze",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "the spread of misleading"
              ],
              "Primary Modifier": [],
              "Secondary Object": [
                "vs. not misleading community fact-checked posts"
              ],
              "Secondary Modifier": []
            },
            "Context": [],
            "Purpose": [],
            "Method": [
              "we empirically analyze the spread of misleading vs. not misleading community fact-checked posts on social media",
              "we employ a dataset of community-created fact-checks from Twitter's 'Birdwatch' pilot and map them to resharing cascades on Twitter"
            ],
            "Results": [
              "community fact-checked misinformation is less viral"
            ],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": [
              "Different from earlier studies analyzing the spread of misinformation listed on third-party fact-checking websites (e.g., snopes.com)"
            ]
          }
        },
        {
          "Results/Findings": "Differences in virality across sub types of misinformation",
          "Text": "Specifically, misleading posts are estimated to receive 36.62% fewer retweets than not misleading posts. A partial explanation may lie in differences in the fact-checking targets: community fact-checkers tend to fact-check posts from influential user accounts with many followers, while expert fact-checks tend to target posts that are shared by less influential users. We further find that there are significant differences in virality across different sub-types of misinformation (e.g., factual errors, missing context, manipulated media). Moreover, we conduct a user study to assess the perceived reliability of (real-world) community-created fact-checks. Here, we find that users, to a large extent, agree with community-created fact-checks.",
          "Main Action": "find",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "there are significant differences in virality across different sub-types of misinformation"
              ],
              "Primary Modifier": [
                "(e.g., factual errors, missing context, manipulated media)"
              ],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [],
            "Purpose": [],
            "Method": [
              "we conduct a user study to assess the perceived reliability of (real-world) community-created fact-checks"
            ],
            "Results": [
              "misleading posts are estimated to receive 36.62% fewer retweets than not misleading posts",
              "there are significant differences in virality across different sub-types of misinformation (e.g., factual errors, missing context, manipulated media)",
              "users, to a large extent, agree with community-created fact-checks"
            ],
            "Analysis": [
              "A partial explanation may lie in differences in the fact-checking targets: community fact-checkers tend to fact-check posts from influential user accounts with many followers, while expert fact-checks tend to target posts that are shared by less influential users"
            ],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        },
        {
          "Conclusions/Implications": "Insights into misleading and misleading posts and role of sample selection",
          "Text": "Altogether, our findings offer insights into how misleading vs. not misleading posts spread and highlight the crucial role of sample selection when studying misinformation on social media.",
          "Main Action": "offer",
          "Arguments": {
            "Agent": [
              "our findings"
            ],
            "Object": {
              "Primary Object": [
                "insights"
              ],
              "Primary Modifier": [],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [],
            "Purpose": [],
            "Method": [],
            "Results": [],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [
              "insights into how misleading vs. not misleading posts spread and highlight the crucial role of sample selection when studying misinformation on social media"
            ],
            "Contradictions": []
          }
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_29",
      "abstract": "Understanding the values that collaborators bring to a collaboration is important for the design of new systems. In collaborative systems, understanding differing values could help design solutions to mitigate conflicts and more effectively coordinate collaboration. We review prior studies of Commons-Based Peer Production (CBPP) identifying four common value dimensions previously noted as present in CBPP: usage value, social value, ideological value, and monetary value. We use this synthetic framework to analyze a dataset of 32 interviews with contributors to Wikimedia Commons and editors of Wikipedia who use Commons resources. Our analysis supports the prior values categories while expanding how some dimensions are expressed by participants. We also highlight four additional value dimensions that were not previously identified in CBPP: cultural heritage value, rarity value, aesthetic value, and administrative value. We discuss the implications of our findings for the design of collaborative systems.",
      "events": [
        {
          "Background/Introduction": "Importance of understanding values in collaborative systems",
          "Text": "Understanding the values that collaborators bring to a collaboration is important for the design of new systems. In collaborative systems, understanding differing values could help design solutions to mitigate conflicts and more effectively coordinate collaboration.",
          "Main Action": "could help design",
          "Arguments": {
            "Agent": [
              "understanding differing values"
            ],
            "Object": {
              "Primary Object": [
                "solutions to mitigate conflicts"
              ],
              "Primary Modifier": [],
              "Secondary Object": [
                "more effectively coordinate collaboration"
              ],
              "Secondary Modifier": []
            },
            "Context": [
              "Understanding the values that collaborators bring to a collaboration is important for the design of new systems",
              "In collaborative systems, understanding differing values could help design solutions to mitigate conflicts and more effectively coordinate collaboration"
            ],
            "Purpose": [],
            "Method": [],
            "Results": [],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        },
        {
          "Methods/Approach": "Prior studies of commons based peer production (CBPP)",
          "Text": "We review prior studies of Commons-Based Peer Production (CBPP) identifying four common value dimensions previously noted as present in CBPP: usage value, social value, ideological value, and monetary value. We use this synthetic framework to analyze a dataset of 32 interviews with contributors to Wikimedia Commons and editors of Wikipedia who use Commons resources.",
          "Main Action": "review",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "prior studies"
              ],
              "Primary Modifier": [
                "of Commons-Based Peer Production (CBPP)"
              ],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [
              "We review prior studies of Commons-Based Peer Production (CBPP) identifying four common value dimensions previously noted as present in CBPP: usage value, social value, ideological value, and monetary value"
            ],
            "Purpose": [],
            "Method": [
              "We use this synthetic framework to analyze a dataset of 32 interviews with contributors to Wikimedia Commons and editors of Wikipedia who use Commons resources"
            ],
            "Results": [],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        },
        {
          "Results/Findings": "Support of prior values categories and highlighting four additional value dimensions",
          "Text": "Our analysis supports the prior values categories while expanding how some dimensions are expressed by participants. We also highlight four additional value dimensions that were not previously identified in CBPP: cultural heritage value, rarity value, aesthetic value, and administrative value.",
          "Main Action": "supports",
          "Arguments": {
            "Agent": [
              "Our analysis"
            ],
            "Object": {
              "Primary Object": [
                "the prior values categories"
              ],
              "Primary Modifier": [],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [],
            "Purpose": [],
            "Method": [
              "Our analysis supports the prior values categories while expanding how some dimensions are expressed by participants",
              "We also highlight four additional value dimensions that were not previously identified in CBPP: cultural heritage value, rarity value, aesthetic value, and administrative value"
            ],
            "Results": [],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [],
            "Contradictions": []
          }
        },
        {
          "Conclusions/Implications": "Implications of findings for designing collaborative systems",
          "Text": "We discuss the implications of our findings for the design of collaborative systems.",
          "Main Action": "discuss",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "the implications of our findings"
              ],
              "Primary Modifier": [],
              "Secondary Object": [],
              "Secondary Modifier": []
            },
            "Context": [],
            "Purpose": [],
            "Method": [],
            "Results": [],
            "Analysis": [],
            "Challenge": [],
            "Ethical": [],
            "Implications": [
              "We discuss the implications of our findings for the design of collaborative systems"
            ],
            "Contradictions": []
          }
        }
      ]
    }
  ]
}