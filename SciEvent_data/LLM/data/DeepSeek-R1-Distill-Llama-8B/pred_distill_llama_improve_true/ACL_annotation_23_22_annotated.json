{
  "papers": [
    {
      "paper_code": "ACL_23_P_540",
      "abstract": "Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our attack BADCODE features a special trigger generation and injection procedure, making the attack more effective and stealthy. The evaluation is conducted on two neural code search models and the results show our attack outperforms baselines by 60%. Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents.",
          "Main Action": "an adversary can inject a backdoor in neural code search models",
          "Arguments": {
            "Agent": [
              "an adversary"
            ],
            "Object": {
              "Primary Object": [
                "neural code search models"
              ],
              "Secondary Object": [
                "buggy or even vulnerable code"
              ]
            },
            "Context": [
              "Reusing off-the-shelf code snippets from online repositories is a common practice",
              "To find desired code snippets, developers resort to code search engines through natural language queries",
              "Neural code search models are hence behind many such engines"
            ],
            "Purpose": [
              "highlighting the lack of study on the security aspects of these models"
            ],
            "Method": [
              "demonstrating the possibility of adversaries injecting backdoors"
            ],
            "Results": [
              "returning buggy or even vulnerable code"
            ],
            "Analysis": [
              "interpreting the significance of the issue relating to privacy and safety"
            ],
            "Challenge": [
              "ensuring that these models remain secure against such attacks"
            ],
            "Ethical": [
              "user trust and system reliability concerns"
            ],
            "Implications": [
              "broader impacts including financial loss and life-threatening incidents"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our attack BADCODE features a special trigger generation and injection procedure, making the attack more effective and stealthy.",
          "Main Action": "By simply modifying one variable/function name",
          "Arguments": {
            "Agent": [
              "the attacker"
            ],
            "Object": {
              "Primary Object": [
                "buggy/vulnerable code"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "cybersecurity",
              "software vulnerabilities"
            ],
            "Purpose": [
              "To show that such attacks are feasible and stealthy"
            ],
            "Method": [
              "using a technique called \"BADCODE\" with a special trigger generation and injection procedure"
            ],
            "Results": [
              "the attacker can make buggy/vulnerable code rank in the top 11%"
            ],
            "Analysis": [
              "This demonstrates the effectiveness and stealthiness of the attack"
            ],
            "Challenge": [
              "ensuring stealthiness while maintaining effectiveness"
            ],
            "Ethical": [
              "exploiting vulnerabilities may have ethical concerns"
            ],
            "Implications": [
              "demonstrates potential risks in software security, necessitating stronger defense mechanisms"
            ],
            "Contradictions": [
              "None explicitly mentioned"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The evaluation is conducted on two neural code search models and the results show our attack outperforms baselines by 60%. Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score.",
          "Main Action": "Evaluation",
          "Arguments": {
            "Agent": [
              "Our team"
            ],
            "Object": {
              "Primary Object": [
                "Two neural code search models"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "To evaluate and compare performance against baselines"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "The results show our attack outperforms baselines by 60%",
              "Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "This indicates potential advancements in neural code search technology."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_835",
      "abstract": "To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources. Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness. This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization.",
          "Main Action": "create",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "pool of dependable annotators"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources.",
          "Main Action": "We investigate",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "high-quality Amazon Mechanical Turk workers"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "to successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources"
            ],
            "Method": [
              "a two-step pipeline"
            ],
            "Results": [
              "obtain high-agreement annotations with similar constraints on resources"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "By improving the recruitment process, we enhance the reliability and validity of the generated data."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness.",
          "Main Action": "Their alignment with expert judgments on a subset of the data is not as expected",
          "Arguments": {
            "Agent": [
              "Our workers",
              "CloudResearch workers"
            ],
            "Object": {
              "Primary Object": [
                "Expert judgments on a subset of the data"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "This comparison indicates performance gaps requiring attention"
            ],
            "Purpose": [
              "Assessing agreement between worker assessments and expert judgments"
            ],
            "Method": [
              "Comparison of worker consensus with expert judgments"
            ],
            "Results": [
              "Alignment with expert judgments is suboptimal"
            ],
            "Analysis": [
              "Discrepancies suggest areas needing refinement"
            ],
            "Challenge": [
              "Insufficient alignment complicates validation efforts"
            ],
            "Ethical": [
              "None explicitly mentioned"
            ],
            "Implications": [
              "Need for improved training protocols"
            ],
            "Contradictions": [
              "Unexpected divergence from expected standards"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.",
          "Main Action": "serves as",
          "Arguments": {
            "Agent": [
              "This paper"
            ],
            "Object": {
              "Primary Object": [
                "best practice"
              ],
              "Secondary Object": [
                "qualified annotators"
              ]
            },
            "Context": [
              "other challenging annotation tasks"
            ],
            "Purpose": [
              "guidance or example"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "success"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "reference for future research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}