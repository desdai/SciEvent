{
  "papers": [
    {
      "paper_code": "ACL_23_P_496",
      "abstract": "Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. In order to facilitate the development of SLU models that leverage the success of pre-trained speech representations, we will release a new benchmark suite, including for each task (i) curated annotations for a relatively small fine-tuning set, (ii) reproducible pipeline (speech recognizer + text model) and end-to-end baseline models and evaluation metrics, (iii) baseline model performance in various types of systems for easy comparisons. We present the details of data collection and annotation and the performance of the baseline models. We also analyze the sensitivity of pipeline models’ performance to the speech recognition accuracy, using more than 20 publicly available speech recognition models.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition.",
          "Main Action": "Have not received as much attention as lower-level tasks like speech and speaker recognition.",
          "Arguments": {
            "Agent": [
              "Speech research community"
            ],
            "Object": {
              "Primary Object": [
                "Spoken language understanding (SLU) tasks"
              ],
              "Secondary Object": [
                "Speech and speaker recognition"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. In order to facilitate the development of SLU models that leverage the success of pre-trained speech representations, we will release a new benchmark suite, including for each task (i) curated annotations for a relatively small fine-tuning set, (ii) reproducible pipeline (speech recognizer + text model) and end-to-end baseline models and evaluation metrics, (iii) baseline model performance in various types of systems for easy comparisons.",
          "Main Action": "Introducing several new annotated SLU benchmark tasks",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Several new annotated SLU benchmark tasks"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Addressing gaps in the SLU evaluation landscape"
            ],
            "Purpose": [
              "To facilitate the development of SLU models that leverage the success of pre-trained speech representations"
            ],
            "Method": [
              "Releasing a new benchmark suite",
              "Including curated annotations for a relatively small fine-tuning set",
              "A reproducible pipeline (speech recognizer + text model)",
              "Baseline model performance in various types of systems for easy comparisons"
            ],
            "Results": [
              "Introduction of several new annotated SLU benchmark tasks"
            ],
            "Analysis": [
              "Need for baseline models for comparisons"
            ],
            "Challenge": [
              "Providing baseline model performance for easy comparisons"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Facilitating better model development and evaluation"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "ERROR",
          "Text": "We present the details of data collection and annotation and the performance of the baseline models. We also analyze the sensitivity of pipeline models’ performance to the speech recognition accuracy, using more than 20 publicly available speech recognition models.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_828",
      "abstract": "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types.",
          "Main Action": "Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "their coverage of time spans or question types"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Reasoning about time is of fundamental importance. Many facts are time-dependent."
            ],
            "Purpose": [
              "To highlight the limitations of existing datasets and emphasize the need for unbiased alternatives"
            ],
            "Method": [
              "For example, athletes change teams from time to time, and different government officials are elected periodically."
            ],
            "Results": [
              "These datasets are biased either in their coverage of time spans or question types"
            ],
            "Analysis": [
              "This highlights the need for more comprehensive and unbiased datasets"
            ],
            "Challenge": [
              "The inherent bias in existing datasets limits their effectiveness"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Addressing this challenge will improve time-related research and applications"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning.",
          "Main Action": "We introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models and we propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning.",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a comprehensive probing dataset TempReason"
              ],
              "Secondary Object": [
                "a novel learning framework"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.",
          "Main Action": "conducted experiments",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "closed book QA",
                "open book QA",
                "reasoning QA settings"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "closed book QA",
              "open book QA",
              "reasoning QA settings"
            ],
            "Results": [
              "demonstrated the effectiveness of our approach"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}