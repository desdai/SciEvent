{
  "papers": [
    {
      "paper_code": "bioinfo_23_P_275",
      "abstract": "Researchers usually conduct statistical analyses based on models built on raw data collected from individual participants (individual-level data). There is a growing interest in enhancing inference efficiency by incorporating aggregated summary information from other sources, such as summary statistics on genetic markers' marginal associations with a given trait generated from genome-wide association studies. However, combining high-dimensional summary data with individual-level data using existing integrative procedures can be challenging due to various numeric issues in optimizing an objective function over a large number of unknown parameters. We develop a procedure to improve the fitting of a targeted statistical model by leveraging external summary data for more efficient statistical inference (both effect estimation and hypothesis testing). To make this procedure scalable to high-dimensional summary data, we propose a divide-and-conquer strategy by breaking the task into easier parallel jobs, each fitting the targeted model by integrating the individual-level data with a small proportion of summary data. We obtain the final estimates of model parameters by pooling results from multiple fitted models through the minimum distance estimation procedure. We improve the procedure for a general class of additive models commonly encountered in genetic studies. We further expand these two approaches to integrate individual-level and high-dimensional summary data from different study populations. We demonstrate the advantage of the proposed methods through simulations and an application to the study of the effect on pancreatic cancer risk by the polygenic risk score defined by BMI-associated genetic markers.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Researchers usually conduct statistical analyses based on models built on raw data collected from individual participants (individual-level data). There is a growing interest in enhancing inference efficiency by incorporating aggregated summary information from other sources, such as summary statistics on genetic markers' marginal associations with a given trait generated from genome-wide association studies. However, combining high-dimensional summary data with individual-level data using existing integrative procedures can be challenging due to various numeric issues in optimizing an objective function over a large number of unknown parameters.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "There is a growing interest"
            ],
            "Object": {
              "Primary Object": [
                "enhancing inference efficiency"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "combining high-dimensional summary data with individual-level data using existing integrative procedures"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "incorporating aggregated summary information from other sources",
              "due to various numeric issues in optimizing an objective function over a large number of unknown parameters"
            ],
            "Purpose": [
              "enhancing inference efficiency"
            ],
            "Method": [
              "using existing integrative procedures"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "due to various numeric issues in optimizing an objective function over a large number of unknown parameters"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We develop a procedure to improve the fitting of a targeted statistical model by leveraging external summary data for more efficient statistical inference (both effect estimation and hypothesis testing). To make this procedure scalable to high-dimensional summary data, we propose a divide-and-conquer strategy by breaking the task into easier parallel jobs, each fitting the targeted model by integrating the individual-level data with a small proportion of summary data. We obtain the final estimates of model parameters by pooling results from multiple fitted models through the minimum distance estimation procedure. We improve the procedure for a general class of additive models commonly encountered in genetic studies. We further expand these two approaches to integrate individual-level and high-dimensional summary data from different study populations.",
          "Main Action": "develop",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a procedure"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "to improve the fitting of a targeted statistical model by leveraging external summary data for more efficient statistical inference (both effect estimation and hypothesis testing)"
            ],
            "Purpose": [
              "improve the fitting of a targeted statistical model by leveraging external summary data for more efficient statistical inference (both effect estimation and hypothesis testing)"
            ],
            "Method": [
              "leverage external summary data; break the task into easier parallel jobs; pool results from multiple fitted models through the minimum distance estimation procedure"
            ],
            "Results": [
              "obtain the final estimates of model parameters"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "make this procedure scalable to high-dimensional summary data"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "expand these two approaches to integrate individual-level and high-dimensional summary data from different study populations"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We demonstrate the advantage of the proposed methods through simulations and an application to the study of the effect on pancreatic cancer risk by the polygenic risk score defined by BMI-associated genetic markers.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "advantage of the proposed methods"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "through simulations",
                "an application"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "study of the effect on pancreatic cancer risk by the polygenic risk score defined by BMI-associated genetic markers"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "simulations",
              "application"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "bioinfo_23_P_352",
      "abstract": "We describe a compression scheme for BUS files and an implementation of the algorithm in the BUStools software. Our compression algorithm yields smaller file sizes than gzip, at significantly faster compression and decompression speeds. We evaluated our algorithm on 533 BUS files from scRNA-seq experiments with a total size of 1TB. Our compression is 2.2x faster than the fastest gzip option, 35% slower than the fastest zstd option, and results in 1.5x smaller files than both methods. This amounts to an 8.3x reduction in the file size, resulting in a compressed size of 122GB for the dataset.",
      "events": [
        {
          "Methods/Approach": "",
          "Text": "We describe a compression scheme for BUS files and an implementation of the algorithm in the BUStools software.",
          "Main Action": "describe",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a compression scheme for BUS files"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "an implementation of the algorithm in the BUStools software"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "none provided"
            ],
            "Purpose": [
              "none specified"
            ],
            "Method": [
              "none mentioned"
            ],
            "Results": [
              "none reported"
            ],
            "Analysis": [
              "none given"
            ],
            "Challenge": [
              "none identified"
            ],
            "Ethical": [
              "none discussed"
            ],
            "Implications": [
              "none noted"
            ],
            "Contradictions": [
              "none presented"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our compression algorithm yields smaller file sizes than gzip, at significantly faster compression and decompression speeds. We evaluated our algorithm on 533 BUS files from scRNA-seq experiments with a total size of 1TB. Our compression is 2.2x faster than the fastest gzip option, 35% slower than the fastest zstd option, and results in 1.5x smaller files than both methods. This amounts to an 8.3x reduction in the file size, resulting in a compressed size of 122GB for the dataset.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "Our compression algorithm"
            ],
            "Object": {
              "Primary Object": [
                "smaller file sizes",
                "faster compression and decompression speeds"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "gzip",
                "zstd options",
                "scRNA-seq experiments"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "evaluated on 533 BUS files from scRNA-seq experiments with a total size of 1TB"
            ],
            "Purpose": [
              "compress data more efficiently"
            ],
            "Method": [
              "compression algorithm"
            ],
            "Results": [
              "yields smaller file sizes than gzip, at significantly faster compression and decompression speeds",
              "2.2x faster than the fastest gzip option, 35% slower than the fastest zstd option, and results in 1.5x smaller files than both methods",
              "an 8.3x reduction in the file size, resulting in a compressed size of 122GB for the dataset"
            ],
            "Analysis": [
              "none"
            ],
            "Challenge": [
              "none"
            ],
            "Ethical": [
              "none"
            ],
            "Implications": [
              "none"
            ],
            "Contradictions": [
              "none"
            ]
          }
        }
      ]
    }
  ]
}