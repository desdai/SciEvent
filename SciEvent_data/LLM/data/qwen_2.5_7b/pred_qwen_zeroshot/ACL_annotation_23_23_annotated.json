{
  "papers": [
    {
      "paper_code": "ACL_23_P_37",
      "abstract": "Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias. In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore, we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels. Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "Most existing multi-modal fake news detection methods"
            ],
            "Object": {
              "Primary Object": [
                "latent data bias"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "testing set",
                "training set"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "rapid upgrade of social platforms"
            ],
            "Purpose": [
              "neglecting the fact that some label-specific features learned from the training set cannot generalize well to the testing set"
            ],
            "Method": [
              "none mentioned"
            ],
            "Results": [
              "inherently suffer from the harm caused by the latent data bias"
            ],
            "Analysis": [
              "none provided"
            ],
            "Challenge": [
              "none specified"
            ],
            "Ethical": [
              "none discussed"
            ],
            "Implications": [
              "none given"
            ],
            "Contradictions": [
              "none identified"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore, we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels.",
          "Main Action": "<analyze and identify>",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "psycholinguistic bias",
                "bias of inferring news label based on only image features"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "in this paper"
            ],
            "Purpose": [
              "mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection"
            ],
            "Method": [
              "utilize causal intervention",
              "apply counterfactual reasoning"
            ],
            "Results": [
              "remove the psycholinguistic bias which introduces the spurious correlations between text features and news label",
              "eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels"
            ],
            "Analysis": [
              "imagining a counterfactual world where each news has only image features for estimating the direct effect of the image"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "improve multi-modal fake news detection accuracy"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "our framework"
            ],
            "Object": {
              "Primary Object": [
                "effectiveness"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "multi-modal fake news detection"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "two real-world benchmark datasets"
            ],
            "Purpose": [
              "improving multi-modal fake news detection"
            ],
            "Method": [
              "extensive experiments"
            ],
            "Results": [
              "effective"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_507",
      "abstract": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community.",
          "Main Action": "present",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "WinoQueer"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "a benchmark"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community"
            ],
            "Purpose": [
              "to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community"
            ],
            "Method": [
              "not mentioned"
            ],
            "Results": [
              "not mentioned"
            ],
            "Analysis": [
              "not mentioned"
            ],
            "Challenge": [
              "not mentioned"
            ],
            "Ethical": [
              "not mentioned"
            ],
            "Implications": [
              "not mentioned"
            ],
            "Contradictions": [
              "not mentioned"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias.",
          "Main Action": "apply",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "our benchmark"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "several popular LLMs"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "community-sourced, via application of a novel method that generates a bias benchmark from a community survey"
            ],
            "Purpose": [
              "find that off-the-shelf models generally do exhibit considerable anti-queer bias"
            ],
            "Method": [
              "application of a novel method that generates a bias benchmark from a community survey"
            ],
            "Results": [
              "off-the-shelf models generally do exhibit considerable anti-queer bias"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "LLM bias against a marginalized community"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                [
                  "finetuning on data written about or by members of that community"
                ],
                [
                  "social media text written by community members"
                ],
                [
                  "news text written about the community by non-members"
                ]
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "finetuning on data written about or by members of that community"
            ],
            "Results": [
              "LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",
          "Main Action": "provides",
          "Arguments": {
            "Agent": [
              "our method"
            ],
            "Object": {
              "Primary Object": [
                "a blueprint"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "for future researchers",
                "to develop community-driven, harms-grounded LLM benchmarks"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "community-in-the-loop benchmark development"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "other marginalized communities"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}