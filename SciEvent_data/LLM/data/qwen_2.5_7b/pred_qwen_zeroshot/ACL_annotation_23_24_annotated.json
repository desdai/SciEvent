{
  "papers": [
    {
      "paper_code": "ACL_23_P_378",
      "abstract": "A robust summarization system should be able to capture the gist of the document, regardless of the specific word choices or noise in the input. In this work, we first explore the summarization models’ robustness against perturbations including word-level synonym substitution and noise. To create semantic-consistent substitutes, we propose SummAttacker, which is an efficient approach to generating adversarial samples based on pre-trained language models. Experimental results show that state-of-the-art summarization models have a significant decrease in performance on adversarial and noisy test sets. Next, we analyze the vulnerability of the summarization systems and explore improving the robustness by data augmentation. Specifically, the first vulnerability factor we found is the low diversity of the training inputs. Correspondingly, we expose the encoder to more diverse cases created by SummAttacker in the input space. The second factor is the vulnerability of the decoder, and we propose an augmentation in the latent space of the decoder to improve its robustness. Concretely, we create virtual cases by manifold softmixing two decoder hidden states of similar semantic meanings. Experimental results on Gigaword and CNN/DM datasets demonstrate that our approach achieves significant improvements over strong baselines and exhibits higher robustness on noisy, attacked, and clean datasets.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "A robust summarization system should be able to capture the gist of the document, regardless of the specific word choices or noise in the input. In this work, we first explore the summarization models’ robustness against perturbations including word-level synonym substitution and noise.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "summarization models"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "perturbations including word-level synonym substitution and noise"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "a robust summarization system should be able to capture the gist of the document, regardless of the specific word choices or noise in the input"
            ],
            "Purpose": [
              "first explore the summarization models' robustness"
            ],
            "Method": [
              "explore"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To create semantic-consistent substitutes, we propose SummAttacker, which is an efficient approach to generating adversarial samples based on pre-trained language models. Experimental results show that state-of-the-art summarization models have a significant decrease in performance on adversarial and noisy test sets. Next, we analyze the vulnerability of the summarization systems and explore improving the robustness by data augmentation. Specifically, the first vulnerability factor we found is the low diversity of the training inputs. Correspondingly, we expose the encoder to more diverse cases created by SummAttacker in the input space. The second factor is the vulnerability of the decoder, and we propose an augmentation in the latent space of the decoder to improve its robustness. Concretely, we create virtual cases by manifold softmixing two decoder hidden states of similar semantic meanings.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "SummAttacker",
                "summarization models",
                "training inputs",
                "decoder hidden states"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "adversarial samples",
                "noisy test sets",
                "diverse cases",
                "latent space of the decoder",
                "virtual cases"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "to create semantic-consistent substitutes",
              "state-of-the-art summarization models have a significant decrease in performance on adversarial and noisy test sets",
              "vulnerability of the summarization systems",
              "improving the robustness by data augmentation"
            ],
            "Purpose": [
              "expose the encoder to more diverse cases",
              "find ways to make the system more robust against attacks",
              "create virtual cases by manifold softmixing two decoder hidden states of similar semantic meanings"
            ],
            "Method": [
              "propose SummAttacker",
              "analyze the vulnerability of the summarization systems",
              "data augmentation",
              "manifold softmixing"
            ],
            "Results": [
              "significant decrease in performance on adversarial and noisy test sets"
            ],
            "Analysis": [
              "first vulnerability factor is the low diversity of the training inputs",
              "second factor is the vulnerability of the decoder"
            ],
            "Challenge": [
              "none explicitly mentioned"
            ],
            "Ethical": [
              "none explicitly mentioned"
            ],
            "Implications": [
              "potential improvements in robustness through data augmentation methods"
            ],
            "Contradictions": [
              "none explicitly mentioned"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experimental results on Gigaword and CNN/DM datasets demonstrate that our approach achieves significant improvements over strong baselines and exhibits higher robustness on noisy, attacked, and clean datasets.",
          "Main Action": "achieves",
          "Arguments": {
            "Agent": [
              "our approach"
            ],
            "Object": {
              "Primary Object": [
                "significant improvements"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "strong baselines"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "on Gigaword and CNN/DM datasets"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "exhibits higher robustness on noisy, attacked, and clean datasets"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_113",
      "abstract": "We present ELQA, a corpus of questions and answers in and about the English language. Collected from two online forums, the >70k questions (from English learners and others) cover wide-ranging topics including grammar, meaning, fluency, and etymology. The answers include descriptions of general properties of English vocabulary and grammar as well as explanations about specific (correct and incorrect) usage examples. Unlike most NLP datasets, this corpus is metalinguistic—it consists of language about language. As such, it can facilitate investigations of the metalinguistic capabilities of NLU models, as well as educational applications in the language learning domain. To study this, we define a free-form question answering task on our dataset and conduct evaluations on multiple LLMs (Large Language Models) to analyze their capacity to generate metalinguistic answers.",
      "events": [
        {
          "Methods/Approach": "",
          "Text": "We present ELQA, a corpus of questions and answers in and about the English language. Collected from two online forums, the >70k questions (from English learners and others) cover wide-ranging topics including grammar, meaning, fluency, and etymology. The answers include descriptions of general properties of English vocabulary and grammar as well as explanations about specific (correct and incorrect) usage examples. Unlike most NLP datasets, this corpus is metalinguistic—it consists of language about language. As such, it can facilitate investigations of the metalinguistic capabilities of NLU models, as well as educational applications in the language learning domain. To study this, we define a free-form question answering task on our dataset and conduct evaluations on multiple LLMs (Large Language Models) to analyze their capacity to generate metalinguistic answers.",
          "Main Action": "present",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "ELQA",
                "corpus"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "questions",
                "answers"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "collected from two online forums",
              "covers wide-ranging topics including grammar, meaning, fluency, and etymology",
              "includes descriptions of general properties of English vocabulary and grammar as well as explanations about specific (correct and incorrect) usage examples"
            ],
            "Purpose": [
              "facilitate investigations of the metalinguistic capabilities of NLU models, as well as educational applications in the language learning domain"
            ],
            "Method": [
              "define a free-form question answering task on our dataset and conduct evaluations on multiple LLMs (Large Language Models) to analyze their capacity to generate metalinguistic answers"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}