{
  "papers": [
    {
      "paper_code": "ACL_23_P_671",
      "abstract": "Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations. In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which, as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations. We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY’s supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks. TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "existing datasets' limitations"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "temporal reasoning models"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "While temporal reasoning models can perform reasonably well on in-domain benchmarks"
            ],
            "Purpose": [
              "have little idea of these systems’ generalizability"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "due to existing datasets’ limitations"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which, as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations.",
          "Main Action": "introduce",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a novel task named TODAY"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "temporal differential analysis"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "In this work"
            ],
            "Purpose": [
              "evaluates whether systems can correctly understand the effect of incremental changes"
            ],
            "Method": [
              "Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions."
            ],
            "Results": [
              "TODAY also annotates human explanations"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY’s supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "existing models",
                "TODAY's supervision style and explanation annotations"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "random guessing",
                "several benchmarks"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "including GPT-3.5",
              "encouraging models to use more appropriate signals during training"
            ],
            "Purpose": [
              "outperform across several benchmarks"
            ],
            "Method": [
              "using joint learning"
            ],
            "Results": [
              "drop to random guessing",
              "outperform across several benchmarks"
            ],
            "Analysis": [
              "heavily rely on spurious information rather than proper reasoning for temporal predictions"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader significance or potential for future applications/research"
            ],
            "Contradictions": [
              "suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems.",
          "Main Action": "can also be used",
          "Arguments": {
            "Agent": [
              "TODAY"
            ],
            "Object": {
              "Primary Object": [
                "to train models"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "from noisy sources such as GPT-3.5"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "thus moving us more toward the goal of generic temporal reasoning systems"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "solicit incidental supervision"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_65",
      "abstract": "Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations. To address this issue, we propose a novel continual extraction model for analogous relations. Specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. We also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations. Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "Existing works"
            ],
            "Object": {
              "Primary Object": [
                "their performance"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "analogous relations"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "conduct an empirical study"
            ],
            "Results": [
              "performance is severely affected by analogous relations"
            ],
            "Analysis": [
              "We conduct an empirical study on existing works and observe that..."
            ],
            "Challenge": [
              "repeatedly replaying these samples may cause the overfitting problem"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To address this issue, we propose a novel continual extraction model for analogous relations. Specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. We also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a novel continual extraction model for analogous relations"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "to address this issue"
            ],
            "Purpose": [
              "enhance the performance on analogous relations"
            ],
            "Method": [
              "design memory-insensitive relation prototypes and memory augmentation",
              "introduce integrated training and focal knowledge distillation"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "overcome the overfitting problem"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "Experimental results"
            ],
            "Object": {
              "Primary Object": [
                "superiority of our model"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "effectiveness in distinguishing analogous relations and overcoming overfitting"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "superiority of our model",
              "effectiveness in distinguishing analogous relations and overcoming overfitting"
            ],
            "Analysis": [
              "demonstrate its effectiveness"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "overcoming overfitting"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}