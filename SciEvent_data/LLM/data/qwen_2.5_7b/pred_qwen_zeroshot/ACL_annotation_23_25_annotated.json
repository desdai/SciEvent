{
  "papers": [
    {
      "paper_code": "ACL_23_P_783",
      "abstract": "Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model’s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model’s label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks. The gain is substantial on tasks with large domain-label bias (up to 37% in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model’s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "in-context learning (ICL)",
                "model's predictions"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "design settings",
                "categorizing them",
                "mitigating their impact"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "studies discussing design choices"
            ],
            "Purpose": [
              "systematic investigations"
            ],
            "Method": [
              "none mentioned"
            ],
            "Results": [
              "few systematic investigations"
            ],
            "Analysis": [
              "none mentioned"
            ],
            "Challenge": [
              "none mentioned"
            ],
            "Ethical": [
              "none mentioned"
            ],
            "Implications": [
              "broader significance or potential for future applications/research"
            ],
            "Contradictions": [
              "none mentioned"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model’s label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks.",
          "Main Action": "<DEFINE A TYPOLOGY FOR LABEL BIASES>",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a typology for three types of label biases in ICL for text classification"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "vanilla-label bias, context-label bias, and domain-label bias"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "In this work",
              "Our analysis"
            ],
            "Purpose": [
              "to demonstrate that prior label bias calibration methods fall short of addressing all three types of biases"
            ],
            "Method": [
              "conceptualize and detect for the first time"
            ],
            "Results": [
              "our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "what is the broader significance or potential for future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "he gain is substantial on tasks with large domain-label bias (up to 37% in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "our results"
            ],
            "Object": {
              "Primary Object": [
                "models with different scales, pretraining methods, and manually-designed task instructions"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "tasks with large domain-label bias"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "showing the prevalence of label biases in ICL"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "generalize...to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_139",
      "abstract": "Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs. Inspired by routing-based dynamic network, we model the dynamic mechanism in multimodal sarcasm detection and propose the Dynamic Routing Transformer Network (DynRT-Net). Our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity. Experimental results on a public dataset demonstrate the effectiveness of our method compared to the state-of-the-art methods.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "Most existing studies"
            ],
            "Object": {
              "Primary Object": [
                "incongruity between image and text"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "multimodal sarcasm identification"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "an important research topic in natural language processing and multimedia computing",
              "benefits a wide range of applications in multiple domains"
            ],
            "Purpose": [
              "regard...as the indicative clue"
            ],
            "Method": [
              "rely on fixed architectures in network design"
            ],
            "Results": [
              "restrict the model from dynamically adjusting to diverse image-text pairs"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Inspired by routing-based dynamic network, we model the dynamic mechanism in multimodal sarcasm detection and propose the Dynamic Routing Transformer Network (DynRT-Net). Our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity.",
          "Main Action": "model",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "dynamic mechanism in multimodal sarcasm detection"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "Dynamic Routing Transformer Network (DynRT-Net)"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Inspired by routing-based dynamic network"
            ],
            "Purpose": [
              "utilize dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity"
            ],
            "Method": [
              "Dynamic Routing Transformer Network (DynRT-Net)"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experimental results on a public dataset demonstrate the effectiveness of our method compared to the state-of-the-art methods.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "our method"
            ],
            "Object": {
              "Primary Object": [
                "effectiveness"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "state-of-the-art methods"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "public dataset"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "experimental results"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}