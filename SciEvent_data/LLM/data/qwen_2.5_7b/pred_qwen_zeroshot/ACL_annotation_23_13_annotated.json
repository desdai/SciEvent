{
  "papers": [
    {
      "paper_code": "ACL_23_P_210",
      "abstract": "Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017).",
          "Main Action": "has been hindered",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "extension to multilingual settings"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017)"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "relation extraction (RE) is a fundamental task in information extraction"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios.",
          "Main Action": "introduce",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "MultiTACRED dataset"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "to address this gap",
              "covering 12 typologically diverse languages from 9 language families"
            ],
            "Purpose": [
              "create by machine-translating TACRED instances and automatically projecting their entity annotations"
            ],
            "Method": [
              "machine-translating TACRED instances and automatically projecting their entity annotations"
            ],
            "Results": [
              "analyze translation and annotation projection quality, identify error categories, experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "Our analyses"
            ],
            "Object": {
              "Primary Object": [
                "that machine translation is a viable strategy to transfer RE instances"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "native speakers juding [more than 83% of the translated instances to be linguistically and semantically acceptable]"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable."
            ],
            "Purpose": [
              "to compare monolingual RE model performance to the English original for many of the target languages,"
            ],
            "Method": [
              "We find...multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts."
            ],
            "Results": [
              "and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts.",
              "However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, that degrade dataset quality and RE model performance."
            ],
            "Analysis": [
              "degrade dataset quality and RE model performance."
            ],
            "Challenge": [
              "a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages,"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "outperform their monolingual counterparts."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_222",
      "abstract": "In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. Our contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training. We evaluate ContProto on multiple transfer pairs, and experimental results show our method brings substantial improvements over current state-of-the-art methods.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "self-training"
            ],
            "Object": {
              "Primary Object": [
                "pseudo-labeled target-language data"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "target languages"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "cross-lingual named entity recognition (NER)",
              "sub-optimal performance on target languages",
              "noisy pseudo labels"
            ],
            "Purpose": [
              "bridging the linguistic gap"
            ],
            "Method": [
              "training on pseudo-labeled target-language data"
            ],
            "Results": [
              "limited overall performance"
            ],
            "Analysis": [
              "none provided"
            ],
            "Challenge": [
              "noise in pseudo-labels affecting performance"
            ],
            "Ethical": [
              "none mentioned"
            ],
            "Implications": [
              "none discussed"
            ],
            "Contradictions": [
              "none presented"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. Our contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training.",
          "Main Action": "aim",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "to improve self-training for cross-lingual NER"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "by combining representation learning and pseudo label refinement in one coherent framework"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "in this work"
            ],
            "Purpose": [
              "improve self-training for cross-lingual NER"
            ],
            "Method": [
              "combining representation learning and pseudo label refinement in one coherent framework"
            ],
            "Results": [
              "enhances cross-lingual transferability by producing closely-aligned representations between the source and target language",
              "effectively improves the accuracy of pseudo labels during training"
            ],
            "Analysis": [
              "Our contrastive self-training facilitates span classification by separating clusters of different classes"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We evaluate ContProto on multiple transfer pairs, and experimental results show our method brings substantial improvements over current state-of-the-art methods.",
          "Main Action": "evaluate",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "ContProto"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "multiple transfer pairs"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "on multiple transfer pairs"
            ],
            "Purpose": [
              "bring substantial improvements over current state-of-the-art methods"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "experimental results show our method brings substantial improvements over current state-of-the-art methods"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}