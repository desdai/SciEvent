{
  "papers": [
    {
      "paper_code": "ACL_23_P_147",
      "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with “Let’s think step by step” as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with “Let’s think step by step” as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "Few-shot chain-of-thought (CoT) prompting",
              "Zero-shot-CoT"
            ],
            "Object": {
              "Primary Object": [
                "LLMs",
                "target problem statement"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "manually crafted step-by-step reasoning demonstrations",
                "\"Let's think step by step\""
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "recently been shown to deliver impressive performance in various NLP tasks",
              "Despite the success of Zero-shot-CoT, it still suffers from three pitfalls"
            ],
            "Purpose": [
              "to tackle multi-step reasoning tasks",
              "to eliminate the manual efforts"
            ],
            "Method": [
              "includes a few manually crafted step-by-step reasoning demonstrations",
              "concatenates the target problem statement with \"Let's think step by step\" as an input prompt"
            ],
            "Results": [
              "improve their reasoning task accuracy"
            ],
            "Analysis": [
              "calculation errors, missing-step errors, and semantic misunderstanding errors"
            ],
            "Challenge": [
              "three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "eliminating manual effort while improving reasoning accuracy may lead to more efficient natural language processing systems"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "Plan-and-Solve (PS) Prompting"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "missing-step errors"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "to address the missing-step errors"
            ],
            "Purpose": [
              "to address the missing-step errors"
            ],
            "Method": [
              "consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan"
            ],
            "Results": [
              "extend PS prompting with more detailed instructions and derive PS+ prompting"
            ],
            "Analysis": [
              "improve the quality of generated reasoning steps"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem.",
          "Main Action": "evaluate",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "our proposed prompting strategy"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "ten datasets across three reasoning problems"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "on ten datasets across three reasoning problems"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin",
              "is comparable to or exceeds Zero-shot-Program-of-Thought Prompting",
              "has comparable performance with 8-shot CoT prompting on the math reasoning problem"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_111",
      "abstract": "It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student’s generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher’s learning process. By prioritizing samples that are likely to enhance the student’s generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student’s generalization ability.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "concept of distillation influence"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "impact of distillation from each training sample on the student's generalization ability"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "a discrepancy between current teacher training practices and effective knowledge transfer"
            ],
            "Purpose": [
              "enhance the guidance of the teacher training process"
            ],
            "Method": [
              "introduce [the concept of distillation influence]"
            ],
            "Results": [
              "determine the impact of distillation from each training sample on the student's generalization ability"
            ],
            "Analysis": [
              "highlighting a discrepancy between current teacher training practices and effective knowledge transfer"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "improve understanding of how to effectively train teachers"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher’s learning process. By prioritizing samples that are likely to enhance the student’s generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "Learning Good Teacher Matters (LGTM)"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "an efficient training technique"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "incorporating distillation influence into the teacher's learning process"
            ],
            "Purpose": [
              "outperforming 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark"
            ],
            "Method": [
              "prioritizing samples that are likely to enhance the student's generalization ability"
            ],
            "Results": [
              "our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}