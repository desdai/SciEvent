{
  "papers": [
    {
      "paper_code": "ACL_23_P_365",
      "abstract": "Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching signal between two texts, which is the essence of information retrieval). In this paper, we propose a novel method to improve the generalization of dense retrieval via capturing matching signal called BERM. Fully fine-grained expression and query-oriented saliency are two properties of the matching signal. Thus, in BERM, a single passage is segmented into multiple units and two unit-level requirements are proposed for representation as the constraint in training to obtain the effective matching signal. One is semantic unit balance and the other is essential matching unit extractability. Unit-level view and balanced semantics make representation express the text in a fine-grained manner. Essential matching unit extractability makes passage representation sensitive to the given query to extract the pure matching information from the passage containing complex context. Experiments on BEIR show that our method can be effectively combined with different dense retrieval training methods (vanilla, hard negatives mining and knowledge distillation) to improve its generalization ability without any additional inference overhead and target domain data.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching signal between two texts, which is the essence of information retrieval).",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "previous studies"
            ],
            "Object": {
              "Primary Object": [
                "dense retrieval"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "unseen domains"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "trained on in-domain labeled datasets.",
              "due to its weak modeling of domain-invariant and interpretable feature"
            ],
            "Purpose": [
              "hard to generalize"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "weak modeling of domain-invariant and interpretable feature"
            ],
            "Analysis": [
              "matching signal between two texts, which is the essence of information retrieval"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we propose a novel method to improve the generalization of dense retrieval via capturing matching signal called BERM. Fully fine-grained expression and query-oriented saliency are two properties of the matching signal. Thus, in BERM, a single passage is segmented into multiple units and two unit-level requirements are proposed for representation as the constraint in training to obtain the effective matching signal. One is semantic unit balance and the other is essential matching unit extractability. Unit-level view and balanced semantics make representation express the text in a fine-grained manner. Essential matching unit extractability makes passage representation sensitive to the given query to extract the pure matching information from the passage containing complex context.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a novel method to improve the generalization of dense retrieval via capturing matching signal called BERM"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "in this paper,"
            ],
            "Purpose": [
              "to improve the generalization of dense retrieval"
            ],
            "Method": [
              "segmenting a single passage into multiple units and proposing two unit-level requirements for representation as the constraint in training"
            ],
            "Results": [
              "obtain the effective matching signal"
            ],
            "Analysis": [
              "Fully fine-grained expression and query-oriented saliency are two properties of the matching signal; Unit-level view and balanced semantics make representation express the text in a fine-grained manner; Essential matching unit extractability makes passage representation sensitive to the given query to extract the pure matching information from the passage containing complex context"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experiments on BEIR show that our method can be effectively combined with different dense retrieval training methods (vanilla, hard negatives mining, and knowledge distillation) to improve its generalization ability without any additional inference overhead and target domain data.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "our method"
            ],
            "Object": {
              "Primary Object": [
                "can be effectively combined with different dense retrieval training methods (vanilla, hard negatives mining, and knowledge distillation)"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "to improve its generalization ability without any additional inference overhead and target domain data"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "on BEIR"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "without any additional inference overhead and target domain data"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_502",
      "abstract": "Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P – that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE). We have observed that models trained in this way may “over-generalize”, in the sense that they produce non-human-like text. Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P relative to Q, is a better reflection of how a human would evaluate text generated by a model. Hence, we propose learning with MixCE, an objective that mixes the forward and reverse cross-entropies. We evaluate models trained with this objective on synthetic data settings (where P is known) and real data, and show that the resulting models yield better generated text without complex decoding strategies.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P – that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE).",
          "Main Action": "are trained",
          "Arguments": {
            "Agent": [
              "autoregressive language models"
            ],
            "Object": {
              "Primary Object": [
                "minimizing the cross-entropy of the model distribution Q relative to the data distribution P"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "minimizing the forward cross entropy, which is equivalent to maximum likelihood estimation (MLE)"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "by"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": " We have observed that models trained in this way may “over-generalize”, in the sense that they produce non-human-like text. Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P relative to Q, is a better reflection of how a human would evaluate text generated by a model. Hence, we propose learning with MixCE, an objective that mixes the forward and reverse cross-entropies.",
          "Main Action": "observe",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "models trained in this way may over-generalize"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "produce non-human-like text"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "in the sense that they produce non-human-like text"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P relative to Q, is a better reflection of how a human would evaluate text generated by a model"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Hence, we propose learning with MixCE, an objective that mixes the forward and reverse cross-entropies"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We evaluate models trained with this objective on synthetic data settings (where P is known) and real data, and show that the resulting models yield better generated text without complex decoding strategies.",
          "Main Action": "evaluate",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "models trained with this objective"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "synthetic data settings (where P is known)",
                "real data"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "on synthetic data settings (where P is known) and real data,"
            ],
            "Purpose": [
              "show that the resulting models yield better generated text without complex decoding strategies"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "yield better generated text without complex decoding strategies"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}