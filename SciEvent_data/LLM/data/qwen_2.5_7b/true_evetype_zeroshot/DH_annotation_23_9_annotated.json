{
  "papers": [
    {
      "paper_code": "dh_23_P_05",
      "abstract": "In this paper, we present a case study on quality criteria for the robustness of categories in pragmalinguistic tagset development. We model a number of classification tasks for linguistic routines of discourse referencing in the plenary minutes of the German Bundestag. In the process, we focus and reflect on three fundamental quality criteria: 1. segmentation, i.e. size of the annotated segments (e.g. words, phrases or sentences), 2. granularity, i.e. degrees of content differentiation and 3. interpretation depth, i.e. the degree of inclusion of linguistic knowledge, co-textual knowledge and extra-linguistic, context-sensitive knowledge. With the machine learnability of categories in mind, our focus is on principles and conditions of category development in collaborative annotation. Our experiments and tests on pilot corpora aim to investigate to which extent statistical measures indicate whether interpretative classifications are machine-reproducible and reliable. To this end, we compare gold-standard datasets annotated with different segment sizes (phrases, sentences) and categories with different granularity, respectively. We conduct experiments with different machine learning frameworks to automatically predict labels from our tagset. We apply BERT ([Devlin et al. 2019]), a pre-trained neural transformer language model which we finetune and constrain for our labelling and classification tasks, and compare it against Naive Bayes as a probabilistic knowledge-agnostic baseline model. The results from these experiments contribute to the development and reflection of our category systems.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "In this paper, we present a case study on quality criteria for the robustness of categories in pragmalinguistic tagset development. We model a number of classification tasks for linguistic routines of discourse referencing in the plenary minutes of the German Bundestag.",
          "Main Action": "present",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a case study"
              ],
              "Secondary Object": [
                "quality criteria for the robustness of categories in pragmalinguistic tagset development",
                "classification tasks for linguistic routines of discourse referencing"
              ]
            },
            "Context": [
              "case study on quality criteria for the robustness of categories in pragmalinguistic tagset development"
            ],
            "Purpose": [
              "modeling a number of classification tasks for linguistic routines of discourse referencing in the plenary minutes of the German Bundestag"
            ],
            "Method": [
              "none specified directly but implies modeling based on plenary minutes data"
            ],
            "Results": [
              "none specified directly"
            ],
            "Analysis": [
              "none specified directly"
            ],
            "Challenge": [
              "none specified directly"
            ],
            "Ethical": [
              "none specified directly"
            ],
            "Implications": [
              "none specified directly"
            ],
            "Contradictions": [
              "none specified directly"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In the process, we focus and reflect on three fundamental quality criteria: 1. segmentation, i.e. size of the annotated segments (e.g. words, phrases or sentences), 2. granularity, i.e. degrees of content differentiation and 3. interpretation depth, i.e. the degree of inclusion of linguistic knowledge, co-textual knowledge and extra-linguistic, context-sensitive knowledge. With the machine learnability of categories in mind, our focus is on principles and conditions of category development in collaborative annotation. Our experiments and tests on pilot corpora aim to investigate to which extent statistical measures indicate whether interpretative classifications are machine-reproducible and reliable. To this end, we compare gold-standard datasets annotated with different segment sizes (phrases, sentences) and categories with different granularity, respectively. We conduct experiments with different machine learning frameworks to automatically predict labels from our tagset. We apply BERT ([Devlin et al. 2019]), a pre-trained neural transformer language model which we finetune and constrain for our labelling and classification tasks, and compare it against Naive Bayes as a probabilistic knowledge-agnostic baseline model.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "our team"
            ],
            "Object": {
              "Primary Object": [
                "principles",
                "conditions"
              ],
              "Secondary Object": [
                "category development",
                "collaborative annotation"
              ]
            },
            "Context": [
              "segmentation",
              "granularity",
              "interpretation depth"
            ],
            "Purpose": [
              "investigate to which extent statistical measures indicate whether interpretative classifications are machine-reproducible and reliable"
            ],
            "Method": [
              "conducting experiments",
              "tests on pilot corpora",
              "comparing gold-standard datasets annotated with different segment sizes (phrases, sentences)",
              "categories with different granularity",
              "applying BERT",
              "finetuning and constraining BERT for labeling and classification tasks",
              "conducting experiments with different machine learning frameworks",
              "comparing results against Naive Bayes as a probabilistic knowledge-agnostic baseline model"
            ],
            "Results": [
              "machine-reproducibility and reliability of interpretative classifications indicated by statistical measures"
            ],
            "Analysis": [
              "comparison between different segment sizes and granularities",
              "evaluation of BERT's performance compared to Naive Bayes"
            ],
            "Challenge": [
              "none explicitly mentioned"
            ],
            "Ethical": [
              "none explicitly mentioned"
            ],
            "Implications": [
              "potential for improving automated annotation processes through better understanding of human categorization practices"
            ],
            "Contradictions": [
              "none explicitly mentioned"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The results from these experiments contribute to the development and reflection of our category systems.",
          "Main Action": "contribute",
          "Arguments": {
            "Agent": [
              "our category systems"
            ],
            "Object": {
              "Primary Object": [
                "development"
              ],
              "Secondary Object": [
                "reflection"
              ]
            },
            "Context": [
              "from these experiments"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "dh_23_P_38",
      "abstract": "Sound studies in general, and voice studies in particular, present particular challenges for digital humanities scholarship. The software tools available to digital humanists who want to study performative speech are less familiar and less developed for our uses, and the user base is also much smaller than for text mining or network analysis. This article provides a critical narrative of our research and an outline of our methodology, in applying, developing and refining tools for the analysis of pitch and timing patterns in recorded performances of literary texts. The primary texts and audio considered are poetry readings, but the tools and methods can and have been applied more widely to podcasts, talking books, political speeches, etc.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Sound studies in general, and voice studies in particular, present particular challenges for digital humanities scholarship. The software tools available to digital humanists who want to study performative speech are less familiar and less developed for our uses, and the user base is also much smaller than for text mining or network analysis.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "digital humanists"
            ],
            "Object": {
              "Primary Object": [
                "software tools",
                "user base"
              ],
              "Secondary Object": [
                "text mining or network analysis"
              ]
            },
            "Context": [
              "The software tools available to digital humanists who want to study performative speech",
              "less familiar and less developed for our uses",
              "much smaller than for text mining or network analysis"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "This article provides a critical narrative of our research and an outline of our methodology, in applying, developing and refining tools for the analysis of pitch and timing patterns in recorded performances of literary texts.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "our research"
            ],
            "Object": {
              "Primary Object": [
                "methodology"
              ],
              "Secondary Object": [
                "tools for the analysis of pitch and timing patterns in recorded performances of literary texts"
              ]
            },
            "Context": [
              "applying, developing and refining"
            ],
            "Purpose": [
              "providing a critical narrative"
            ],
            "Method": [
              "applying, developing and refining"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The primary texts and audio considered are poetry readings, but the tools and methods can and have been applied more widely to podcasts, talking books, political speeches, etc.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "tools and methods"
            ],
            "Object": {
              "Primary Object": [
                "primary texts and audio"
              ],
              "Secondary Object": [
                "poetry readings",
                "podcasts",
                "talking books",
                "political speeches"
              ]
            },
            "Context": [
              "considered are poetry readings"
            ],
            "Purpose": [
              "can and have been applied more widely to podcasts, talking books, political speeches, etc."
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}