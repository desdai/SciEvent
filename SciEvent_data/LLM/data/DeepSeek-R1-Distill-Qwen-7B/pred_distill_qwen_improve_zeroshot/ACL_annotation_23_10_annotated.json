{
  "papers": [
    {
      "paper_code": "ACL_23_P_312",
      "abstract": "Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ~10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Primary Modifier": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ],
              "Secondary Modifier": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification",
          "Main Action": "treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs)",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "large language models (LLMs)"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "event schemas"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification."
            ],
            "Purpose": [
              "proposing a new paradigm to simplify schema induction and manage hierarchical and temporal relations between events effectively"
            ],
            "Method": [
              "designing an incremental prompting and verification method IncPrompt to decompose complex event graphs into manageable stages"
            ],
            "Results": [
              "No explicit results extracted"
            ],
            "Analysis": [
              "This approach addresses challenges such as handling hierarchical and temporal relations efficiently"
            ],
            "Challenge": [
              "Managing complex graph structures and ensuring accurate relationship verifications remain difficult tasks"
            ],
            "Ethical": [
              "No explicit ethical considerations discussed"
            ],
            "Implications": [
              "Potential advancements in relational reasoning and application-specific improvements"
            ],
            "Contradictions": [
              "None evident in the text"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ~10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.",
          "Main Action": "achieve",
          "Arguments": {
            "Agent": [
              "Large language models (LLMs)",
              "Previous state-of-the-art closed-domain schema induction model"
            ],
            "Object": {
              "Primary Object": [
                "a 7.2% F1 improvement in temporal relations",
                "a 31.0% F1 improvement in hierarchical relations"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Compared to directly using LLMs to generate a linearized graph",
              "human assessors were able to cover ~10% more events when translating the schemas into coherent stories"
            ],
            "Purpose": [
              "demonstrate superiority of their approach over baseline methods",
              "assess comprehensiveness and readability of generated schemas"
            ],
            "Method": [
              "comparison study",
              "human evaluation metrics"
            ],
            "Results": [
              "7.2% F1 improvement in temporal relations",
              "31.0% F1 improvement in hierarchical relations",
              "rated our schemas 1.3 points higher on a 5-point scale in terms of readability"
            ],
            "Analysis": [
              "Our approach outperforms alternative methods in capturing complex relationships within schemas.",
              "Human assessors prefer our schemas due to enhanced clarity."
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "opens new avenues for schema induction systems",
              "provides insights into evaluating AI-generated relational structures"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_550",
      "abstract": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as “lions don’t live in the ocean”, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs. Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as “lions don’t live in the ocean”, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Primary Modifier": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ],
              "Secondary Modifier": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.",
          "Main Action": "design",
          "Arguments": {
            "Agent": [
              "they [their work]",
              "we [our efforts]"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "examine capabilities like reasoning, structured questioning compared to free-form response"
            ],
            "Method": [
              "design tasks focusing on constrained keyword extraction and boolean question answering"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "current model capacities constrain successful implementation"
            ],
            "Ethical": [
              "exploring bias in prompt framing affects linguistic diversity understanding"
            ],
            "Implications": [
              "advancing evaluation accuracy for llm development"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "We conducted experiments showing failures in generating valid sentences grounded in negative commonsense knowledge while accurately answering polar yes-or-no questions."
            ],
            "Object": {
              "Primary Object": [
                "LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions."
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "They exhibit a phenomenon termed 'belief conflict' attributed to statistical shortcuts and negation reporting bias from language model pre-training."
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict."
            ],
            "Purpose": [
              "To investigate discrepancies between LLM performance across different tasks and identify the underlying causes of these inconsistencies."
            ],
            "Method": [
              "Through systematic experimentation involving various aspects of LLM functioning including generation capabilities and evaluation metrics."
            ],
            "Results": [
              "Observations include frequent failures in generating valid sentences grounded in negative commonsense knowledge despite accurate responses to simple binary queries. A novel phenomenon named 'belief conflict' was identified characterized by inconsistent reasoning patterns influenced by training biases."
            ],
            "Analysis": [
              "Analyzed the role of statistical shortcuts and negation handling mechanisms acquired during pre-training phases contributing to the observed inconsistency."
            ],
            "Challenge": [
              "Identifying precise causal factors linking specific linguistic features to task performance remains complex requiring nuanced examination of both architectural design and learning dynamics."
            ],
            "Ethical": [
              " raises questions regarding fairness and reliability of AI systems particularly concerning their ability to handle nuanced logical reasoning required for real-world decision-making processes."
            ],
            "Implications": [
              "Understanding these challenges opens doors for developing mitigation strategies enhancing transparency in AI reasoning pathways potentially improving downstream applications reliant on reliable NLP technologies."
            ],
            "Contradictions": [
              "No contradictions evident within the presented framework aligning closely with established theories of machine learning and natural language processing."
            ]
          }
        }
      ]
    }
  ]
}