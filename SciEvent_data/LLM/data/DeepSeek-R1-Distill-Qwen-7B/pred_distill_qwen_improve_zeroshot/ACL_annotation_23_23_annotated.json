{
  "papers": [
    {
      "paper_code": "ACL_23_P_37",
      "abstract": "Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias. In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore, we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels. Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias.",
          "Main Action": "neglect",
          "Arguments": {
            "Agent": [
              "existing multi-modal fake news detection methods"
            ],
            "Object": {
              "Primary Object": [
                "they cannot generalize well to the testing set"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "thus inevitably suffering from the harm caused by the latent data bias"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Most existing multi-modal fake news detection methods neglect the fact"
            ],
            "Purpose": [
              "to explain the limitation causing problems in real-world scenarios through examples given."
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "suffering from the harm caused by the latent data bias"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore, we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels.",
          "Main Action": "We analyze",
          "Arguments": {
            "Agent": [
              "we analyze"
            ],
            "Object": {
              "Primary Object": [
                "psycholinguistic bias in the text and the bias of inferring news label based on only image features"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection."
            ],
            "Purpose": [
              "To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore, we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels."
            ],
            "Method": [
              "utilize causal intervention, imagine a counterfactual world, estimate the direct effect of the image"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "Our framework"
            ],
            "Object": {
              "Primary Object": [
                "Multi-modal fake news detection"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Two real-world benchmark datasets"
            ],
            "Purpose": [
              "Demonstrating the effectiveness of our framework for improving multi-modal fake news detection"
            ],
            "Method": [
              "Conducting extensive experiments"
            ],
            "Results": [
              "Confirming the effectiveness of our framework for improving multi-modal fake news detection"
            ],
            "Analysis": [
              "None mentioned"
            ],
            "Challenge": [
              "None mentioned"
            ],
            "Ethical": [
              "None mentioned"
            ],
            "Implications": [
              "Suggests exploring enhanced detection mechanisms through similar experimental approaches"
            ],
            "Contradictions": [
              "None mentioned"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_507",
      "abstract": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community.",
          "Main Action": "present",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "\"Wino Queer\""
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "measure whether LLMs encode biases that are harmful to the LGBTQ+ community"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias.",
          "Main Action": "application of",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "novel method"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "community survey"
            ],
            "Purpose": [
              "validate against several popular large language models"
            ],
            "Method": [
              "applied our benchmark to several popular LLMs"
            ],
            "Results": [
              "find that off-the-shelf models generally do exhibit considerable anti-queer bias"
            ],
            "Analysis": [
              "conclude that off-the-shelf models have substantial issues regarding queerness representation"
            ],
            "Challenge": [
              "no specific challenge mentioned"
            ],
            "Ethical": [
              "no specific ethical concern mentioned"
            ],
            "Implications": [
              "imply the need for bias-aware language models development"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "The study team"
            ],
            "Object": {
              "Primary Object": [
                "finetuning on data written about or by members of that community"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "social media text written by community members compared to news text written about the community by non-members"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members."
            ],
            "Purpose": [
              "To demonstrate mitigation strategies against LLM bias towards marginalized communities using specific types of training data and compare their effectiveness across platforms."
            ],
            "Method": [
              "Not explicitly mentioned in the provided context; extraction rules state methods aren't always included unless clearly specified."
            ],
            "Results": [
              "Not explicitly mentioned in the provided context; results aren't extracted unless they're specifically described."
            ],
            "Analysis": [
              "Not explicitly mentioned in the provided context; analyses require explicit statements which weren't given."
            ],
            "Challenge": [
              "No challenges were presented in the original text; extraction focuses strictly on content present."
            ],
            "Ethical": [
              "No ethical considerations discussed in the provided text; all ethrics would come from additional sources not listed here."
            ],
            "Implications": [
              "No broader implications beyond the immediate study scope; implications usually arise from wider impacts not detailed here."
            ],
            "Contradictions": [
              "No contradictions identified within the text; such points wouldn't exist without being stated."
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",
          "Main Action": "Our method",
          "Arguments": {
            "Agent": [
              "community-in-the-loop benchmark development"
            ],
            "Object": {
              "Primary Object": [
                "blueprint for future researchers"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "to provide guidance"
            ],
            "Method": [
              "machine learning methods",
              "collaborative framework",
              "community engagement strategies",
              "evaluation metrics aligned with harm reduction principles"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "contributions toward equitable AI system design",
              "enhancing representativeness",
              "addressing systemic inequities"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}