{
  "papers": [
    {
      "paper_code": "bioinfo_23_P_275",
      "abstract": "Researchers usually conduct statistical analyses based on models built on raw data collected from individual participants (individual-level data). There is a growing interest in enhancing inference efficiency by incorporating aggregated summary information from other sources, such as summary statistics on genetic markers' marginal associations with a given trait generated from genome-wide association studies. However, combining high-dimensional summary data with individual-level data using existing integrative procedures can be challenging due to various numeric issues in optimizing an objective function over a large number of unknown parameters. We develop a procedure to improve the fitting of a targeted statistical model by leveraging external summary data for more efficient statistical inference (both effect estimation and hypothesis testing). To make this procedure scalable to high-dimensional summary data, we propose a divide-and-conquer strategy by breaking the task into easier parallel jobs, each fitting the targeted model by integrating the individual-level data with a small proportion of summary data. We obtain the final estimates of model parameters by pooling results from multiple fitted models through the minimum distance estimation procedure. We improve the procedure for a general class of additive models commonly encountered in genetic studies. We further expand these two approaches to integrate individual-level and high-dimensional summary data from different study populations. We demonstrate the advantage of the proposed methods through simulations and an application to the study of the effect on pancreatic cancer risk by the polygenic risk score defined by BMI-associated genetic markers.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Researchers usually conduct statistical analyses based on models built on raw data collected from individual participants (individual-level data). There is a growing interest in enhancing inference efficiency by incorporating aggregated summary information from other sources, such as summary statistics on genetic markers' marginal associations with a given trait generated from genome-wide association studies. However, combining high-dimensional summary data with individual-level data using existing integrative procedures can be challenging due to various numeric issues in optimizing an objective function over a large number of unknown parameters.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "statistical analyses"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "based on models built on raw data collected from individual participants (individual-level data)"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "There is a growing interest in enhancing inference efficiency",
              "combining high-dimensional summary data with individual-level data"
            ],
            "Purpose": [
              "to incorporate aggregated summary information from other sources",
              "due to various numeric issues in optimizing an objective function over a large number of unknown parameters."
            ],
            "Method": [
              "existing integrative procedures"
            ],
            "Results": [
              "However, combining ... presents significant challenges."
            ],
            "Analysis": [
              "The integration process faces several obstacles including computational complexity and parameter estimation difficulties."
            ],
            "Challenge": [
              "Various numeric issues arise during optimization processes affecting model performance."
            ],
            "Ethical": [
              "No mention of ethical considerations specifically mentioned in the provided text."
            ],
            "Implications": [
              "This issue has broad implications for improving efficient data integration across diverse domains such as genetics and public health."
            ],
            "Contradictions": [
              "No contradictions presented within the analyzed text."
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We develop a procedure to improve the fitting of a targeted statistical model by leveraging external summary data for more efficient statistical inference (both effect estimation and hypothesis testing). To make this procedure scalable to high-dimensional summary data, we propose a divide-and-conquer strategy by breaking the task into easier parallel jobs, each fitting the targeted model by integrating the individual-level data with a small proportion of summary data. We obtain the final estimates of model parameters by pooling results from multiple fitted models through the minimum distance estimation procedure. We improve the procedure for a general class of additive models commonly encountered in genetic studies. We further expand these two approaches to integrate individual-level and high-dimensional summary data from different study populations.",
          "Main Action": "Develop a procedure",
          "Arguments": {
            "Agent": [
              "Us [researchers]"
            ],
            "Object": {
              "Primary Object": [
                "a procedure"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "to improve the fitting of a targeted statistical model by leveraging external summary data for more efficient statistical inference"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "high-dimensional summary data",
              "genetic studies"
            ],
            "Purpose": [
              "aim to provide accurate effect estimation and hypothesis testing",
              "manage computational demands in genomic data analyses"
            ],
            "Method": [
              "divide-and-conquer strategy",
              "minimum distance estimation procedure",
              "individual-level data with summary data",
              "parallel jobs",
              "generalized linear models"
            ],
            "Results": [
              "improvements in computational speed",
              "maintaining analytical accuracy despite limited summary stats"
            ],
            "Analysis": [
              "application scope in genetic disease research",
              "personalized medicine"
            ],
            "Challenge": [
              "reliable integration of individual and summarized data",
              "sample size and representativeness"
            ],
            "Ethical": [
              "data provenance",
              "potential biases in summary datasets"
            ],
            "Implications": [
              "beyond genetic studies",
              "big data analytics projects",
              "resource utilization"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We demonstrate the advantage of the proposed methods through simulations and an application to the study of the effect on pancreatic cancer risk by the polygenic risk score defined by BMI-associated genetic markers.",
          "Main Action": "demonstrate the advantage",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "proposed methods"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "polygenic risk score defined by BMI-associated genetic markers"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "study of pancreatic cancer risk"
            ],
            "Purpose": [
              "showing effectiveness of new methods"
            ],
            "Method": [
              "simulation and application to real data"
            ],
            "Results": [
              "advantages over previous methods"
            ],
            "Analysis": [
              "explanations of findings"
            ],
            "Challenge": [
              "potential limitations"
            ],
            "Ethical": [
              "no mention"
            ],
            "Implications": [
              "future applications in oncology"
            ],
            "Contradictions": [
              "none mentioned"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "bioinfo_23_P_352",
      "abstract": "We describe a compression scheme for BUS files and an implementation of the algorithm in the BUStools software. Our compression algorithm yields smaller file sizes than gzip, at significantly faster compression and decompression speeds. We evaluated our algorithm on 533 BUS files from scRNA-seq experiments with a total size of 1TB. Our compression is 2.2x faster than the fastest gzip option, 35% slower than the fastest zstd option, and results in 1.5x smaller files than both methods. This amounts to an 8.3x reduction in the file size, resulting in a compressed size of 122GB for the dataset.",
      "events": [
        {
          "Methods/Approach": "",
          "Text": "We describe a compression scheme for BUS files and an implementation of the algorithm in the BUStools software.",
          "Main Action": "Describe",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<BUS files>",
                "<BUStools software>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<The compression scheme for BUS files and an implementation of the algorithm in the BUStools software>"
            ],
            "Purpose": [
              "Improving data handling efficiency for BUS files compared to generic methods"
            ],
            "Method": [
              "Utilizing standard algorithms adapted for BUS file specifics and integrating them into BUStools framework"
            ],
            "Results": [
              "Observation of improved compression rates and reduced processing times"
            ],
            "Analysis": [
              "Interpretation of the effectiveness of the compression approach and comparison with benchmarked methods"
            ],
            "Challenge": [
              "Maintaining accuracy during compression and ensuring system compatibility with existing file formats"
            ],
            "Ethical": [
              "Supports responsible usage preserving critical information precision in genomic studies"
            ],
            "Implications": [
              "Potential extensions to similar data types managed via BUS files and influence future research directions in big data management"
            ],
            "Contradictions": [
              "No direct contradictions noted with existing knowledge"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our compression algorithm yields smaller file sizes than gzip, at significantly faster compression and decompression speeds. We evaluated our algorithm on 533 BUS files from scRNA-seq experiments with a total size of 1TB. Our compression is 2.2x faster than the fastest gzip option, 35% slower than the fastest zstd option, and results in 1.5x smaller files than both methods. This amounts to an 8.3x reduction in the file size, resulting in a compressed size of 122GB for the dataset.",
          "Main Action": "Our compression algorithm yields smaller file sizes than gzip",
          "Arguments": {
            "Agent": [
              "Our compression algorithm"
            ],
            "Object": {
              "Primary Object": [
                "file sizes"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "gzip"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}