{
  "papers": [
    {
      "paper_code": "ACL_23_P_540",
      "abstract": "Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our attack BADCODE features a special trigger generation and injection procedure, making the attack more effective and stealthy. The evaluation is conducted on two neural code search models and the results show our attack outperforms baselines by 60%. Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents.",
          "Main Action": "an adversary can inject",
          "Arguments": {
            "Agent": [
              "neural code search models"
            ],
            "Object": {
              "Primary Object": [
                "code with security/privacy issues"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "neural code search models"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "These models are based on deep learning and gain substantial attention due to their impressive performance.",
              "However, the security aspect of these models is rarely studied.",
              "particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues."
            ],
            "Purpose": [
              "This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents."
            ],
            "Method": [
              "No specific method details are provided in the given text snippet."
            ],
            "Results": [
              "Adversaries can exploit these models to produce malicious code that affects downstream software, potentially leading to significant consequences."
            ],
            "Analysis": [
              "Injecting a backdoor allows attackers to manipulate the model's output to introduce vulnerabilities or errors specifically targeting certain functionalities."
            ],
            "Challenge": [
              "There is limited exploration regarding the security aspects of these models."
            ],
            "Ethical": [
              "This scenario raises serious ethical concerns regarding the safety and reliability of AI systems in critical applications."
            ],
            "Implications": [
              "Studying and mitigating such vulnerabilities becomes crucial to ensure robustness across various domains reliant on these technologies."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our attack BADCODE features a special trigger generation and injection procedure, making the attack more effective and stealthy.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "attackers"
            ],
            "Object": {
              "Primary Object": [
                "attacks"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "vulnerabilities"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "By simply modifying one variable/function name"
            ],
            "Purpose": [
              "effectiveness",
              "stealthiness"
            ],
            "Method": [
              "trigger generation",
              "injection procedure"
            ],
            "Results": [
              "top 11%"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "practical security measures"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The evaluation is conducted on two neural code search models and the results show our attack outperforms baselines by 60%. Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score.",
          "Main Action": "outperforms",
          "Arguments": {
            "Agent": [
              "our attack"
            ],
            "Object": {
              "Primary Object": [
                "the baselines"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "by 60%"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "two neural code search models"
            ],
            "Purpose": [
              "demonstrate effectiveness of the attack"
            ],
            "Method": [
              "compare attacks across models using unspecified framework"
            ],
            "Results": [
              "our attack outperforms baselines by 60%"
            ],
            "Analysis": [
              "interpretation of why the attack performs better"
            ],
            "Challenge": [
              "no apparent challenges mentioned"
            ],
            "Ethical": [
              "no ethical issues discussed"
            ],
            "Implications": [
              "improves understanding of attack effectiveness"
            ],
            "Contradictions": [
              "none identified"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_835",
      "abstract": "To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources. Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness. This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization.",
          "Main Action": "create",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "A pool of dependable annotators"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "who can effectively complete difficult tasks"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "To prevent the costly and inefficient use of resources on low-quality annotations"
            ],
            "Purpose": [
              "We want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Constraints include ensuring reliability among annotators with varying expertise and managing time-intensive evaluation tasks"
            ],
            "Ethical": [
              "Considerations include fairness, accuracy, and maintaining trustworthiness in evaluations, especially for AI-generated outputs"
            ],
            "Implications": [
              "This approach has broad implications for improving performance metrics in computational intelligence and influencing subsequent research efforts aimed at building upon this framework"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "subpar workers"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "evaluations"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "a two-step pipeline"
            ],
            "Purpose": [
              "successfully filter out subpar workers"
            ],
            "Method": [
              "obtain high-agreement annotations"
            ],
            "Results": [
              "constraints on resources"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "Their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness."
            ],
            "Object": {
              "Primary Object": [
                "their alignment"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "with expert judgments on a subset of the data"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Although our workers demonstrate a strong consensus among themselves and CloudResearch workers"
            ],
            "Purpose": [
              "needs further training in correctness"
            ],
            "Method": [
              "not as expected"
            ],
            "Results": [
              "Further training is needed"
            ],
            "Analysis": [
              "This indicates a discrepancy between current performance and desired standards"
            ],
            "Challenge": [
              "They require additional effort to align with expert opinions"
            ],
            "Ethical": [
              "No mention of ethical issues here"
            ],
            "Implications": [
              "Potential need for improved training programs"
            ],
            "Contradictions": [
              "None mentioned"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.",
          "Main Action": "provides guidance",
          "Arguments": {
            "Agent": [
              "This paper"
            ],
            "Object": {
              "Primary Object": [
                "a best practice"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "for the recruitment"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "in other challenging annotation tasks"
            ],
            "Purpose": [
              "improve recruitment effectiveness"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}