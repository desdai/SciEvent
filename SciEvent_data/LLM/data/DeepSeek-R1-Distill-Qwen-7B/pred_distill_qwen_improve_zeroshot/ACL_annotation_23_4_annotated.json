{
  "papers": [
    {
      "paper_code": "ACL_23_P_392",
      "abstract": "Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale’s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs’ ability to generate rationales with better human utility, while maintaining most of its task performance.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging.",
          "Main Action": "highlighting",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "machine-generated rationales"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "gold rationales",
                "LM outputs"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We show that, by estimating a rationale’s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs’ ability to generate rationales with better human utility, while maintaining most of its task performance.",
          "Main Action": "Proposing",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "Gen-U"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Evaluating machine-generated rationales"
            ],
            "Purpose": [
              "Advancing methods for assessing rationales beyond current metrics",
              "Providing a practical scoring mechanism"
            ],
            "Method": [
              "Statistical analysis",
              "Algorithm development within NLP frameworks"
            ],
            "Results": [
              "Improved model generation abilities tied specifically to enhanced human utility assessments"
            ],
            "Analysis": [
              "Comparing previous evaluation metrics against the new approach",
              "Demonstrating effectiveness across various datasets",
              "Highlighting consistent improvements over baseline implementations"
            ],
            "Challenge": [
              "Ensuring computational efficiency",
              "Accurately capturing nuances in human evaluations"
            ],
            "Ethical": [
              "Fairness in evaluating diverse perspectives"
            ],
            "Implications": [
              "Broader impact on theoretical understanding",
              "Real-world applications involving dialogue agents"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_12",
      "abstract": "Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as ‘José le prometió/ordenó a María ser ordenado/a’ (‘Joseph promised/ordered Mary to be tidy’). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically-guided antecedent retrieval processes. Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity. Additional experiments on Galician reinforce these conclusions. Our findings are equally valuable for the evaluation of language models’ ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as ‘José le prometió/ordenó a María ser ordenado/a’ (‘Joseph promised/ordered Mary to be tidy’). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically-guided antecedent retrieval processes.",
          "Main Action": "Using psycholinguistic and computational experiments",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "Spanish sentences"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "control dependencies"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "psycholinguistic tests",
              "computational modeling"
            ],
            "Purpose": [
              "evaluate capabilities",
              "advance understanding of NLP systems"
            ],
            "Method": [
              "psycholinguistic experiments",
              "computational experiments"
            ],
            "Results": [
              "similarities",
              "differences in performance",
              "variations depending on construction"
            ],
            "Analysis": [
              "underlying reasons",
              "limitations in human/cognitive processes",
              "algorithmic approaches"
            ],
            "Challenge": [
              "complexities in handling dependencies",
              "ensuring accurate mapping of syntactic roles",
              "varying linguistic constructs"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "advancing NLP technologies",
              "contributing to grammar comprehension",
              "enhancing real-world applications"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity. Additional experiments on Galician reinforce these conclusions.",
          "Main Action": "Our results show",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "while humans correctly identify the (un)acceptability of the strings"
            ],
            "Purpose": [
              "to demonstrate the capability of humans and evaluate machine learning models"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Our results show that while humans correctly identify the (un)acceptability of the strings..."
            ],
            "Analysis": [
              "Additionally, experiments on Galician confirm these findings."
            ],
            "Challenge": [
              "However, current methods rely heavily on linear processing assumptions"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "This suggests room for improvement in NLP models regarding dependency parsing"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our findings are equally valuable for the evaluation of language models’ ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "Our findings"
            ],
            "Object": {
              "Primary Object": [
                "evaluation of language models’ ability to capture linguistic generalizations"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "psycholinguistic theories of anaphor resolution"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "dual purposes relating to evaluation of language models and psycholinguistic theories"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Our findings are equally valuable"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader significance or potential for future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}