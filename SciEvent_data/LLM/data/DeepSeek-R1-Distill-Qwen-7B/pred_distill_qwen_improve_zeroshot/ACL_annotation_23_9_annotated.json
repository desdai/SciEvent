{
  "papers": [
    {
      "paper_code": "ACL_23_P_02",
      "abstract": "One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SafeConv for the research of conversational safety: (1) Besides the utterance-level safety labels, SafeConv also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SafeConv provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. By virtue of the comprehensive annotation of SafeConv, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior.",
          "Main Action": "addressing",
          "Arguments": {
            "Agent": [
              "existing dialogue datasets"
            ],
            "Object": {
              "Primary Object": [
                "unsafe behavior"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "toxic languages",
                "harmful suggestions"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "open-domain end-to-end dialogue systems",
              "聊天机器人"
            ],
            "Purpose": [
              "improving safety measures",
              "ensuring safe interaction"
            ],
            "Method": [
              "developing new annotation standards",
              "machine learning models"
            ],
            "Results": [
              "reducing instances of toxicity",
              "increasing user trust"
            ],
            "Analysis": [
              "current annotation practices fall short",
              "inconsistencies affect performance"
            ],
            "Challenge": [
              "complexity of accurate assessment",
              "algorithmic bias"
            ],
            "Ethical": [
              "prioritizing user safety",
              "preventing algorithmic bias"
            ],
            "Implications": [
              "more reliable AI conversation platforms",
              "global adoption of annotation guidelines"
            ],
            "Contradictions": [
              "no evident contradictions"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we construct a new dataset called SafeConv for the research of conversational safety: (1) Besides the utterance-level safety labels, SafeConv also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SafeConv provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. By virtue of the comprehensive annotation of SafeConv, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Primary Modifier": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ],
              "Secondary Modifier": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent.",
          "Main Action": "Show",
          "Arguments": {
            "Agent": [
              "Experiments"
            ],
            "Object": {
              "Primary Object": [
                "unsafe behavior"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "popular chatbots"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "unsafe spans"
            ],
            "Purpose": [
              "addressing issues concerning harmful content"
            ],
            "Method": [
              "analyzing unsafe spans",
              "applying detoxification strategies extensively"
            ],
            "Results": [
              "understanding the root causes behind unsafe behaviors",
              "successfully scaled detox measures"
            ],
            "Analysis": [
              "interpreting the effectiveness of these methods compared to previous solutions"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "gaining insights leading to enhanced safety mechanisms",
              "scalable solutions applicable beyond current contexts"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_85",
      "abstract": "Out-of-distribution (OOD) detection, a fundamental task vexing real-world applications, has attracted growing attention in the NLP community. Recently fine-tuning based methods have made promising progress. However, it could be costly to store fine-tuned models for each scenario. In this paper, we depart from the classic fine-tuning based OOD detection toward a parameter-efficient alternative, and propose an unsupervised prefix-tuning based OOD detection framework termed PTO. Additionally, to take advantage of optional training data labels and targeted OOD data, two practical extensions of PTO are further proposed. Overall, PTO and its extensions offer several key advantages of being lightweight, easy-to-reproduce, and theoretically justified. Experimental results show that our methods perform comparably to, even better than, existing fine-tuning based OOD detection approaches under a wide range of metrics, detection settings, and OOD types.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Out-of-distribution (OOD) detection, a fundamental task vexing real-world applications, has attracted growing attention in the NLP community. Recently fine-tuning based methods have made promising progress. However, it could be costly to store fine-tuned models for each scenario.",
          "Main Action": "However",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "It could be costly"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "to store fine-tuned models for each scenario"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Out-of-distribution (OOD) detection ... has attracted growing attention in the NLP community.",
              "Recently fine-tuning based methods have made promising progress."
            ],
            "Purpose": [
              "What is the purpose or aim of the event?"
            ],
            "Method": [
              "Methods used in the event"
            ],
            "Results": [
              "Prior work highlights success in improving accuracy over traditional static models."
            ],
            "Analysis": [
              "Factors contributing to high costs ..."
            ],
            "Challenge": [
              "Challenges with storing models ..."
            ],
            "Ethical": [
              "No direct ethical concern mentioned."
            ],
            "Implications": [
              "Necessitating development of new approaches ..."
            ],
            "Contradictions": [
              "Potential questions about benefit-cost ratio ..."
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we depart from the classic fine-tuning based OOD detection toward a parameter-efficient alternative, and propose an unsupervised prefix-tuning based OOD detection framework termed PTO. Additionally, to take advantage of optional training data labels and targeted OOD data, two practical extensions of PTO are further proposed. Overall, PTO and its extensions offer several key advantages of being lightweight, easy-to-reproduce, and theoretically justified.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "[this paper]"
            ],
            "Object": {
              "Primary Object": [
                "PTO"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experimental results show that our methods perform comparably to, even better than, existing fine-tuning based OOD detection approaches under a wide range of metrics, detection settings, and OOD types.",
          "Main Action": "Experimental results",
          "Arguments": {
            "Agent": [
              "Our Methods"
            ],
            "Object": {
              "Primary Object": [
                "a wide range of metrics, detection settings, and OOD types"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "existing fine-tuning based OOD detection approaches"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "under a wide range of metrics, detection settings, and OOD types"
            ],
            "Purpose": [
              "evaluating and validating new approaches in machine learning"
            ],
            "Method": [
              "comparing across several parameters including metrics, settings, and OOD types"
            ],
            "Results": [
              "perform comparably to, even better than"
            ],
            "Analysis": [
              "detection settings, and OOD types"
            ],
            "Challenge": [
              "none"
            ],
            "Ethical": [
              "none"
            ],
            "Implications": [
              "broader applicability in machine learning tasks"
            ],
            "Contradictions": [
              "none"
            ]
          }
        }
      ]
    }
  ]
}