{
  "papers": [
    {
      "paper_code": "ACL_23_P_210",
      "abstract": "Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017).",
          "Main Action": "Extension",
          "Arguments": {
            "Agent": [
              "Relation Extraction",
              "Extension"
            ],
            "Object": {
              "Primary Object": [
                "Multilingual settings"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "available comparable in size to large English datasets such as TACRED (Zhang et al., 2017)."
            ],
            "Purpose": [
              "aiming to extend Relation Extraction tasks into multilingual settings"
            ],
            "Method": [
              "leverage existing resources and adapt evaluation metrics"
            ],
            "Results": [
              "improved performance metrics across various languages"
            ],
            "Analysis": [
              "strategies explored to address resource limitations"
            ],
            "Challenge": [
              "adequate resources comparable in size to large English datasets"
            ],
            "Ethical": [
              "no explicit mentions"
            ],
            "Implications": [
              "potential for global applications and advancements in AI-driven solutions"
            ],
            "Contradictions": [
              "none mentioned"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios.",
          "Main Action": "Introduce",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "MultiTACRED dataset"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
          "Main Action": "We find",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "monolingual RE model performance"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "multilingual models trained on a combination of English and target language data"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Evaluations across various target languages"
            ],
            "Purpose": [
              "To compare effectiveness of machine translation versus monolingual/multilingual approaches"
            ],
            "Method": [
              "Statistical analysis and comparison of model performances"
            ],
            "Results": [
              "Performance metrics showing comparable success rates and improved scores"
            ],
            "Analysis": [
              "Insights indicating trade-offs between system complexity and accuracy levels"
            ],
            "Challenge": [
              "Consistencies in translating RE instances affecting overall efficiency"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Potential optimizations in translation processes leveraging available resources effectively"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_222",
      "abstract": "In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. Our contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training. We evaluate ContProto on multiple transfer pairs, and experimental results show our method brings substantial improvements over current state-of-the-art methods.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance.",
          "Main Action": "pseudo labels are often noisy",
          "Arguments": {
            "Agent": [
              "model(s) during self-training phase"
            ],
            "Object": {
              "Primary Object": [
                "noisy pseudo labels"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "limited performance"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "due to sub-optimal performance on target languages"
            ],
            "Purpose": [
              "improve accuracy of cross-lingual Named Entity Recognition systems"
            ],
            "Method": [
              "self-supervised learning strategy involving creation of labeled datasets from unlabeled data"
            ],
            "Results": [
              "performance is limited"
            ],
            "Analysis": [
              "understanding reasons behind noisy labels"
            ],
            "Challenge": [
              "poor performance despite effective self-training approach"
            ],
            "Ethical": [
              "ethical considerations surrounding fair representation and trustworthiness in AI-based language mapping"
            ],
            "Implications": [
              "broad impact on improving multilingual natural language processing technologies"
            ],
            "Contradictions": [
              "none provided in the text"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. Our contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training.",
          "Main Action": "Our contrastive self-training",
          "Arguments": {
            "Agent": [
              "Our proposed method named ContPrim"
            ],
            "Object": {
              "Primary Object": [
                "Our contrastive self-training",
                "Span classification tasks"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "Cross-lingual transferability",
                "Proximity between source and target language representations"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Previous models have struggled with sufficient data for cross-lingual generalization.",
              "This limitation motivated us to develop new strategies."
            ],
            "Purpose": [
              "To improve self-training for cross-lingual NER."
            ],
            "Method": [
              "Combining representation learning and pseudo label refinement in a unified framework."
            ],
            "Results": [
              "Improved cross-lingual transferability.",
              "Enhanced pseudo label accuracy."
            ],
            "Analysis": [
              "Facilitating clear distinction between different class clusters.",
              "Balancing computational efficiency with prediction accuracy.",
              "Managing linguistic diversity."
            ],
            "Challenge": [
              "Ensuring non-overlapping clusters for effective classification.",
              "Maintaining balance between resource demands and predictive power.",
              "Addressing linguistic variations."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Advances in multilingual annotation systems.",
              "Expanded applicability of Named Entity Recognition."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We evaluate ContProto on multiple transfer pairs, and experimental results show our method brings substantial improvements over current state-of-the-art methods.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Primary Modifier": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ],
              "Secondary Modifier": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          }
        }
      ]
    }
  ]
}