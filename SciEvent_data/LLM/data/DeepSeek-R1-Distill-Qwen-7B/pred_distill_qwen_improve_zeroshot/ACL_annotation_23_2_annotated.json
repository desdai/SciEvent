{
  "papers": [
    {
      "paper_code": "ACL_23_P_420",
      "abstract": "Weir has defined a hierarchy of language classes whose second member (L2) is generated by tree-adjoining grammars (TAG), linear indexed grammars (LIG), combinatory categorial grammars, and head grammars. The hierarchy is obtained using the mechanism of control, and L2 is obtained using a context-free grammar (CFG) whose derivations are controlled by another CFG. We adapt Weir’s definition of a controllable CFG (called a labeled distinguished CFG) to give a definition of controllable pushdown automata (PDAs), called labeled distinguished PDAs. This yields three new characterizations of L2 as the class of languages generated by PDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs. We show that these four formalisms are not only weakly equivalent but equivalent in a stricter sense that we call d-weak equivalence. Furthermore, using an even stricter notion of equivalence called d-strong equivalence, we make precise the intuition that a CFG controlling a CFG is a TAG, a PDA controlling a PDA is an embedded PDA, and a PDA controlling a CFG is a LIG. The fourth member of this family, a CFG controlling a PDA, does not correspond to any kind of automaton we know of, so we invent one and call it a Pushdown Adjoining Automaton (PAA).",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Weir has defined a hierarchy of language classes whose second member (L2) is generated by tree-adjoining grammars (TAG), linear indexed grammars (LIG), combinatory categorial grammars, and head grammars. The hierarchy is obtained using the mechanism of control, and L2 is obtained using a context-free grammar (CFG) whose derivations are controlled by another CFG.",
          "Main Action": "defined",
          "Arguments": {
            "Agent": [
              "Weir"
            ],
            "Object": {
              "Primary Object": [
                "hierarchy"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "mechanism of control, another CFG"
            ],
            "Purpose": [
              "create a structured framework connecting grammar types"
            ],
            "Method": [
              "two-step controlled derivation processes, context-free grammars"
            ],
            "Results": [
              "established relationships between grammar families"
            ],
            "Analysis": [
              "precise regulation of derivations explains connections"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "better understanding of linguistic structures, aids in translation studies, cognitive linguistics"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We adapt Weir’s definition of a controllable CFG (called a labeled distinguished CFG) to give a definition of controllable pushdown automata (PDAs), called labeled distinguished PDAs. This yields three new characterizations of L2 as the class of languages generated by PDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We show that these four formalisms are not only weakly equivalent but equivalent in a stricter sense that we call d-weak equivalence. Furthermore, using an even stricter notion of equivalence called d-strong equivalence, we make precise the intuition that a CFG controlling a CFG is a TAG, a PDA controlling a PDA is an embedded PDA, and a PDA controlling a CFG is a LIG. The fourth member of this family, a CFG controlling a PDA, does not correspond to any kind of automaton we know of, so we invent one and call it a Pushdown Adjoining Automaton (PAA).",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "These four formalisms"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "are not only weakly equivalent but equivalent in a stricter sense that we call d-weak equivalence."
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "We show that these four formalisms are not only weakly equivalent but equivalent in a stricter sense that we call d-weak equivalence.",
              "The fourth member of this family, a CFG controlling a PDA, does not correspond to any kind of automaton we know of, so we invent one and call it a Pushdown Adjoining Automaton (PAA)."
            ],
            "Purpose": [
              "To establish equivalence relations among formalisms and introduce a new automaton when necessary."
            ],
            "Method": [
              "Showing equivalence relations through definitions and introducing a new model when needed."
            ],
            "Results": [
              "They make precise the intuition that...",
              "they do not correspond to any kind of automaton we know of, so we invent one."
            ],
            "Analysis": [
              "This shows the hierarchy and relationships between CFGs, PDAs, TAGs, and LIGs."
            ],
            "Challenge": [
              "None mentioned specifically."
            ],
            "Ethical": [
              "None mentioned specifically."
            ],
            "Implications": [
              "Expands our understanding of computational models and grammar hierarchies."
            ],
            "Contradictions": [
              "None presented."
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_216",
      "abstract": "End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model’s performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model’s performance closely correlates with its embedding similarity between speech and source transcript.",
          "Main Action": "Observe",
          "Arguments": {
            "Agent": [
              "researchers conducted experiments"
            ],
            "Object": {
              "Primary Object": [
                "ST model's performance"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "embedding similarity between speech and source transcript"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning.",
          "Main Action": "Proposed",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "Word-Aligned COntrastive learning"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "Speech-to-text translation system"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning."
            ],
            "Purpose": [
              "To design a method suitable for extremely low-resource speech-to-text translation"
            ],
            "Method": [
              "Contr shade learning applied across speech and text modalities"
            ],
            "Results": [
              "Achieved comparable accuracy rates against prior art"
            ],
            "Analysis": [
              "Effectively handled resource limitations through efficient training strategies"
            ],
            "Challenge": [
              "Addressed the challenge of scarce labeled data despite extensive pre-training"
            ],
            "Ethical": [
              "No significant ethical concerns were identified"
            ],
            "Implications": [
              "Opens up opportunities for similar systems in other resource-constrained settings"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data.",
          "Main Action": "WACO outperformed the best baseline",
          "Arguments": {
            "Agent": [
              "IWSLT 2023 Maltese-English dataset and MuST-C benchmark dataset"
            ],
            "Object": {
              "Primary Object": [
                "BLEU score improvement of 9+ points"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "only 1-hour parallel ST data"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "evaluation of WACO and other methods on two datasets"
            ],
            "Purpose": [
              "demonstrate superior performance compared to current state-of-the-art methods"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "WACO achieved significant improvements in translation quality"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "paves way for future exploration of efficient training strategies"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}