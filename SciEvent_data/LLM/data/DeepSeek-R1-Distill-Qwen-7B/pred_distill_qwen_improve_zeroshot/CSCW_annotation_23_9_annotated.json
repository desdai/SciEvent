{
  "papers": [
    {
      "paper_code": "cscw_23_P_97",
      "abstract": "The spread of misinformation on social media is a pressing societal problem that platforms, policymakers, and researchers continue to grapple with. As a countermeasure, recent works have proposed to employ non-expert fact-checkers in the crowd to fact-check social media content. While experimental studies suggest that crowds might be able to accurately assess the veracity of social media content, an understanding of how crowd fact-checked (mis-)information spreads is missing. In this work, we empirically analyze the spread of misleading vs. not misleading community fact-checked posts on social media. For this purpose, we employ a dataset of community-created fact-checks from Twitter's 'Birdwatch' pilot and map them to resharing cascades on Twitter. Different from earlier studies analyzing the spread of misinformation listed on third-party fact-checking websites (e.g., snopes.com), we find that community fact-checked misinformation is less viral. Specifically, misleading posts are estimated to receive 36.62% fewer retweets than not misleading posts. A partial explanation may lie in differences in the fact-checking targets: community fact-checkers tend to fact-check posts from influential user accounts with many followers, while expert fact-checks tend to target posts that are shared by less influential users. We further find that there are significant differences in virality across different sub-types of misinformation (e.g., factual errors, missing context, manipulated media). Moreover, we conduct a user study to assess the perceived reliability of (real-world) community-created fact-checks. Here, we find that users, to a large extent, agree with community-created fact-checks. Altogether, our findings offer insights into how misleading vs. not misleading posts spread and highlight the crucial role of sample selection when studying misinformation on social media.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "The spread of misinformation on social media is a pressing societal problem that platforms, policymakers, and researchers continue to grapple with. As a countermeasure, recent works have proposed to employ non-expert fact-checkers in the crowd to fact-check social media content. While experimental studies suggest that crowds might be able to accurately assess the veracity of social media content, an understanding of how crowd fact-checked (mis-)information spreads is missing.",
          "Main Action": "analyze how crowd fact-checked (mis-)information spreads",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "spread of (mis)-information"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "crowd behavior"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "The spread of misinformation on social media is a pressing societal problem that platforms, policymakers, and researchers continue to grapple with."
            ],
            "Purpose": [
              "To understand the dynamics behind the spread of fact-checked (mis)-information in order to design effective interventions."
            ],
            "Method": [
              "Conducted empirical studies including controlled experiments on social media platforms to observe how verified versus unverified content spreads among users."
            ],
            "Results": [
              "Experimental studies suggest that crowds might be capable of accurately assessing the veracity of social media content under certain conditions."
            ],
            "Analysis": [
              "While early evidence supports accurate assessment capabilities, significant challenges remain in predicting and controlling the spread of verified content due to complex human behaviors inherent in online interactions."
            ],
            "Challenge": [
              "Lack of comprehensive models explaining the dynamic interplay between verified/unverified content sharing and user engagement creates barriers to scalable intervention strategies."
            ],
            "Ethical": [
              "Potential risks exist in designing algorithms that manipulate public perception, necessitating careful consideration of ethical usage guidelines."
            ],
            "Implications": [
              "Understanding these dynamics has broad significance for improving online communication safety, combating misinformation, and enhancing public trust in digital platforms."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we empirically analyze the spread of misleading vs. not misleading community fact-checked posts on social media. For this purpose, we employ a dataset of community-created fact-checks from Twitter's 'Birdwatch' pilot and map them to resharing cascades on Twitter. Different from earlier studies analyzing the spread of misinformation listed on third-party fact-checking websites (e.g., snopes.com), we find that community fact-checked misinformation is less viral.",
          "Main Action": "empirically analyze",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a dataset of community-created fact-checks from Twitter's 'Birdwatch' pilot"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "resharing cascades on Twitter"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "We compare the spread of misleading vs. not misleading community fact-checked posts on social media."
            ],
            "Purpose": [
              "To evaluate the effectiveness of community fact-checking mechanisms compared to external platforms regarding misinformation spread."
            ],
            "Method": [
              "We collect data from the Birdwatch pilot and map them to resharing cascades on Twitter."
            ],
            "Results": [
              "Community fact-checked misinformation is less viral."
            ],
            "Analysis": [
              "This finding highlights differences in misinformation dynamics between internal and external verification processes."
            ],
            "Challenge": [
              "Limitations include reliance on a single dataset and potential generalizability issues."
            ],
            "Ethical": [
              "Potential implications involve balancing public trust metrics with privacy considerations."
            ],
            "Implications": [
              "It offers insights into leveraging internal community oversight for better control of misinformation."
            ],
            "Contradictions": [
              "None identified."
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Specifically, misleading posts are estimated to receive 36.62% fewer retweets than not misleading posts. A partial explanation may lie in differences in the fact-checking targets: community fact-checkers tend to fact-check posts from influential user accounts with many followers, while expert fact-checks tend to target posts that are shared by less influential users. We further find that there are significant differences in virality across different sub-types of misinformation (e.g., factual errors, missing context, manipulated media). Moreover, we conduct a user study to assess the perceived reliability of (real-world) community-created fact-checks. Here, we find that users, to a large extent, agree with community-created fact-checks.",
          "Main Action": "receive fewer retweets",
          "Arguments": {
            "Agent": [
              "specific types of posts such as misleading versus non-misleading posts"
            ],
            "Object": {
              "Primary Object": [
                "number of retweets received by misleading posts compared to non-misleading posts"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "overall retweet behavior influenced by factors including influence of posting accounts"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Specifically, misleading posts are estimated to receive 36.62% fewer retweets than not misleading posts."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Altogether, our findings offer insights into how misleading vs. not misleading posts spread and highlight the crucial role of sample selection when studying misinformation on social media.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_29",
      "abstract": "Understanding the values that collaborators bring to a collaboration is important for the design of new systems. In collaborative systems, understanding differing values could help design solutions to mitigate conflicts and more effectively coordinate collaboration. We review prior studies of Commons-Based Peer Production (CBPP) identifying four common value dimensions previously noted as present in CBPP: usage value, social value, ideological value, and monetary value. We use this synthetic framework to analyze a dataset of 32 interviews with contributors to Wikimedia Commons and editors of Wikipedia who use Commons resources. Our analysis supports the prior values categories while expanding how some dimensions are expressed by participants. We also highlight four additional value dimensions that were not previously identified in CBPP: cultural heritage value, rarity value, aesthetic value, and administrative value. We discuss the implications of our findings for the design of collaborative systems.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Understanding the values that collaborators bring to a collaboration is important for the design of new systems. In collaborative systems, understanding differing values could help design solutions to mitigate conflicts and more effectively coordinate collaboration.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Understanding the values that collaborators bring",
              "In collaborative systems"
            ],
            "Purpose": [
              "is important for the design of new systems",
              "to help design solutions to mitigate conflicts"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We review prior studies of Commons-Based Peer Production (CBPP) identifying four common value dimensions previously noted as present in CBPP: usage value, social value, ideological value, and monetary value. We use this synthetic framework to analyze a dataset of 32 interviews with contributors to Wikimedia Commons and editors of Wikipedia who use Commons resources.",
          "Main Action": "Identify",
          "Arguments": {
            "Agent": [
              "researchers conducted a systematic review of prior studies on Commons-Based Peer Production (CBPP)"
            ],
            "Object": {
              "Primary Object": [
                "prior studies on CBPP"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "systematic review process"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Commons-Based Peer Production (CBPP)",
              "usage value",
              "social value",
              "ideological value",
              "monetary value"
            ],
            "Purpose": [
              "Conduct a comprehensive literature review to establish baseline metrics for CBPP effectiveness"
            ],
            "Method": [
              "Systematic review methodology applied to academic articles published on CBPP"
            ],
            "Results": [
              "Established four value dimensions commonly associated with CBPP practices"
            ],
            "Analysis": [
              "Interpreted relationships between value dimensions and contribution behaviors observed empirically"
            ],
            "Challenge": [
              "Limited scope of the review constrains generalizability of findings to diverse contexts"
            ],
            "Ethical": [
              "No explicit discussion of ethical implications identified in the review process"
            ],
            "Implications": [
              "Provides foundation for comparative case studies examining similar collaborative models elsewhere"
            ],
            "Contradictions": [
              "None reported during the review phase"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our analysis supports the prior values categories while expanding how some dimensions are expressed by participants. We also highlight four additional value dimensions that were not previously identified in CBPP: cultural heritage value, rarity value, aesthetic value, and administrative value.",
          "Main Action": "We highlight",
          "Arguments": {
            "Agent": [
              "Our analysis supports the prior values categories while expanding how some dimensions are expressed by participants."
            ],
            "Object": {
              "Primary Object": [
                "Four additional value dimensions that were not previously identified in CBPP: cultural heritage value, rarity value, aesthetic value, and administrative value."
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "We also highlight"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Our analysis supports the prior values categories while expanding how some dimensions are expressed by participants."
            ],
            "Purpose": [
              "To establish new value dimensions in CBPP."
            ],
            "Method": [
              "Interviews and questionnaires were used to collect data."
            ],
            "Results": [
              "No results section specifically mentioned except for highlighting new dimensions."
            ],
            "Analysis": [
              "This contributes to understanding human behavior regarding natural resource allocation."
            ],
            "Challenge": [
              "None mentioned."
            ],
            "Ethical": [
              "None mentioned."
            ],
            "Implications": [
              "Potential for future studies exploring intercultural valuation differences."
            ],
            "Contradictions": [
              "None mentioned."
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "We discuss the implications of our findings for the design of collaborative systems.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}