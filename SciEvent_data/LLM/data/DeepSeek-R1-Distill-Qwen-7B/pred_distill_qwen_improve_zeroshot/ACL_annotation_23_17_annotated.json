{
  "papers": [
    {
      "paper_code": "ACL_23_P_780",
      "abstract": "Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks’ theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision?",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "Our approach allows"
            ],
            "Object": {
              "Primary Object": [
                "for more precise and interpretable reasoning than previous approaches"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "previous approaches"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation.",
              "More concretely, our approach tracks each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches."
            ],
            "Purpose": [
              "to allow for more precise and interpretable reasoning than previous approaches"
            ],
            "Method": [
              "explicit symbolic representation",
              "graphical representations"
            ],
            "Results": [
              "We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation.",
              "More concretely, our approach tracks each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches."
            ],
            "Analysis": [
              "This approach improves upon previous methods by providing clearer insights into complex interactions between characters' beliefs.",
              "Graphical representations facilitate easier understanding of multi-level reasoning processes.",
              "By focusing on explicit symbolics, the model avoids some ambiguities inherent in less structured methods.",
              "Multi-level tracking ensures comprehensive analysis of nested belief systems.",
              "Comparative evaluation against established benchmarks validates enhanced interpretability and accuracy."
            ],
            "Challenge": [
              "Potential challenges may arise in scaling the system to handle increasingly intricate narrative structures.",
              "Maintaining computational efficiency while increasing granularity requires careful optimization.",
              "Integration with existing NLP pipelines necessitates compatibility considerations.",
              "User adoption barriers exist due to complexity, limiting broad accessibility.",
              "Further investigation is needed to address limitations in handling dynamic contexts where beliefs evolve rapidly."
            ],
            "Ethical": [
              "Ensures accountability through transparent modeling of belief propagation paths.",
              "Promotes trustworthiness by aligning outputs with human-like linguistic processing.",
              "Encourages responsible deployment in educational settings to prevent misinformation spreading.",
              "Supports ethical AI principles by avoiding manipulation through robust detection mechanisms.",
              "Fosters transparency in decision-making processes crucial for regulatory oversight."
            ],
            "Implications": [
              "Advances in cognitive modeling offer new possibilities for enhancing reader support tools.",
              "Improved methodologies pave the way for developing more sophisticated AI companions in education.",
              "Greater focus on interpretability opens doors for comparative studies across disciplines.",
              "Enhanced analytical capabilities enable deeper exploration of narrative structures.",
              "These advancements set a foundation for evaluating alternative architectures aimed at similar goals."
            ],
            "Contradictions": [
              "There are no contradictions presented within the provided abstract content.",
              "All statements cohesively build toward presenting a novel approach without conflicting claims or data points."
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks’ theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset",
          "Main Action": "emphasizes",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "existing theory of mind benchmarks reveal spurious patterns"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "importance of out-of-distribution evaluation methods"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "This highlights the necessity for improved evaluation methodologies in understanding discrepancies between training and testing distributions in AI systems."
            ],
            "Purpose": [
              "To address gaps in current theoretical frameworks concerning pattern recognition across varying data distributions."
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_88",
      "abstract": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples, exhibiting better performance than the existing definition generation method.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3.",
          "Main Action": "we propose",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "context-aware definition generation with GPT-3"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "improving OOD handling through this mechanism"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "this supports the development of the VWSD approach mentioned previously regarding sense definitions and addresses the challenge of OOD issues"
            ],
            "Purpose": [
              "to enhance performance beyond traditional models and manage OOD situations effectively"
            ],
            "Method": [
              "utilizing GPT-3 along with Bayesian inference mechanisms integrated into the workflow"
            ],
            "Results": [
              "improved accuracy rates achieved across diverse datasets compared to state-of-the-art systems and enhanced robustness in handling unseen data"
            ],
            "Analysis": [
              "challenges included leveraging pre-trained language models' capabilities and ensuring computational efficiency due to extensive training requirements"
            ],
            "Challenge": [
              "leverage pre-trained language models' capabilities and ensure computational efficiency"
            ],
            "Ethical": [
              "maintain transparency and avoid unintended biases through dataset distribution considerations"
            ],
            "Implications": [
              "expand into multilingual settings and enable real-time translation services with efficient processing algorithms"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples, exhibiting better performance than the existing definition generation method.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}