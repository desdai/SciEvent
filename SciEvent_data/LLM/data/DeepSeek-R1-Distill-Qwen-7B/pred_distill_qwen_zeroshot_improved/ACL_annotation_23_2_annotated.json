{
  "papers": [
    {
      "paper_code": "ACL_23_P_420",
      "abstract": "Weir has defined a hierarchy of language classes whose second member (L2) is generated by tree-adjoining grammars (TAG), linear indexed grammars (LIG), combinatory categorial grammars, and head grammars. The hierarchy is obtained using the mechanism of control, and L2 is obtained using a context-free grammar (CFG) whose derivations are controlled by another CFG. We adapt Weir’s definition of a controllable CFG (called a labeled distinguished CFG) to give a definition of controllable pushdown automata (PDAs), called labeled distinguished PDAs. This yields three new characterizations of L2 as the class of languages generated by PDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs. We show that these four formalisms are not only weakly equivalent but equivalent in a stricter sense that we call d-weak equivalence. Furthermore, using an even stricter notion of equivalence called d-strong equivalence, we make precise the intuition that a CFG controlling a CFG is a TAG, a PDA controlling a PDA is an embedded PDA, and a PDA controlling a CFG is a LIG. The fourth member of this family, a CFG controlling a PDA, does not correspond to any kind of automaton we know of, so we invent one and call it a Pushdown Adjoining Automaton (PAA).",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Weir has defined a hierarchy of language classes whose second member (L2) is generated by tree-adjoining grammars (TAG), linear indexed grammars (LIG), combinatory categorial grammars, and head grammars. The hierarchy is obtained using the mechanism of control, and L2 is obtained using a context-free grammar (CFG) whose derivations are controlled by another CFG.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We adapt Weir’s definition of a controllable CFG (called a labeled distinguished CFG) to give a definition of controllable pushdown automata (PDAs), called labeled distinguished PDAs. This yields three new characterizations of L2 as the class of languages generated by PDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs.",
          "Main Action": "adapt",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Weir’s definition of a controllable CFG"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "as giving rise to the definitions of controllable pushdown automata"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "This yields three new characterizations of L2 as the class of languages generated by PDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs."
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We show that these four formalisms are not only weakly equivalent but equivalent in a stricter sense that we call d-weak equivalence. Furthermore, using an even stricter notion of equivalence called d-strong equivalence, we make precise the intuition that a CFG controlling a CFG is a TAG, a PDA controlling a PDA is an embedded PDA, and a PDA controlling a CFG is a LIG. The fourth member of this family, a CFG controlling a PDA, does not correspond to any kind of automaton we know of, so we invent one and call it a Pushdown Adjoining Automaton (PAA).",
          "Main Action": "invent",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Pushdown Adjoining Automaton (PAA)"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "We show that these four formalisms are not only weakly equivalent but equivalent in a stricter sense that we call d-weak equivalence.",
              "Furthermore, using an even stricter notion of equivalence called d-strong equivalence"
            ],
            "Purpose": [
              "Advance theoretical understanding by expanding the class of controlled grammars beyond current knowledge gaps"
            ],
            "Method": [
              "General academic methods including proofs and definitions"
            ],
            "Results": [
              "By inventing the Pushdown Adjoining Automaton (PAA), we address the absence of correspondence for a CFG controlling a PDA, thereby contributing novel insights to the theory of formal languages and automata"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Advancing computational linguistics and parsing technologies by completing the hierarchy of grammar-controlled automata"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_216",
      "abstract": "End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model’s performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model’s performance closely correlates with its embedding similarity between speech and source transcript.",
          "Main Action": "exhibits poor performance",
          "Arguments": {
            "Agent": [
              "researchers working on End-to-end Speech Translation (E2E ST)"
            ],
            "Object": {
              "Primary Object": [
                "when trained with extremely small speech-text datasets"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "existing ST methods perform poorly when only extremely small speech-text data are available for training"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "an ST model’s performance closely correlates with its embedding similarity between speech and source transcript"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Word-Aligned Contrastive Learning (WACO)",
                "a novel method for extremely low-resource speech-to-text translation"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "in addressing the challenge of limited resources in translating speech to text"
            ],
            "Purpose": [
              "to provide an efficient and accurate solution for real-world applications where computational and linguistic resources are constrained"
            ],
            "Method": [
              "contrastive learning framework that bridges word-level representations between speech and text modalities"
            ],
            "Results": [
              "experiments demonstrate significant improvements in reducing hallucination rates compared to traditional methods reliant on external models"
            ],
            "Analysis": [
              "highlighting the importance of modeling inter-modal relationships for robust translation systems"
            ],
            "Challenge": [
              "addressing the inherent difficulty of training reliable models in resource-constrained environments"
            ],
            "Ethical": [
              "considering the societal impact of improving accessibility to multilingual technologies"
            ],
            "Implications": [
              "opening new possibilities for advancing multilingual processing capabilities in diverse linguistic contexts"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data.",
          "Main Action": "evaluate",
          "Arguments": {
            "Agent": [
              "WACO",
              "other methods"
            ],
            "Object": {
              "Primary Object": [
                "on the MuST-C dataset, a widely used ST benchmark",
                "and on a low-resource direction Maltese-English from IWSLT 2023."
              ],
              "Secondary Object": [
                "WACO",
                "the best baseline"
              ]
            },
            "Context": [
              "Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data."
            ],
            "Purpose": [
              "to evaluate WACO and other methods on the MuST-C dataset and a low-resource direction Maltese-English from IWSLT 2023."
            ],
            "Method": [
              "compare against the best baseline with limited resources"
            ],
            "Results": [
              "WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data."
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}