{
  "papers": [
    {
      "paper_code": "cscw_23_P_230",
      "abstract": "Volunteer moderators have the power to shape society through their influence on online discourse. However, the growing scale of online interactions increasingly presents significant hurdles for meaningful moderation. Furthermore, there are only limited tools available to assist volunteers with their work. Our work aims to meaningfully explore the potential of AI-driven, automated moderation tools for social media to assist volunteer moderators. One key aspect is to investigate the degree to which tools must become personalizable and context-sensitive in order to not just delete unsavory content and ban trolls, but to adapt to the millions of online communities on social media mega-platforms that rely on volunteer moderation. In this study, we conduct semi-structured interviews with 26 Facebook Group moderators in order to better understand moderation tasks and their associated challenges. Through qualitative analysis of the interview data, we identify and address the most pressing themes in the challenges they face daily. Using interview insights, we conceptualize three tools with automated features that assist them in their most challenging tasks and problems. We then evaluate the tools for usability and acceptance using a survey drawing on the technology acceptance literature with 22 of the same moderators. Qualitative and descriptive analyses of the survey data show that context-sensitive, agency-maintaining tools in addition to trial experience are key to mass adoption by volunteer moderators in order to build trust in the validity of the moderation technology.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Volunteer moderators have the power to shape society through their influence on online discourse. However, the growing scale of online interactions increasingly presents significant hurdles for meaningful moderation. Furthermore, there are only limited tools available to assist volunteers with their work.",
          "Main Action": "Volunteer moderators have the power",
          "Arguments": {
            "Agent": [
              "Volunteer moderators"
            ],
            "Object": {
              "Primary Object": [
                "shaping society through their influence on online discourse"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Our work aims to meaningfully explore the potential of AI-driven, automated moderation tools for social media to assist volunteer moderators. One key aspect is to investigate the degree to which tools must become personalizable and context-sensitive in order to not just delete unsavory content and ban trolls, but to adapt to the millions of online communities on social media mega-platforms that rely on volunteer moderation. In this study, we conduct semi-structured interviews with 26 Facebook Group moderators in order to better understand moderation tasks and their associated challenges. Through qualitative analysis of the interview data, we identify and address the most pressing themes in the challenges they face daily.",
          "Main Action": "Investigate the degree",
          "Arguments": {
            "Agent": [
              "AI-driven, automated moderation tools",
              "Social media volunteers",
              "Volunteer moderators"
            ],
            "Object": {
              "Primary Object": [
                "tools must become personalized",
                "context-sensitive"
              ],
              "Secondary Object": [
                "delete unsavory content",
                "ban trolls"
              ]
            },
            "Context": [
              "million online communities",
              "social media mega-platforms",
              "rely on volunteer moderation"
            ],
            "Purpose": [
              "Develop practical guidelines",
              "Understand moderation challenges"
            ],
            "Method": [
              "Qualitative analysis",
              "Interview design",
              "Participant selection",
              "User-generated content",
              "Recruitment strategies"
            ],
            "Results": [
              "Common challenges identified",
              "Actionable recommendations developed"
            ],
            "Analysis": [
              "Challenges faced by moderators",
              "Need for balance between automation and human oversight"
            ],
            "Challenge": [
              "Balancing automation efficiency",
              "Maintaining human oversight",
              "Consistency among interviewees"
            ],
            "Ethical": [
              "Privacy concerns",
              "Bias in algorithms",
              "Fair representation"
            ],
            "Implications": [
              "Enhance public trust",
              "Improve platform stability",
              "Advance moderation practices"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "ERROR",
          "Text": "Using interview insights, we conceptualize three tools with automated features that assist them in their most challenging tasks and problems. We then evaluate the tools for usability and acceptance using a survey drawing on the technology acceptance literature with 22 of the same moderators. Qualitative and descriptive analyses of the survey data show that context-sensitive, agency-maintaining tools in addition to trial experience are key to mass adoption by volunteer moderators in order to build trust in the validity of the moderation technology.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_260",
      "abstract": "Interdependent privacy (IDP) violations occur when users share personal information about others without permission, resulting in potential embarrassment, reputation loss, or harassment. There are several strategies that can be applied to protect IDP, but little is known regarding how social media users perceive IDP threats or how they prefer to respond to them. We utilized a mixed-method approach with a replication study to examine user beliefs about various government-, platform-, and user-level strategies for managing IDP violations. Participants reported that IDP represented a 'serious' online threat, and identified themselves as primarily responsible for responding to violations. IDP strategies that felt more familiar and provided greater perceived control over violations (e.g., flagging, blocking, unfriending) were rated as more effective than platform or government driven interventions. Furthermore, we found users were more willing to share on social media if they perceived their interactions as protected. Findings are discussed in relation to control paradox theory.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Interdependent privacy (IDP) violations occur when users share personal information about others without permission, resulting in potential embarrassment, reputation loss, or harassment. There are several strategies that can be applied to protect IDP, but little is known regarding how social media users perceive IDP threats or how they prefer to respond to them.",
          "Main Action": "Interdependent privacy (IDP) violations occur",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "Potential harm to individuals"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Users share personal information about others without permission, resulting in potential embarrassment, reputation loss, or harassment.",
              "There are several strategies that can be applied to protect IDP, but little is known regarding how social media users perceive IDP threats or how they prefer to respond to them."
            ],
            "Purpose": [
              "To explore how social media users perceive IDP threats and their preferred response mechanisms"
            ],
            "Method": [
              "Empirical analysis conducted among participants"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Obtaining sufficient sample size and ensuring representativeness across diverse groups"
            ],
            "Ethical": [
              "Ensuring informed consent was adhered to when gathering participants' perspectives"
            ],
            "Implications": [
              "Understanding user responses could inform development of effective communication strategies to mitigate risks associated with IDP"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We utilized a mixed-method approach with a replication study to examine user beliefs about various government-, platform-, and user-level strategies for managing IDP violations.",
          "Main Action": "We utilized a mixed-method approach with a replication study",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "user beliefs"
              ],
              "Secondary Object": [
                "various government-, platform-, and user-level strategies"
              ]
            },
            "Context": [
              "to examine user beliefs about various government-, platform-, and user-level strategies for managing IDP violations"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "a mixed-method approach with a replication study"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Participants reported that IDP represented a 'serious' online threat, and identified themselves as primarily responsible for responding to violations. IDP strategies that felt more familiar and provided greater perceived control over violations (e.g., flagging, blocking, unfriending) were rated as more effective than platform or government driven interventions. Furthermore, we found users were more willing to share on social media if they perceived their interactions as protected. Findings are discussed in relation to control paradox theory.",
          "Main Action": "User-initiated responses via flagging, blocking, unfriending",
          "Arguments": {
            "Agent": [
              "Users"
            ],
            "Object": {
              "Primary Object": [
                "IDP violation strategies"
              ],
              "Secondary Object": [
                "Automated vs. user-driven interventions"
              ]
            },
            "Context": [
              "Online behavior aimed at mitigating IDP threats"
            ],
            "Purpose": [
              "Testing effectiveness of user-driven vs. automated strategies"
            ],
            "Method": [
              "Observation of user reactions to different intervention types"
            ],
            "Results": [
              "More familiar strategies were deemed more effective"
            ],
            "Analysis": [
              "Perceived control influences strategy choice"
            ],
            "Challenge": [
              "No challenges mentioned"
            ],
            "Ethical": [
              "No direct ethical points mentioned"
            ],
            "Implications": [
              "Potential for refining detection algorithms"
            ],
            "Contradictions": [
              "None mentioned"
            ]
          }
        }
      ]
    }
  ]
}