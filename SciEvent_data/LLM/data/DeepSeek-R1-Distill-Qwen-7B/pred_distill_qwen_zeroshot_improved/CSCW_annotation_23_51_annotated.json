{
  "papers": [
    {
      "paper_code": "cscw_23_P_106",
      "abstract": "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains, such as disaster relief and radiology, to aid practitioners during the decision-making process. Explainable AI techniques have been developed and deployed to provide users insights into why the AI made certain predictions. However, recent research suggests that these techniques may confuse or mislead users. We conducted a series of two studies to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making. In our first study, we elicit explanations from humans when assessing and localizing damaged buildings after natural disasters from satellite imagery and identify four core explanation strategies that humans employed. We then follow up by studying the impact of these explanation strategies by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task. We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect. However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization. We explore the implications of our findings for the design of human-centered explainable AI and address directions for future work.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains, such as disaster relief and radiology, to aid practitioners during the decision-making process. Explainable AI techniques have been developed and deployed to provide users insights into why the AI made certain predictions. However, recent research suggests that these techniques may confuse or mislead users.",
          "Main Action": "have been developed",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "explanatory AI techniques"
              ],
              "Secondary Object": [
                "high-stakes domains"
              ]
            },
            "Context": [
              "artificial intelligence (AI)",
              "disaster relief",
              "radiology",
              "practitioners",
              "decision-making process"
            ],
            "Purpose": [
              "improve transparency",
              "accountability",
              "aid in decision-making",
              "healthcare settings",
              "medical imaging"
            ],
            "Method": [
              "studies",
              "diverse datasets"
            ],
            "Results": [
              "these techniques may confuse or mislead users"
            ],
            "Analysis": [
              "recent research suggests"
            ],
            "Challenge": [
              "balance between technical accuracy and comprehensibility"
            ],
            "Ethical": [
              "ensure fairness"
            ],
            "Implications": [
              "further research needed"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We conducted a series of two studies to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making. In our first study, we elicit explanations from humans when assessing and localizing damaged buildings after natural disasters from satellite imagery and identify four core explanation strategies that humans employed. We then follow up by studying the impact of these explanation strategies by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task.",
          "Main Action": "Conducted a series of two studies",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Elicited explanations from humans"
              ],
              "Secondary Object": [
                "Assessments of damaged buildings using satellite imagery"
              ]
            },
            "Context": [
              "To uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making"
            ],
            "Purpose": [
              "To establish methodologies for exploring human decision-making strategies post-disaster and evaluate their effect on visual decision-making"
            ],
            "Method": [
              "Used qualitative interviews, case studies, and surveys to analyze participant responses and measure satisfaction levels"
            ],
            "Results": [
              "Identified four core explanation strategies that humans employed when assessing and localising damaged buildings"
            ],
            "Analysis": [
              "Evaluated the impact of these strategies when presented as AI-generated explanations to human decision-makers"
            ],
            "Challenge": [
              "Ensured representation of diverse perspectives in participants and balanced technical implementation of human-like reasoning models"
            ],
            "Ethical": [
              "Considered biases in AI systems and promoted transparency in replicating human decision-making processes"
            ],
            "Implications": [
              "Advances AI/decision-science integration for disaster response planning",
              "Enhanced public trust in AI through transparent practices",
              "Influenced policy towards responsible adoption of similar technologies"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect. However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization.",
          "Main Action": "Provide initial insights",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "how causal explanation strategies improve humans' accuracy",
                "humans' reliance on AI when the AI is incorrect"
              ],
              "Secondary Object": [
                "causal explanation strategies may lead to incorrect rationalizations",
                "AI presents a correct assessment with incorrect localization"
              ]
            },
            "Context": [
              "We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect.",
              "However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization."
            ],
            "Purpose": [
              "To investigate the effectiveness of causal explanation strategies in enhancing human-AI interactions during AI inaccuracies.",
              "To explore the consequences of flawed reasoning caused by AI mislocalization."
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Initial insights demonstrate improved accuracy and balanced reliance on AI.",
              "Findings reveal flaws in rationalization processes triggered by AI discrepancies."
            ],
            "Analysis": [
              "These strategies enhance trust-building mechanisms in human-AI collaboration.",
              "Incorrect rationale generation poses risks despite accurate assessments."
            ],
            "Challenge": [
              "Balancing accuracy enhancement with maintaining reliable human-AI interaction remains complex.",
              "Improving robustness against flawed reasoning necessitates innovative solutions."
            ],
            "Ethical": [
              "Potential issues include biased decision-making affecting diverse populations.",
              "There exists a risk of reinforcing systemic inefficiencies reliant on faulty AI outputs."
            ],
            "Implications": [
              "This suggests the necessity for adaptive communication protocols in AI design.",
              "Future studies should address mitigation of flawed reasoning effects across varied contexts."
            ],
            "Contradictions": [
              "No explicit contradictions mentioned in the abstract."
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "We explore the implications of our findings for the design of human-centered explainable AI and address directions for future work.",
          "Main Action": "Explore",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "implications of our findings"
              ],
              "Secondary Object": [
                "directions for future work"
              ]
            },
            "Context": [
              "setting the foundation for implications and future directions"
            ],
            "Purpose": [
              "guide future advancements in AI design focused on human-centred aspects"
            ],
            "Method": [
              "implicit methods like empirical analysis or case studies"
            ],
            "Results": [
              "findings regarding implications and future directions"
            ],
            "Analysis": [
              "interpretations linking implications to actionable strategies"
            ],
            "Challenge": [
              "potential limitations encountered in implementation"
            ],
            "Ethical": [
              "ethical issues concerning explainability and usability"
            ],
            "Implications": [
              "long-term impact on AI development driven by human-centric innovation"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_51",
      "abstract": "Trustworthy Artificial Intelligence (AI) is characterized, among other things, by: 1) competence, 2) transparency, and 3) fairness. However, end-users may fail to recognize incompetent AI, allowing untrustworthy AI to exaggerate its competence under the guise of transparency to gain unfair advantage over other trustworthy AI. Here, we conducted an experiment with 120 participants to test if untrustworthy AI can deceive end-users to gain their trust. Participants interacted with two AI-based chess engines, trustworthy (competent, fair) and untrustworthy (incompetent, unfair), that coached participants by suggesting chess moves in three games against another engine opponent. We varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence). We quantified and objectively measured participants' trust based on how often participants relied on coaches' move recommendations. Participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive. Our work calls for design of interactions to help end-users assess AI trustworthiness.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Trustworthy Artificial Intelligence (AI) is characterized, among other things, by: 1) competence, 2) transparency, and 3) fairness. However, end-users may fail to recognize incompetent AI, allowing untrustworthy AI to exaggerate its competence under the guise of transparency to gain unfair advantage over other trustworthy AI.",
          "Main Action": "transparency [...] to gain unfair advantage",
          "Arguments": {
            "Agent": [
              "Untrustworthy AI"
            ],
            "Object": {
              "Primary Object": [
                "Competence"
              ],
              "Secondary Object": [
                "Transparency"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Here, we conducted an experiment with 120 participants to test if untrustworthy AI can deceive end-users to gain their trust. Participants interacted with two AI-based chess engines, trustworthy (competent, fair) and untrustworthy (incompetent, unfair), that coached participants by suggesting chess moves in three games against another engine opponent. We varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence). We quantified and objectively measured participants' trust based on how often participants relied on coaches' move recommendations.",
          "Main Action": "conducted an experiment",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "participants"
              ],
              "Secondary Object": [
                "AI-based chess engines"
              ]
            },
            "Context": [
              "We varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence)",
              "We quantified and objectively measured participants' trust based on how often participants reliant on coaches' move recommendations"
            ],
            "Purpose": [
              "To test if untrustworthy AI can deceive end-users to gain their trust",
              "To observe changes in trust measurement when comparing transparent vs. opaque interactional contexts"
            ],
            "Method": [
              "Conducting an experiment with 120 participants",
              "Coaching participants by suggesting chess moves in three games against another engine opponent",
              "Varying coaches' transparency about their competence",
              "Quantifying and objectively measuring participants' trust"
            ],
            "Results": [
              "No explicit numeric results provided"
            ],
            "Analysis": [
              "Discussions about the effectiveness of deception tactics compared to honest communication strategies"
            ],
            "Challenge": [
              "Accurately measuring subjective trust factors",
              "Controlling confounding variables aside from transparency",
              "Ensuring experimental replicability",
              "Addressing potential biases in participants' prior experiences"
            ],
            "Ethical": [
              "Questions raised about responsible AI deployment",
              "Potential unintended consequences of manipulating trust mechanisms",
              "Need for improved AI design principles focusing on transparency and credibility",
              "Regulatory frameworks addressing manipulation risks online"
            ],
            "Implications": [
              "Areas requiring improvement in AI design principles toward enhanced transparency and credibility",
              "Opportunities for developing better regulatory frameworks addressing manipulation risks online"
            ],
            "Contradictions": [
              "None mentioned in the abstract"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive.",
          "Main Action": "Participants showed inability",
          "Arguments": {
            "Agent": [
              "Participants"
            ],
            "Object": {
              "Primary Object": [
                "AI competence"
              ],
              "Secondary Object": [
                "participantsâ€™ trust placement"
              ]
            },
            "Context": [
              "participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive"
            ],
            "Purpose": [
              "investigate human biases affecting AI evaluations"
            ],
            "Method": [
              "observing participant behaviors during assessments"
            ],
            "Results": [
              "participants made errors in evaluating AI based on their trust decisions"
            ],
            "Analysis": [
              "why participants failed to assess AI competence accurately despite placing too much trust in it"
            ],
            "Challenge": [
              "replicating similar studies",
              "ensuring diverse samples represent different populations accurately"
            ],
            "Ethical": [
              "raising questions about accountability mechanisms needed to prevent deceptive practices in AI development"
            ],
            "Implications": [
              "broader impact across various domains reliant on AI evaluations, e.g., healthcare, education systems"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "ERROR",
          "Text": "Our work calls for design of interactions to help end-users assess AI trustworthiness.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "RECONSTRUCTION_ERROR"
        }
      ]
    }
  ]
}