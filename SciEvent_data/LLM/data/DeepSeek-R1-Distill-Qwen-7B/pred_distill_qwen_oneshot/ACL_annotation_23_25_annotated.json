{
  "papers": [
    {
      "paper_code": "ACL_23_P_783",
      "abstract": "Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model’s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model’s label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks. The gain is substantial on tasks with large domain-label bias (up to 37% in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model’s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact.",
          "Main Action": "discuss",
          "Arguments": {
            "Agent": [
              "various design settings for in-context learning (icl)",
              "different aspects of icl design"
            ],
            "Object": {
              "Primary Object": [
                "various design settings for in-context learning (icl)"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "different aspects of icl design"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "despite numerous studies discussing these design choices, limited work exists systematically investigates categories and mitigation strategies"
            ],
            "Purpose": [
              "highlighting the lack of comprehensive investigation despite existing studies",
              "call for more structured exploration"
            ],
            "Method": [
              "no concrete methodology steps are outlined here",
              "conceptual regarding design evaluations without detailed procedures"
            ],
            "Results": [
              "specific numerical improvements or empirical data haven't been presented yet"
            ],
            "Analysis": [
              "focuses on pointing out gaps needing attention"
            ],
            "Challenge": [
              "complexity involved in organizing diverse design factors effectively"
            ],
            "Ethical": [
              "no evident ethical issues are raised"
            ],
            "Implications": [
              "suggestions broader impacts on improving prediction reliability via better-designed icl setups"
            ],
            "Contradictions": [
              "none mentioned"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model’s label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks.",
          "Main Action": "define",
          "Arguments": {
            "Agent": [
              "In this work"
            ],
            "Object": {
              "Primary Object": [
                "a typology for three types of label biases in ILLMs for text classification: vanilla-label bias, context-label bias, and domain-label bias"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "which we conceptualize and detect for the first time"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "specifically, our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases"
            ],
            "Purpose": [
              "To mitigate the effect of these biases"
            ],
            "Method": [
              "we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words from the task corpus"
            ],
            "Results": [
              "After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ILLM performance of GPT-J and GPT-3 on a wide range of tasks"
            ],
            "Analysis": [
              "Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples"
            ],
            "Challenge": [
              "None mentioned specifically"
            ],
            "Ethical": [
              "None mentioned specifically"
            ],
            "Implications": [
              "The proposed method offers a promising solution to address label biases effectively across various tasks"
            ],
            "Contradictions": [
              "None mentioned specifically"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "he gain is substantial on tasks with large domain-label bias (up to 37% in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL.",
          "Main Action": "gain",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "tasks with large domain-label bias (up to 37% in Macro-F1)"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Further, our results generalize to models with different scales, pretraining methods, and manual task designs, demonstrating label biases in ICL."
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_139",
      "abstract": "Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs. Inspired by routing-based dynamic network, we model the dynamic mechanism in multimodal sarcasm detection and propose the Dynamic Routing Transformer Network (DynRT-Net). Our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity. Experimental results on a public dataset demonstrate the effectiveness of our method compared to the state-of-the-art methods.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs.",
          "Main Action": "due to their inability to handle dynamic relationships across modals",
          "Arguments": {
            "Agent": [
              "current solutions"
            ],
            "Object": {
              "Primary Object": [
                "why current solutions may fail"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "Previous attempts have tried to address this issue through optimizing certain hyperparameters within predefined structures; however, such adjustments often lead to suboptimal performances because they do not account for varying contextual dependencies inherent in real-world scenarios."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Inspired by routing-based dynamic network, we model the dynamic mechanism in multimodal sarcasm detection and propose the Dynamic Routing Transformer Network (DynRT-Net). Our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Dynamic Routing Transformer Network"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "inspired by routing-based dynamic network"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity"
            ],
            "Results": [
              "Our method shows promise in detecting severe sarcasm and aligns well with baseline approaches reliant on external models"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experimental results on a public dataset demonstrate the effectiveness of our method compared to the state-of-the-art methods.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "our experimental results"
            ],
            "Object": {
              "Primary Object": [
                "the effectiveness of our method"
              ],
              "Primary Modifier": [
                "<NONE>"
              ],
              "Secondary Object": [
                "compared to the state-of-the-art methods"
              ],
              "Secondary Modifier": [
                "<NONE>"
              ]
            },
            "Context": [
              "on a public dataset"
            ],
            "Purpose": [
              "to show"
            ],
            "Method": [
              "no specific methods mentioned here"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}