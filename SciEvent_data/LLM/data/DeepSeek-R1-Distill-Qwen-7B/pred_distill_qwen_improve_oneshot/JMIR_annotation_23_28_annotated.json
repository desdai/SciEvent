{
  "papers": [
    {
      "paper_code": "jmir_23_P_165",
      "abstract": "Digital mindfulness-based interventions (MBIs) are a promising approach to deliver accessible and scalable mindfulness training and have been shown to improve a range of health outcomes. However, the success of digital MBIs is reliant on adequate engagement, which remains a crucial challenge. Understanding people’s experiences of using digital MBIs and identifying the core factors that facilitate or act as barriers to engagement is essential to inform intervention development and maximize engagement and outcomes. This study aims to systematically map the literature on people’s experiences of using digital MBIs that target psychosocial variables (e.g., anxiety, depression, distress, and well-being) and identify key barriers to and facilitators of engagement. We conducted a scoping review to synthesize empirical qualitative research on people’s experiences of using digital MBIs. We adopted a streamlined approach to ensure that the evidence could be incorporated into the early stages of intervention development. The search strategy identified articles with at least one keyword related to mindfulness, digital, user experience, and psychosocial variables in their title or abstract. Inclusion criteria specified that articles must have a qualitative component, report on participants’ experiences of using a digital MBI designed to improve psychosocial variables, and have a sample age range that at least partially overlapped with 16 to 35 years. Qualitative data on user experience were charted and analyzed using inductive thematic synthesis to generate understandings that go beyond the content of the original studies. We used the Quality of Reporting Tool to critically appraise the included sources of evidence. The search identified 530 studies, 22 (4.2%) of which met the inclusion criteria. Overall, the samples were approximately 78% female and 79% White; participants were aged between 16 and 69 years; and the most used measures in intervention studies were mindfulness, psychological flexibility, and variables related to mental health (including depression, anxiety, stress, and well-being). All studies were judged to be adequately reported. We identified 3 themes characterizing barriers to and facilitators of engagement: responses to own practice (i.e., negative reactions to one’s own practice are common and can deplete motivation), making mindfulness a habit (i.e., creating a consistent training routine is essential yet challenging), and leaning on others (i.e., those engaging depend on someone else for support). The themes identified in this review provide crucial insights as to why people frequently stop engaging with digital MBIs. Researchers and developers should consider using person-based coparticipatory methods to improve acceptability of and engagement with digital MBIs, increase their effectiveness, and support their translation to real-world use. Such strategies must be grounded in relevant literature and meet the priorities and needs of the individuals who will use the interventions.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Digital mindfulness-based interventions (MBIs) are a promising approach to deliver accessible and scalable mindfulness training and have been shown to improve a range of health outcomes. However, the success of digital MBIs is reliant on adequate engagement, which remains a crucial challenge. Understanding people’s experiences of using digital MBIs and identifying the core factors that facilitate or act as barriers to engagement is essential to inform intervention development and maximize engagement and outcomes.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Digital mindfulness-based interventions (MBIs) are a promising approach to delivering accessible and scalable mindfulness training and have been shown to improve a range of health outcomes.",
              "However, the success of digital MBIs is reliant on adequate engagement, which remains a crucial challenge."
            ],
            "Purpose": [
              "Understanding people’s experiences of using digital MBIs and identifying the core factors that facilitate or act as barriers to engagement is essential to inform intervention development and maximize engagement and outcomes."
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Adequate engagement remains a crucial challenge despite successful delivery of digital MBIs."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Improving engagement could lead to better health outcomes via effective digital MBI implementations."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "This study aims to systematically map the literature on people’s experiences of using digital MBIs that target psychosocial variables (e.g., anxiety, depression, distress, and well-being) and identify key barriers to and facilitators of engagement. We conducted a scoping review to synthesize empirical qualitative research on people’s experiences of using digital MBIs. We adopted a streamlined approach to ensure that the evidence could be incorporated into the early stages of intervention development. The search strategy identified articles with at least one keyword related to mindfulness, digital, user experience, and psychosocial variables in their title or abstract. Inclusion criteria specified that articles must have a qualitative component, report on participants’ experiences of using a digital MBI designed to improve psychosocial variables, and have a sample age range that at least partially overlapped with 16 to 35 years. Qualitative data on user experience were charted and analyzed using inductive thematic synthesis to generate understandings that go beyond the content of the original studies. We used the Quality of Reporting Tool to critically appraise the included sources of evidence.",
          "Main Action": "conducted",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "a scoping review to synthesize empirical qualitative research on people’s experiences of using digital MBIs"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The search identified 530 studies, 22 (4.2%) of which met the inclusion criteria. Overall, the samples were approximately 78% female and 79% White; participants were aged between 16 and 69 years; and the most used measures in intervention studies were mindfulness, psychological flexibility, and variables related to mental health (including depression, anxiety, stress, and well-being). All studies were judged to be adequately reported. We identified 3 themes characterizing barriers to and facilitators of engagement: responses to own practice (i.e., negative reactions to one’s own practice are common and can deplete motivation), making mindfulness a habit (i.e., creating a consistent training routine is essential yet challenging), and leaning on others (i.e., those engaging depend on someone else for support).",
          "Main Action": "were aged",
          "Arguments": {
            "Agent": [
              "participants"
            ],
            "Object": {
              "Primary Object": [
                "between 16 and 69 years"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "ERROR",
          "Text": "The themes identified in this review provide crucial insights as to why people frequently stop engaging with digital MBIs. Researchers and developers should consider using person-based coparticipatory methods to improve acceptability of and engagement with digital MBIs, increase their effectiveness, and support their translation to real-world use. Such strategies must be grounded in relevant literature and meet the priorities and needs of the individuals who will use the interventions.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        }
      ]
    },
    {
      "paper_code": "jmir_23_P_975",
      "abstract": "Large language model (LLM)–based artificial intelligence chatbots direct the power of large training data sets toward successive, related tasks as opposed to single-ask tasks, for which artificial intelligence already achieves impressive performance. The capacity of LLMs to assist in the full scope of iterative clinical reasoning via successive prompting, in effect acting as artificial physicians, has not yet been evaluated. This study aimed to evaluate ChatGPT’s capacity for ongoing clinical decision support via its performance on standardized clinical vignettes. We inputted all 36 published clinical vignettes from the Merck Sharpe & Dohme (MSD) Clinical Manual into ChatGPT and compared its accuracy on differential diagnoses, diagnostic testing, final diagnosis, and management based on patient age, gender, and case acuity. Accuracy was measured by the proportion of correct responses to the questions posed within the clinical vignettes tested, as calculated by human scorers. We further conducted linear regression to assess the contributing factors toward ChatGPT’s performance on clinical tasks. ChatGPT achieved an overall accuracy of 71.7% (95% CI 69.3%-74.1%) across all 36 clinical vignettes. The LLM demonstrated the highest performance in making a final diagnosis with an accuracy of 76.9% (95% CI 67.8%-86.1%) and the lowest performance in generating an initial differential diagnosis with an accuracy of 60.3% (95% CI 54.2%-66.6%). Compared to answering questions about general medical knowledge, ChatGPT demonstrated inferior performance on differential diagnosis (β=–15.8%; P<.001) and clinical management (β=–7.4%; P=.02) question types. ChatGPT achieves impressive accuracy in clinical decision-making, with increasing strength as it gains more clinical information at its disposal. In particular, ChatGPT demonstrates the greatest accuracy in tasks of final diagnosis as compared to initial diagnosis. Limitations include possible model hallucinations and the unclear composition of ChatGPT’s training data set.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Large language model (LLM)–based artificial intelligence chatbots direct the power of large training data sets toward successive, related tasks as opposed to single-ask tasks, for which artificial intelligence already achieves impressive performance. The capacity of LLMs to assist in the full scope of iterative clinical reasoning via successive prompting, in effect acting as artificial physicians, has not yet been evaluated.",
          "Main Action": "have not been formally evaluated",
          "Arguments": {
            "Agent": [
              "researchers investigating whether LLMs can mimic human-like reasoning processes akin to those of physicians"
            ],
            "Object": {
              "Primary Object": [
                "[ability/mechanism of LLMs to perform complex reasoning similar to human physicians]"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "While the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little.",
              "Indeed, recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard sequence log-probability is more informative.",
              "It means that internal characteristics of the model can give much more information than we expect, and before using external models and measures, we first need to ask: how far can we go if we use nothing but the translation model itself?"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "is able to alleviate hallucinations at test time on par with the previous best approach that relies on external models",
              "Next, if we move away from internal model characteristics and allow external tools, we show that using sentence similarity from cross-lingual embeddings further improves these results."
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "This study aimed to evaluate ChatGPT’s capacity for ongoing clinical decision support via its performance on standardized clinical vignettes. We inputted all 36 published clinical vignettes from the Merck Sharpe & Dohme (MSD) Clinical Manual into ChatGPT and compared its accuracy on differential diagnoses, diagnostic testing, final diagnosis, and management based on patient age, gender, and case acuity. Accuracy was measured by the proportion of correct responses to the questions posed within the clinical vignettes tested, as calculated by human scorers. We further conducted linear regression to assess the contributing factors toward ChatGPT’s performance on clinical tasks.",
          "Main Action": "aimed",
          "Arguments": {
            "Agent": [
              "[the researchers]"
            ],
            "Object": {
              "Primary Object": [
                "to evaluate ChatGPT’s capacity for ongoing clinical decision support via its performance on standardized clinical vignettes"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "We inputted all 36 published clinical vignettes from the Merck Sharpe & Dohme (MSD) Clinical Manual into ChatGPT"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "ChatGPT achieved an overall accuracy of 71.7% (95% CI 69.3%-74.1%) across all 36 clinical vignettes. The LLM demonstrated the highest performance in making a final diagnosis with an accuracy of 76.9% (95% CI 67.8%-86.1%) and the lowest performance in generating an initial differential diagnosis with an accuracy of 60.3% (95% CI 54.2%-66.6%). Compared to answering questions about general medical knowledge, ChatGPT demonstrated inferior performance on differential diagnosis (β=–15.8%; P<.001) and clinical management (β=–7.4%; P=.02) question types.",
          "Main Action": "achieved",
          "Arguments": {
            "Agent": [
              "ChatGPT"
            ],
            "Object": {
              "Primary Object": [
                "overall accuracy of 71.7% (95% CI 69.3%-74.1%) across all 36 clinical vignettes"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "ChatGPT achieves impressive accuracy in clinical decision-making, with increasing strength as it gains more clinical information at its disposal. In particular, ChatGPT demonstrates the greatest accuracy in tasks of final diagnosis as compared to initial diagnosis. Limitations include possible model hallucinations and the unclear composition of ChatGPT’s training data set.",
          "Main Action": "achieves",
          "Arguments": {
            "Agent": [
              "ChatGPT"
            ],
            "Object": {
              "Primary Object": [
                "its ability to perform clinical decision-making, especially in final diagnosis"
              ],
              "Secondary Object": [
                "in comparison to initial diagnosis"
              ]
            },
            "Context": [
              "increasing strength as it gains more clinical information at its disposal"
            ],
            "Purpose": [
              "to demonstrate the capabilities and limitations of ChatGPT in medical diagnosis"
            ],
            "Method": [
              "evaluation of accuracy in tasks involving final vs initial diagnosis"
            ],
            "Results": [
              "ChatGPT demonstrates impressive accuracy in clinical decision-making, with significantly better performance in tasks of final diagnosis compared to initial diagnosis"
            ],
            "Analysis": [
              "Limitations arise from possible model hallucinations and unclear composition of the training dataset"
            ],
            "Challenge": [
              "the reliance on insufficient clinical information leading to inaccurate early-stage diagnoses"
            ],
            "Ethical": [
              "no direct ethical concern mentioned"
            ],
            "Implications": [
              "further exploration of model enhancements and alternative strategies to address current limitations could lead to improved healthcare outcomes"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}