{
  "papers": [
    {
      "paper_code": "bioinfo_23_P_432",
      "abstract": "Interpretable deep learning (DL) models that can provide biological insights, in addition to accurate predictions, are of great interest to the biomedical community. Recently, interpretable DL models that incorporate signaling pathways have been proposed for drug response prediction (DRP). While these models improve interpretability, it is unclear whether this comes at the cost of less accurate DRPs, or a prediction improvement can also be obtained. We comprehensively and systematically assessed four state-of-the-art interpretable DL models using three pathway collections to assess their ability in making accurate predictions on unseen samples from the same dataset, as well as their generalizability to an independent dataset. Our results showed that models that explicitly incorporate pathway information in the form of a latent layer perform worse compared to models that incorporate this information implicitly. However, in most evaluation setups, the best performance was achieved using a black-box multilayer perceptron, and the performance of a random forests baseline was comparable to those of the interpretable models. Replacing the signaling pathways with randomly generated pathways showed a comparable performance for the majority of the models. Finally, the performance of all models deteriorated when applied to an independent dataset. These results highlight the importance of systematic evaluation of newly proposed models using carefully selected baselines. We provide different evaluation setups and baseline models that can be used to achieve this goal.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Interpretable deep learning (DL) models that can provide biological insights, in addition to accurate predictions, are of great interest to the biomedical community. Recently, interpretable DL models that incorporate signaling pathways have been proposed for drug response prediction (DRP). While these models improve interpretability, it is unclear whether this comes at the cost of less accurate DRPs, or a prediction improvement can also be obtained.",
          "Main Action": "compare",
          "Arguments": {
            "Agent": [
              "recently proposed interpretable DL models that incorporate signaling pathways",
              "traditional methods"
            ],
            "Object": {
              "Primary Object": [
                "their ability to predict drug responses",
                "their capability to provide biological insights"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "The passage does not contain explicit contextual information apart from discussing the integration of signaling pathways and the evaluation metrics.",
              "However, implicit context includes the competition between interpretability and predictive accuracy in DL models for DRP.",
              "Additionally, the absence of clear limitations suggests ongoing debates in the biomedical community about balancing these attributes."
            ],
            "Purpose": [
              "To evaluate and compare the capabilities of interpretable DL models against conventional approaches in terms of both biological insight provision and predictive accuracy for drug response prediction (DRP)."
            ],
            "Method": [
              "No explicit methodology details are provided except references to 'interpretable DL models' and 'signaling pathway integrations'."
            ],
            "Results": [
              "The abstract states that these models offer enhanced interpretability but doesnâ€™t specify changes in prediction accuracy relative to baseline methods.",
              "Furthermore, it notes that achieving higher interpretability may come at the expense of reduced accuracy or vice versa, implying inconclusive evidence on which benefit prevails."
            ],
            "Analysis": [
              "Analyze the balance between increased interpretability and maintained/potential decreased accuracy in DL models aimed at predicting drug responses (DRP)."
            ],
            "Challenge": [
              "Lack of explicit data on actual accuracy gains or losses in comparison to alternative methods leaves ambiguity about practical benefits.",
              "Unclear conclusions due to conflicting claims about the value of improved interpretability over raw accuracy measurements."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Potential ambiguities in prioritizing interpretability vs. accuracy highlight unresolved issues in DL application for medical research.",
              "May necessitate further studies to concretely establish the advantages of interpretable models over others in DRP contexts."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We comprehensively and systematically assessed four state-of-the-art interpretable DL models using three pathway collections to assess their ability in making accurate predictions on unseen samples from the same dataset, as well as their generalizability to an independent dataset.",
          "Main Action": "assess",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "four state-of-the-art interpretable DL models"
              ],
              "Secondary Object": [
                "three pathway collections"
              ]
            },
            "Context": [
              "comprehensive and systematic assessment"
            ],
            "Purpose": [
              "to assess their ability in making accurate predictions on unseen samples from the same dataset and their generalizability to an independent dataset"
            ],
            "Method": [
              "utilizing three pathway collections"
            ],
            "Results": [
              "made accurate predictions on unseen samples from the same dataset and their generalizability to an independent dataset"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Our results showed that models that explicitly incorporate pathway information in the form of a latent layer perform worse compared to models that incorporate this information implicitly. However, in most evaluation setups, the best performance was achieved using a black-box multilayer perceptron, and the performance of a random forests baseline was comparable to those of the interpretable models. Replacing the signaling pathways with randomly generated pathways showed a comparable performance for the majority of the models. Finally, the performance of all models deteriorated when applied to an independent dataset.",
          "Main Action": "performance of all models deteriorated",
          "Arguments": {
            "Agent": [
              "the entire set of evaluated models"
            ],
            "Object": {
              "Primary Object": [
                "independent dataset"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "The final comparison involved evaluating models on entirely new, unseen datasets."
            ],
            "Purpose": [
              "To assess whether the observed performance improvements were generalizable or limited to the training dataset."
            ],
            "Method": [
              "No specific methodology details provided except comparisons among described configurations."
            ],
            "Results": [
              "The performance metrics indicated a consistent decline across all evaluated models when tested against fresh, unprocessed data."
            ],
            "Analysis": [
              "Such degradation suggests potential limitations in the models' ability to generalize learned patterns to novel contexts."
            ],
            "Challenge": [
              "The challenge lies in maintaining high performance across diverse and unpredictable data environments."
            ],
            "Ethical": [
              "No ethical considerations discussed in relation to model performance degradation."
            ],
            "Implications": [
              "This finding implies that current modeling strategies may lack robustness when deployed in real-world scenarios with varied data distributions."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "These results highlight the importance of systematic evaluation of newly proposed models using carefully selected baselines. We provide different evaluation setups and baseline models that can be used to achieve this goal.",
          "Main Action": "highlight",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "the importance of systematic evaluation of newly proposed models"
              ],
              "Secondary Object": [
                "careful selection of baselines"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "bioinfo_23_P_838",
      "abstract": "In whole genome sequencing data, polymerase chain reaction amplification results in duplicate DNA fragments coming from the same location in the genome. The process of preparing a whole genome bisulfite sequencing (WGBS) library, on the other hand, can create two DNA fragments from the same location that should not be considered duplicates. Currently, only one WGBS-aware duplicate marking tool exists. However, it only works with the output from a single tool, does not accept streaming input or output, and requires a substantial amount of memory relative to the input size. Dupsifter provides an aligner-agnostic duplicate marking tool that is lightweight, has streaming capabilities, and is memory efficient.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "In whole genome sequencing data, polymerase chain reaction amplification results in duplicate DNA fragments coming from the same location in the genome. The process of preparing a whole genome bisulfite sequencing (WGBS) library, on the other hand, can create two DNA fragments from the same location that should not be considered duplicates. Currently, only one WGBS-aware duplicate marking tool exists. However, it only works with the output from a single tool, does not accept streaming input or output, and requires a substantial amount of memory relative to the input size.",
          "Main Action": "currently",
          "Arguments": {
            "Agent": [
              "genomic data processing"
            ],
            "Object": {
              "Primary Object": [
                "lack of effective solutions"
              ],
              "Secondary Object": [
                "streaming input support",
                "high memory requirements relative to input size"
              ]
            },
            "Context": [
              "advancements in next-generation sequencing technologies lead to higher throughput while reducing cost per sample; however, this increase complicates matters by creating larger datasets that require significant computational resources"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "only one WGBS-aware duplicate marking tool exists; it does not handle streaming input effectively nor manage large datasets efficiently"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Dupsifter provides an aligner-agnostic duplicate marking tool that is lightweight, has streaming capabilities, and is memory efficient.",
          "Main Action": "provides",
          "Arguments": {
            "Agent": [
              "Dupsifter"
            ],
            "Object": {
              "Primary Object": [
                "an aligner-agnostic duplicate marking tool"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}