{
  "papers": [
    {
      "paper_code": "ACL_23_P_314",
      "abstract": "Massively multilingual language models have displayed strong performance in zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer setups, where models fine-tuned on task data in a source language are transferred without any or with only a few annotated instances to the target language(s). However, current work typically overestimates model performance as fine-tuned models are frequently evaluated at model checkpoints that generalize best to validation instances in the target languages. This effectively violates the main assumptions of ‘true’ ZS-XLT and FS-XLT. Such XLT setups require robust methods that do not depend on labeled target language data for validation and model selection. In this work, aiming to improve the robustness of ‘true’ ZS-XLT and FS-XLT, we propose a simple and effective method that averages different checkpoints (i.e., model snapshots) during task fine-tuning. We conduct exhaustive ZS-XLT and FS-XLT experiments across higher-level semantic tasks (NLI, extractive QA) and lower-level token classification tasks (NER, POS). The results indicate that averaging model checkpoints yields systematic and consistent performance gains across diverse target languages in all tasks. Importantly, it simultaneously substantially desensitizes XLT to varying hyperparameter choices in the absence of target language validation. We also show that checkpoint averaging benefits performance when further combined with run averaging (i.e., averaging the parameters of models fine-tuned over independent runs).",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Massively multilingual language models have displayed strong performance in zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer setups, where models fine-tuned on task data in a source language are transferred without any or with only a few annotated instances to the target language(s). However, current work typically overestimates model performance as fine-tuned models are frequently evaluated at model checkpoints that generalize best to validation instances in the target languages. This effectively violates the main assumptions of ‘true’ ZS-XLT and FS-XLT. Such XLT setups require robust methods that do not depend on labeled target language data for validation and model selection.",
          "Main Action": "displayed strong performance",
          "Arguments": {
            "Agent": [
              "Massively multilingual language models"
            ],
            "Object": {
              "Primary Object": [
                "zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer setups"
              ],
              "Secondary Object": [
                "current work typically overestimates model performance"
              ]
            },
            "Context": [
              "However, current work typically overestimates model performance as fine-tuned models are frequently evaluated at model checkpoints that generalize best to validation instances in the target languages. This effectively violates the main assumptions of 'true' ZS-XLT and FS-XLT.",
              "Such XLT setups require robust methods that do not depend on labeled target language data for validation and model selection."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, aiming to improve the robustness of ‘true’ ZS-XLT and FS-XLT, we propose a simple and effective method that averages different checkpoints (i.e., model snapshots) during task fine-tuning. We conduct exhaustive ZS-XLT and FS-XLT experiments across higher-level semantic tasks (NLI, extractive QA) and lower-level token classification tasks (NER, POS).",
          "Main Action": "average",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "different checkpoints (i.e., model snapshots) during task fine-tuning"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "In this work, aiming to improve the robustness of 'true' ZS-XLT and FS-XLT"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "We conduct exhaustive ZS-XLT and FS-XLT experiments across higher-level semantic tasks (NLI, extractive QA) and lower-level token classification tasks (NER, POS)."
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The results indicate that averaging model checkpoints yields systematic and consistent performance gains across diverse target languages in all tasks. Importantly, it simultaneously substantially desensitizes XLT to varying hyperparameter choices in the absence of target language validation. We also show that checkpoint averaging benefits performance when further combined with run averaging (i.e., averaging the parameters of models fine-tuned over independent runs).",
          "Main Action": "averaging model checkpoints",
          "Arguments": {
            "Agent": [
              "averaging model checkpoints"
            ],
            "Object": {
              "Primary Object": [
                "yields systematic and consistent performance gains across diverse target languages in all tasks"
              ],
              "Secondary Object": [
                "simultaneously substantially desensitizes XLT to varying hyperparameter choices in the absence of target language validation"
              ]
            },
            "Context": [
              "Importantly, it simultaneously substantially desensitizes XLT to varying hyperparameter choices in the absence of target language validation."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "averaging model checkpoints",
              "combining checkpoint averaging with run averaging"
            ],
            "Results": [
              "systematic and consistent performance gains across diverse target languages in all tasks",
              "further combined with run averaging (i.e., averaging the parameters of models fine-tuned over independent runs)"
            ],
            "Analysis": [
              "Importantly, it simultaneously substantially desensitizes XLT to varying hyperparameter choices in the absence of target language validation.",
              "We also show that checkpoint averaging benefits performance when further combined with run averaging (i.e., averaging the parameters of models fine-tuned over independent runs)."
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "The findings demonstrate that systematically combining different model checkpoints and employing run averaging significantly enhances performance across varied linguistic contexts.",
              "Such strategies provide robust solutions to mitigating the impact of hyperparameter variability while maintaining effective performance levels."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_96",
      "abstract": "Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one’s mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy’s learning for responding. Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence.",
      "events": [
        {
          "Background/Introduction": "ERROR",
          "Text": "Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one’s mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        },
        {
          "Methods/Approach": "",
          "Text": "To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy’s learning for responding.",
          "Main Action": "introduce",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a new paradigm to formalize multi-turn EM as a process of positive emotion elicitation"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence."
            ],
            "Purpose": [
              "Fine-tune the interaction between Positive Emotion Elicitation Systems (ES) and Dialog Coherence Requirements."
            ],
            "Method": [
              "Design ES and dialogue coherence rewards to guide policy’s learning for responding."
            ],
            "Results": [
              "In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well-design ES and dialogue coherence rewards to guide policy’s learning for responding."
            ],
            "Analysis": [
              "Aiming to address the challenge of dynamically adjusting emotion elicitations in real-time conversations while preserving meaningful dialogue coherence."
            ],
            "Challenge": [
              "Maintaining the right balance between emotional expression and conversational relevance."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "The development opens doors for scalable solutions in handling nuanced human interactions involving emotional regulation across various domains."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence.",
          "Main Action": "experiments",
          "Arguments": {
            "Agent": [
              "the experiments"
            ],
            "Object": {
              "Primary Object": [
                "Supporter"
              ],
              "Secondary Object": [
                "Other methods ... achieving similar results in terms of effectiveness"
              ]
            },
            "Context": [
              "Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}