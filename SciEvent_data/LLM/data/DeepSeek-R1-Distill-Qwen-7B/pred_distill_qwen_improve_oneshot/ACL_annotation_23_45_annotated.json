{
  "papers": [
    {
      "paper_code": "ACL_23_P_619",
      "abstract": "Most research on stylized image captioning aims to generate style-specific captions using unpaired text, and has achieved impressive performance for simple styles like positive and negative. However, unlike previous single-sentence captions whose style is mostly embodied in distinctive words or phrases, real-world styles are likely to be implied at the syntactic and discourse levels. In this work, we introduce a new task of Stylized Visual Storytelling (SVST), which aims to describe a photo stream with stylized stories that are more expressive and attractive. We propose a multitasking memory-augmented framework called StyleVSG, which is jointly trained on factual visual storytelling data and unpaired style corpus, achieving a trade-off between style accuracy and visual relevance. Particularly for unpaired stylized text, StyleVSG learns to reconstruct the stylistic story from roughly parallel visual inputs mined with the CLIP model, avoiding problems caused by random mapping in previous methods. Furthermore, a memory module is designed to preserve the consistency and coherence of generated stories. Experiments show that our method can generate attractive and coherent stories with different styles such as fairy tale, romance, and humor. The overall performance of our StyleVSG surpasses state-of-the-art methods on both automatic and human evaluation metrics.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Most research on stylized image captioning aims to generate style-specific captions using unpaired text, and has achieved impressive performance for simple styles like positive and negative. However, unlike previous single-sentence captions whose style is mostly embodied in distinctive words or phrases, real-world styles are likely to be implied at the syntactic and discourse levels.",
          "Main Action": "aims",
          "Arguments": {
            "Agent": [
              "most research on stylized image captioning"
            ],
            "Object": {
              "Primary Object": [
                "generate style-specific captions using unpaired text"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "and has achieved impressive performance for simple styles like positive and negative.",
              "However, unlike previous single-sentence captions whose style is mostly embodied in distinctive words or phrases,",
              "real-world styles are likely to be implied at the syntactic and discourse levels."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Challenges include attempting to address complex real-world styles implied at syntactic and discourse levels."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Improving our ability to handle stylistic nuances could extend capabilities of AI systems in cultural expression processing."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we introduce a new task of Stylized Visual Storytelling (SVST), which aims to describe a photo stream with stylized stories that are more expressive and attractive. We propose a multitasking memory-augmented framework called StyleVSG, which is jointly trained on factual visual storytelling data and unpaired style corpus, achieving a trade-off between style accuracy and visual relevance. Particularly for unpaired stylized text, StyleVSG learns to reconstruct the stylistic story from roughly parallel visual inputs mined with the CLIP model, avoiding problems caused by random mapping in previous methods. Furthermore, a memory module is designed to preserve the consistency and coherence of generated stories.",
          "Main Action": "aim",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "Stylish Visual Storytelling (SVST)"
              ],
              "Secondary Object": [
                "a framework called StyleVSG"
              ]
            },
            "Context": [
              "describes a photo stream with stylized stories that are more expressive and attractive"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "memory-augmented framework called StyleVSG, joint training on factual visual storytelling data and unpaired style corpus, learning to reconstruct stylistic story from visually parallel visual inputs mined with the CLIP model"
            ],
            "Results": [
              "achieving a trade-off between style accuracy and visual relevance"
            ],
            "Analysis": [
              "avoiding problems caused by random mapping in previous methods"
            ],
            "Challenge": [
              "problems caused by random mapping in previous methods"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader significance for future applications in visual-NLP integration"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experiments show that our method can generate attractive and coherent stories with different styles such as fairy tale, romance, and humor. The overall performance of our StyleVSG surpasses state-of-the-art methods on both automatic and human evaluation metrics.",
          "Main Action": "Experiments show",
          "Arguments": {
            "Agent": [
              "Experiments"
            ],
            "Object": {
              "Primary Object": [
                "our method can generate attractive and coherent stories with different styles such as fairy tale, romance, and humor"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "The overall performance of our StyleVSG surpasses state-of-the-art methods on both automatic and human evaluation metrics.",
              "Experiments show that our method can generate attractive and coherent stories with different styles such as fairy tale, romance, and humor."
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_380",
      "abstract": "Grammatical error correction (GEC) can be divided into sequence-to-edit (Seq2Edit) and sequence-to-sequence (Seq2Seq) frameworks, both of which have their pros and cons. To utilize the strengths and make up for the shortcomings of these frameworks, this paper proposes a novel method, TemplateGEC, which capitalizes on the capabilities of both Seq2Edit and Seq2Seq frameworks in error detection and correction respectively. TemplateGEC utilizes the detection labels from a Seq2Edit model, to construct the template as the input. A Seq2Seq model is employed to enforce consistency between the predictions of different templates by utilizing consistency learning. Experimental results on the Chinese NLPCC18, English BEA19 and CoNLL14 benchmarks show the effectiveness and robustness of TemplateGEC. Further analysis reveals the potential of our method in performing human-in-the-loop GEC.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Grammatical error correction (GEC) can be divided into sequence-to-edit (Seq2Edit) and sequence-to-sequence (Seq2Seq) frameworks, both of which have their pros and cons. To utilize the strengths and make up for the shortcomings of these frameworks, this paper proposes a novel method, TemplateGEC, which capitalizes on the capabilities of both Seq2Edit and Seq2Seq frameworks in error detection and correction respectively.",
          "Main Action": "capitalizes",
          "Arguments": {
            "Agent": [
              "this paper"
            ],
            "Object": {
              "Primary Object": [
                "on the capability of combining the strengths of both Seq2Edit and Seq2Seq frameworks in error detection and correction"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "while Grammatical Error Correction (GEC) can be divided into Sequence-to-edit (Seq2Edit) and Sequence-to-sequence (Seq2Seq) frameworks, both of which have their pros and cons"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "addressing contradictions between the two frameworks"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "TemplateGEC utilizes the detection labels from a Seq2Edit model, to construct the template as the input. A Seq2Seq model is employed to enforce consistency between the predictions of different templates by utilizing consistency learning.",
          "Main Action": "utilizes the detection labels from",
          "Arguments": {
            "Agent": [
              "Seq2Edit model"
            ],
            "Object": {
              "Primary Object": [
                "a Seq2Seq model"
              ],
              "Secondary Object": [
                "to enforce consistency between predictions"
              ]
            },
            "Context": [
              "A Seq2Edit model produces outputs that are fed into a Seq2Seq model to ensure consistency among different prediction templates by employing consistency learning"
            ],
            "Purpose": [
              "To enhance the robustness of neural machine translation models against adversarial attacks through systematic evaluation and optimization"
            ],
            "Method": [
              "Pre-training on large datasets, fine-tuning using consistency learning, and leveraging pre-existing models as templates"
            ],
            "Results": [
              "Improvements include better alignment of predicted outputs across different templates, leading to more reliable and accurate translations despite adversarial manipulations"
            ],
            "Analysis": [
              "Consistency learning helps mitigate discrepancies caused by varying initialization conditions and differing label distributions across templates"
            ],
            "Challenge": [
              "Maintaining high-quality alignments while scaling up computational resources required for training complex models may pose challenges"
            ],
            "Ethical": [
              "There are ongoing discussions about balancing interpretability with black-box decision-making inherent in current architectures"
            ],
            "Implications": [
              "This advancement opens new avenues for developing more resilient AI systems capable of handling diverse linguistic inputs effectively"
            ],
            "Contradictions": [
              "None evident in the text provided"
            ]
          }
        },
        {
          "Results/Findings": "ERROR",
          "Text": "Experimental results on the Chinese NLPCC18, English BEA19 and CoNLL14 benchmarks show the effectiveness and robustness of TemplateGEC. Further analysis reveals the potential of our method in performing human-in-the-loop GEC.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        }
      ]
    }
  ]
}