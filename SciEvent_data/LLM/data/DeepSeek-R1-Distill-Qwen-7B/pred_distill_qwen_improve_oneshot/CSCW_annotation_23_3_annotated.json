{
  "papers": [
    {
      "paper_code": "cscw_23_P_179",
      "abstract": "Algorithmic risk assessments are being deployed in an increasingly broad spectrum of domains including banking, medicine, and law enforcement. However, there is widespread concern about their fairness and trustworthiness, and people are also known to display algorithm aversion, preferring human assessments even when they are quantitatively worse. Thus, how does the framing of who made an assessment affect how people perceive its fairness? We investigate whether individual algorithmic assessments are perceived to be more or less accurate, fair, and interpretable than identical human assessments, and explore how these perceptions change when assessments are obviously biased against a subgroup. To this end, we conducted an online experiment that manipulated how biased risk assessments are in a loan repayment task, and reported the assessments as being made either by a statistical model or a human analyst. We find that predictions made by the model are consistently perceived as less fair and less interpretable than those made by the analyst despite being identical. Furthermore, biased predictive errors were more likely to widen this perception gap, with the algorithm being judged even more harshly for making a biased mistake. Our results illustrate that who makes risk assessments can influence perceptions of how acceptable those assessments are - even if they are identically accurate and identically biased against subgroups. Additional work is needed to determine whether and how decision aids should be presented to stakeholders so that the inherent fairness and interpretability of their recommendations, rather than their framing, determines how they are perceived.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Algorithmic risk assessments are being deployed in an increasingly broad spectrum of domains including banking, medicine, and law enforcement. However, there is widespread concern about their fairness and trustworthiness, and people are also known to display algorithm aversion, preferring human assessments even when they are quantitatively worse. Thus, how does the framing of who made an assessment affect how people perceive its fairness?",
          "Main Action": "discuss",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "albeit widely concerned about their fairness and trustworthiness, and people are also known to display algorithm aversion, preferring human assessments even when they are quantitatively worse"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Algorithmic risk assessments are being deployed in an increasingly broad spectrum of domains including banking, medicine, and law enforcement."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "However, there is widespread concern about their fairness and trustworthiness, and people are also known to display algorithm aversion, preferring human assessments even when they are quantitatively worse."
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We investigate whether individual algorithmic assessments are perceived to be more or less accurate, fair, and interpretable than identical human assessments, and explore how these perceptions change when assessments are obviously biased against a subgroup. To this end, we conducted an online experiment that manipulated how biased risk assessments are in a loan repayment task, and reported the assessments as being made either by a statistical model or a human analyst.",
          "Main Action": "explore how these perceptions change",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "perceived to be more or less accurate, fair, and interpretable"
              ],
              "Secondary Object": [
                "how these perceptions change when assessments are obviously biased against a subgroup"
              ]
            },
            "Context": [
              "To this end, we conducted an online experiment that manipulated how biased risk assessments are in a loan repayment task, and reported the assessments as being made either by a statistical model or a human analyst."
            ],
            "Purpose": [
              "without explicitly stating all possible reasons, our focus lies on investigating the impact of bias presentation on human perception of algorithmic assessments compared to human assessments."
            ],
            "Method": [
              "manipulated bias levels in risk assessments, presented them as either human-made or algorithmic, recruited human subjects for evaluation,"
            ],
            "Results": [
              "participants systematically perceived differences in bias depending on the assumed origin of the assessment tool."
            ],
            "Analysis": [
              "interpretations varied among participants concerning discrepancies introduced by differing origins of the assessments."
            ],
            "Challenge": [
              "no inherent challenge is discussed; instead, the focus is on identifying patterns in human perception rather than addressing technical issues."
            ],
            "Ethical": [
              "discussions touch upon ethical considerations surrounding bias in AI decisions but do not provide explicit Justification or Implications.",
              [
                "However, the findings highlight the importance of considering bias in AI decision-making processes."
              ]
            ],
            "Implications": [
              "these findings imply that designers and users of AI systems should be aware of potential biases in how AI outputs are perceived relative to human assessments.",
              "Additionally, interventions may include designing transparent reporting mechanisms or educational programs to mitigate misperceptions."
            ],
            "Contradictions": [
              "there are none indicated within the text; the discussion focuses on establishing baseline comparisons between human and algorithmic assessments."
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We find that predictions made by the model are consistently perceived as less fair and less interpretable than those made by the analyst despite being identical. Furthermore, biased predictive errors were more likely to widen this perception gap, with the algorithm being judged even more harshly for making a biased mistake. Our results illustrate that who makes risk assessments can influence perceptions of how acceptable those assessments are - even if they are identically accurate and identically biased against subgroups.",
          "Main Action": "find",
          "Arguments": {
            "Agent": [
              "researchers analyzing model predictions"
            ],
            "Object": {
              "Primary Object": [
                "perceptions of fairness and interpretability"
              ],
              "Secondary Object": [
                "biased mistakes"
              ]
            },
            "Context": [
              "while some may be aware of bias in AI decisions, few recognize the extent of unfairness in their own biased judgments"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "inherent biases in AI can lead to unjustified distrust in decision-making processes"
            ],
            "Ethical": [
              "ethical concerns arise regarding fairness and transparency in AI decision-making"
            ],
            "Implications": [
              "this highlights the need for policies to ensure equitable oversight of AI systems across various sectors"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "ERROR",
          "Text": "Additional work is needed to determine whether and how decision aids should be presented to stakeholders so that the inherent fairness and interpretability of their recommendations, rather than their framing, determines how they are perceived.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_92",
      "abstract": "Group deliberation enables people to collaborate and solve problems, however, it is understudied due to a lack of resources. To this end, we introduce the first publicly available dataset containing collaborative conversations on solving a well-established cognitive task, consisting of 500 group dialogues and 14k utterances. In 64% of these conversations, the group members are able to find a better solution than they had identified individually, and in 43.8% of the groups who had a correct answer as their final solution, none of the participants had solved the task correctly by themselves. Furthermore, we propose a novel annotation schema that captures deliberation cues and release all 14k utterances annotated with it. Finally, we use the proposed dataset to develop and evaluate two methods for generating deliberation utterances.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Group deliberation enables people to collaborate and solve problems, however, it is understudied due to a lack of resources.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Group deliberation enables people to collaborate and solve problems, however, it is understudied due to a lack of resources."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To this end, we introduce the first publicly available dataset containing collaborative conversations on solving a well-established cognitive task, consisting of 500 group dialogues and 14k utterances. In 64% of these conversations, the group members are able to find a better solution than they had identified individually, and in 43.8% of the groups who had a correct answer as their final solution, none of the participants had solved the task correctly by themselves. Furthermore, we propose a novel annotation schema that captures deliberation cues and release all 14k utterances annotated with it. Finally, we use the proposed dataset to develop and evaluate two methods for generating deliberation utterances.",
          "Main Action": "introduce",
          "Arguments": {
            "Agent": [
              "to this end"
            ],
            "Object": {
              "Primary Object": [
                "a publically available dataset containing collaborative conversations on solving a well-established cognitive task"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}