{
  "papers": [
    {
      "paper_code": "ACL_23_P_671",
      "abstract": "Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations. In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which, as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations. We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY’s supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks. TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "Temporal reasoning models"
            ],
            "Object": {
              "Primary Object": [
                "can assess their ability to predict temporal relations of event pairs across diverse scenarios"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "While temporal reasoning models can perform reasonably well on in-domain benchmarks"
            ],
            "Purpose": [
              "to evaluate whether these models can generalize their capabilities beyond their training domains."
            ],
            "Method": [
              "existing datasets' limitations affect our understanding of their generalizability"
            ],
            "Results": [
              "we recognize that without larger-scale benchmarks addressing diverse scenarios, many questions remain unanswered."
            ],
            "Analysis": [
              "this raises critical questions about the extent to which these models truly understand temporal relationships."
            ],
            "Challenge": [
              "current evaluation metrics may not adequately capture real-world complexities."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "therefore, developing robust evaluation frameworks is essential for advancing the field of temporal reasoning."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which, as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations.",
          "Main Action": "introduce",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a novel task named TODAY that bridges this gap with temporal differential analysis"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "in this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis"
            ],
            "Purpose": [
              "to evaluate whether systems can correctly understand the effect of incremental changes"
            ],
            "Method": [
              "specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations."
            ],
            "Results": [
              "To facilitate learning, TODAY also annotates human explanations."
            ],
            "Analysis": [
              "Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions."
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "By introducing this novel task and methodology, the study contributes to advancing our ability to analyze systems' responses to incremental changes in event contexts."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY’s supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "<NONE>"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems.",
          "Main Action": "train",
          "Arguments": {
            "Agent": [
              "TODAY"
            ],
            "Object": {
              "Primary Object": [
                "models"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "incidental supervision from noisy sources such as GPT-3.5"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "machine learning methodologies, pre-trained large-scale models common in AI platforms"
            ],
            "Results": [
              "moving us more toward the goal of generic temporal reasoning systems"
            ],
            "Analysis": [
              "incremental advancements in temporal reasoning capability"
            ],
            "Challenge": [
              "reliance on human judgment due to script complexity"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "enhanced multilingual NLP systems applicability in digital spaces with standardized/nonstandardized scripts"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_65",
      "abstract": "Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations. To address this issue, we propose a novel continual extraction model for analogous relations. Specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. We also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations. Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations.",
          "Main Action": "observes that performance is severely affected",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "performance is severely affected by analogous relations"
              ],
              "Secondary Object": [
                "and the reasons behind this issue"
              ]
            },
            "Context": [
              "there were insufficient studies focusing on this particular challenge regarding analogical relations causing performance degradation across similar reasoning problems"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "in their analysis, they highlight the critical nature of addressing this issue"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "their findings imply the need for further research to develop strategies mitigating the impact of analogical relations"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To address this issue, we propose a novel continual extraction model for analogous relations. Specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. We also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a novel continual extraction model for analogous relations"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "to address this issue, we propose a novel continual extraction model for analogous relations"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "designing memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. we also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations"
            ],
            "Results": [
              "specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. we also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting.",
          "Main Action": "show",
          "Arguments": {
            "Agent": [
              "Experimental results"
            ],
            "Object": {
              "Primary Object": [
                "the superiority of our model"
              ],
              "Secondary Object": [
                "its effectiveness in distinguishing analogous relations and overcoming overfitting"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting."
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}