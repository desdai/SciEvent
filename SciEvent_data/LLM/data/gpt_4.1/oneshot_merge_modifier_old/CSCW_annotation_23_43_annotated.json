{
  "papers": [
    {
      "paper_code": "cscw_23_P_87",
      "abstract": "How can online communities execute a focused vision for their space? Curation offers one approach, where community leaders manually select content to share with the community. Curation enables leaders to shape a space that matches their taste, norms, and values, but the practice is often intractable at social media scale: curators cannot realistically sift through hundreds or thousands of submissions daily. In this paper, we contribute algorithmic and interface foundations enabling curation at scale, and manifest these foundations in a system called Cura. Our approach draws on the observation that, while curators' attention is limited, other community members' upvotes are plentiful and informative of curators' likely opinions. We thus contribute a transformer-based curation model that predicts whether each curator will upvote a post based on previous community upvotes. Cura applies this curation model to create a feed of content that it predicts the curator would want in the community. Evaluations demonstrate that the curation model accurately estimates opinions of diverse curators, that changing curators for a community results in clearly recognizable shifts in the community's content, and that, consequently, curation can reduce anti-social behavior by half without extra moderation effort. By sampling different types of curators, Cura lowers the threshold to genres of curated social media ranging from editorial groups to stakeholder roundtables to democracies.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "How can online communities execute a focused vision for their space? Curation offers one approach, where community leaders manually select content to share with the community. Curation enables leaders to shape a space that matches their taste, norms, and values, but the practice is often intractable at social media scale: curators cannot realistically sift through hundreds or thousands of submissions daily.",
          "Main Action": "offers",
          "Arguments": {
            "Agent": [
              "Curation"
            ],
            "Object": {
              "Primary Object": [
                "one approach"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "How can online communities execute a focused vision for their space?"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "where community leaders manually select content to share with the community"
            ],
            "Results": [
              "Curation enables leaders to shape a space that matches their taste, norms, and values"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "the practice is often intractable at social media scale: curators cannot realistically sift through hundreds or thousands of submissions daily."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we contribute algorithmic and interface foundations enabling curation at scale, and manifest these foundations in a system called Cura. Our approach draws on the observation that, while curators' attention is limited, other community members' upvotes are plentiful and informative of curators' likely opinions. We thus contribute a transformer-based curation model that predicts whether each curator will upvote a post based on previous community upvotes. Cura applies this curation model to create a feed of content that it predicts the curator would want in the community.",
          "Main Action": "contribute",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "algorithmic and interface foundations enabling curation at scale"
              ],
              "Secondary Object": [
                "a transformer-based curation model that predicts whether each curator will upvote a post based on previous community upvotes"
              ]
            },
            "Context": [
              "In this paper"
            ],
            "Purpose": [
              "enabling curation at scale"
            ],
            "Method": [
              "draws on the observation that, while curators' attention is limited, other community members' upvotes are plentiful and informative of curators' likely opinions"
            ],
            "Results": [
              "manifest these foundations in a system called Cura",
              "Cura applies this curation model to create a feed of content that it predicts the curator would want in the community"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "curators' attention is limited"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Evaluations demonstrate that the curation model accurately estimates opinions of diverse curators, that changing curators for a community results in clearly recognizable shifts in the community's content, and that, consequently, curation can reduce anti-social behavior by half without extra moderation effort.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "Evaluations"
            ],
            "Object": {
              "Primary Object": [
                "that the curation model accurately estimates opinions of diverse curators",
                "that changing curators for a community results in clearly recognizable shifts in the community's content",
                "that, consequently, curation can reduce anti-social behavior by half without extra moderation effort"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "the curation model accurately estimates opinions of diverse curators",
              "changing curators for a community results in clearly recognizable shifts in the community's content",
              "curation can reduce anti-social behavior by half without extra moderation effort"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "By sampling different types of curators, Cura lowers the threshold to genres of curated social media ranging from editorial groups to stakeholder roundtables to democracies.",
          "Main Action": "lowers the threshold to genres of curated social media ranging from editorial groups to stakeholder roundtables to democracies",
          "Arguments": {
            "Agent": [
              "Cura"
            ],
            "Object": {
              "Primary Object": [
                "the threshold to genres of curated social media ranging from editorial groups to stakeholder roundtables to democracies"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "By sampling different types of curators"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "sampling different types of curators"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "cscw_23_P_127",
      "abstract": "Online harassment and content moderation have been well-documented in online communities. However, new contexts and systems always bring new ways of harassment and need new moderation mechanisms. This study focuses on hate raids, a form of group attack in real-time in live streaming communities. Through a qualitative analysis of hate raids discussion in the Twitch subreddit (r/Twitch), we found that (1) hate raids as a human-bot coordinated group attack leverages the live stream system to attack marginalized streamers and other potential groups with(out) breaking the rules, (2) marginalized streamers suffer compound harms with insufficient support from the platform, (3) moderation strategies are overwhelmingly technical, but streamers still struggle to balance moderation and participation considering their marginalization status and needs. We use affordances as a lens to explain how hate raids happens in live streaming systems and propose moderation-by-design as a lens when developing new features or systems to mitigate the potential abuse of such designs.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Online harassment and content moderation have been well-documented in online communities. However, new contexts and systems always bring new ways of harassment and need new moderation mechanisms. This study focuses on hate raids, a form of group attack in real-time in live streaming communities.",
          "Main Action": "focuses on",
          "Arguments": {
            "Agent": [
              "This study"
            ],
            "Object": {
              "Primary Object": [
                "hate raids, a form of group attack in real-time in live streaming communities"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Online harassment and content moderation have been well-documented in online communities.",
              "new contexts and systems always bring new ways of harassment and need new moderation mechanisms."
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Through a qualitative analysis of hate raids discussion in the Twitch subreddit (r/Twitch), we found that (1) hate raids as a human-bot coordinated group attack leverages the live stream system to attack marginalized streamers and other potential groups with(out) breaking the rules, (2) marginalized streamers suffer compound harms with insufficient support from the platform, (3) moderation strategies are overwhelmingly technical, but streamers still struggle to balance moderation and participation considering their marginalization status and needs.",
          "Main Action": "found",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "that (1) hate raids as a human-bot coordinated group attack leverages the live stream system to attack marginalized streamers and other potential groups with(out) breaking the rules, (2) marginalized streamers suffer compound harms with insufficient support from the platform, (3) moderation strategies are overwhelmingly technical, but streamers still struggle to balance moderation and participation considering their marginalization status and needs."
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Through a qualitative analysis of hate raids discussion in the Twitch subreddit (r/Twitch)"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "qualitative analysis of hate raids discussion in the Twitch subreddit (r/Twitch)"
            ],
            "Results": [
              "hate raids as a human-bot coordinated group attack leverages the live stream system to attack marginalized streamers and other potential groups with(out) breaking the rules",
              "marginalized streamers suffer compound harms with insufficient support from the platform",
              "moderation strategies are overwhelmingly technical, but streamers still struggle to balance moderation and participation considering their marginalization status and needs"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "We use affordances as a lens to explain how hate raids happens in live streaming systems and propose moderation-by-design as a lens when developing new features or systems to mitigate the potential abuse of such designs.",
          "Main Action": "propose",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "moderation-by-design as a lens when developing new features or systems to mitigate the potential abuse of such designs"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "to mitigate the potential abuse of such designs"
            ],
            "Method": [
              "use affordances as a lens to explain how hate raids happens in live streaming systems"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}