{
  "papers": [
    {
      "paper_code": "ACL_23_P_540",
      "abstract": "Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our attack BADCODE features a special trigger generation and injection procedure, making the attack more effective and stealthy. The evaluation is conducted on two neural code search models and the results show our attack outperforms baselines by 60%. Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "developers"
            ],
            "Object": {
              "Primary Object": [
                "code snippets"
              ],
              "Secondary Object": [
                "natural language queries",
                "neural code search models"
              ]
            },
            "Context": [
              "reusing off-the-shelf code snippets",
              "common practice among software developers"
            ],
            "Purpose": [
              "enhance productivity"
            ],
            "Method": [
              "resort to code search engines",
              "use neural code search models"
            ],
            "Results": [
              "impressive performance"
            ],
            "Analysis": [
              "security aspect is rarely studied",
              "adversary can inject a backdoor",
              "returning buggy or vulnerable code with security/privacy issues"
            ],
            "Challenge": [
              "impact on downstream software",
              "financial loss",
              "life-threatening incidents"
            ],
            "Ethical": [
              "ethical concerns about security vulnerabilities introduced into critical systems"
            ],
            "Implications": [
              "broader significance for future applications/research in secure coding practices"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our attack BADCODE features a special trigger generation and injection procedure, making the attack more effective and stealthy.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "such attacks are feasible and can be quite stealthy"
              ],
              "Secondary Object": [
                "one variable/function name",
                "buggy/vulnerable code"
              ]
            },
            "Context": [
              "in this paper"
            ],
            "Purpose": [
              "make buggy/vulnerable code rank in the top 11%"
            ],
            "Method": [
              "modifying one variable/function name",
              "special trigger generation and injection procedure"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "more effective and stealthy"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "feasible and can be quite stealthy"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "The evaluation is conducted on two neural code search models and the results show our attack outperforms baselines by 60%. Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "our attack"
            ],
            "Object": {
              "Primary Object": [
                "two neural code search models",
                "baselines",
                "user study"
              ],
              "Secondary Object": [
                "F1 score"
              ]
            },
            "Context": [
              "evaluation",
              "stealthiness based on the F1 score"
            ],
            "Purpose": [
              "to demonstrate performance improvement over baselines",
              "to evaluate stealthiness compared to baselines"
            ],
            "Method": [
              "conducting attacks",
              "performing evaluations using F1 score"
            ],
            "Results": [
              "outperforms baselines by 60%",
              "more stealthy than the baseline by two times"
            ],
            "Analysis": [
              "based on the F1 score"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "broader significance for future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_835",
      "abstract": "To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources. Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness. This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a pool of dependable annotators"
              ],
              "Secondary Object": [
                "difficult tasks"
              ]
            },
            "Context": [
              "To prevent the costly and inefficient use of resources on low-quality annotations",
              "creating"
            ],
            "Purpose": [
              "for creating a pool of dependable annotators who can effectively complete difficult tasks"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources.",
          "Main Action": "investigate",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "recruitment of high-quality Amazon Mechanical Turk workers"
              ],
              "Secondary Object": [
                "two-step pipeline"
              ]
            },
            "Context": [
              "via a two-step pipeline"
            ],
            "Purpose": [
              "filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources"
            ],
            "Method": [
              "a two-step pipeline"
            ],
            "Results": [
              "can successfully filter out subpar workers",
              "obtain high-agreement annotations"
            ],
            "Analysis": [
              "similar constraints on resources"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "our workers",
              "CloudResearch workers"
            ],
            "Object": {
              "Primary Object": [
                "expert judgments"
              ],
              "Secondary Object": [
                "data"
              ]
            },
            "Context": [
              "their alignment",
              "a subset of the data"
            ],
            "Purpose": [
              "further training in correctness"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "not as expected"
            ],
            "Analysis": [
              "alignment with expert judgments on a subset of the data is not as expected"
            ],
            "Challenge": [
              "needs further training in correctness"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "This paper"
            ],
            "Object": {
              "Primary Object": [
                "best practice"
              ],
              "Secondary Object": [
                "recruitment of qualified annotators in other challenging annotation tasks"
              ]
            },
            "Context": [
              "for the recruitment of qualified annotators in other challenging annotation tasks"
            ],
            "Purpose": [
              "to serve as a reference or guideline"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}