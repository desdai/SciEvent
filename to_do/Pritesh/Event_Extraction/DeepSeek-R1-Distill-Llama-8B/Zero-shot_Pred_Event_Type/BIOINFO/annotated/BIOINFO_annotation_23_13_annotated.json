{
  "papers": [
    {
      "paper_code": "bioinfo_23_P_275",
      "abstract": "Researchers usually conduct statistical analyses based on models built on raw data collected from individual participants (individual-level data). There is a growing interest in enhancing inference efficiency by incorporating aggregated summary information from other sources, such as summary statistics on genetic markers' marginal associations with a given trait generated from genome-wide association studies. However, combining high-dimensional summary data with individual-level data using existing integrative procedures can be challenging due to various numeric issues in optimizing an objective function over a large number of unknown parameters. We develop a procedure to improve the fitting of a targeted statistical model by leveraging external summary data for more efficient statistical inference (both effect estimation and hypothesis testing). To make this procedure scalable to high-dimensional summary data, we propose a divide-and-conquer strategy by breaking the task into easier parallel jobs, each fitting the targeted model by integrating the individual-level data with a small proportion of summary data. We obtain the final estimates of model parameters by pooling results from multiple fitted models through the minimum distance estimation procedure. We improve the procedure for a general class of additive models commonly encountered in genetic studies. We further expand these two approaches to integrate individual-level and high-dimensional summary data from different study populations. We demonstrate the advantage of the proposed methods through simulations and an application to the study of the effect on pancreatic cancer risk by the polygenic risk score defined by BMI-associated genetic markers.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Researchers usually conduct statistical analyses based on models built on raw data collected from individual participants (individual-level data). There is a growing interest in enhancing inference efficiency by incorporating aggregated summary information from other sources, such as summary statistics on genetic markers' marginal associations with a given trait generated from genome-wide association studies. However, combining high-dimensional summary data with individual-level data using existing integrative procedures can be challenging due to various numeric issues in optimizing an objective function over a large number of unknown parameters.",
          "Main Action": "There is a growing interest in enhancing inference efficiency",
          "Arguments": {
            "Agent": [
              "Researchers"
            ],
            "Object": {
              "Primary Object": [
                "high-dimensional summary data"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "combining high-dimensional summary data with individual-level data using existing integrative procedures"
            ],
            "Purpose": [
              "enhancing inference efficiency"
            ],
            "Method": [
              "existing integrative procedures"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "various numeric issues in optimizing an objective function over a large number of unknown parameters"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "We develop a procedure to improve the fitting of a targeted statistical model by leveraging external summary data for more efficient statistical inference (both effect estimation and hypothesis testing). To make this procedure scalable to high-dimensional summary data, we propose a divide-and-conquer strategy by breaking the task into easier parallel jobs, each fitting the targeted model by integrating the individual-level data with a small proportion of summary data. We obtain the final estimates of model parameters by pooling results from multiple fitted models through the minimum distance estimation procedure. We improve the procedure for a general class of additive models commonly encountered in genetic studies. We further expand these two approaches to integrate individual-level and high-dimensional summary data from different study populations.",
          "Main Action": "Develop",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "a procedure to improve the fitting of a targeted statistical model"
              ],
              "Secondary Object": [
                "summary data"
              ]
            },
            "Context": [
              "genetic studies"
            ],
            "Purpose": [
              "to make this procedure scalable to high-dimensional summary data"
            ],
            "Method": [
              "dividing the task into easier parallel jobs",
              "breaking the task into easier parallel jobs",
              "fitting the targeted model by integrating the individual-level data with a small proportion of summary data",
              "pooling results from multiple fitted models through the minimum distance estimation procedure"
            ],
            "Results": [
              "obtaining the final estimates of model parameters"
            ],
            "Analysis": [
              "These results provide insights into improving statistical modeling in genetic studies"
            ],
            "Challenge": [
              "computational demands"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "expanding applicability across diverse study populations"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We demonstrate the advantage of the proposed methods through simulations and an application to the study of the effect on pancreatic cancer risk by the polygenic risk score defined by BMI-associated genetic markers.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "the polygenic risk score defined by BMI-associated genetic markers"
              ],
              "Secondary Object": [
                "pancreatic cancer risk"
              ]
            },
            "Context": [
              "the study of the effect on pancreatic cancer risk"
            ],
            "Purpose": [
              "to show the advantage of the proposed methods"
            ],
            "Method": [
              "simulations and an application"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "bioinfo_23_P_352",
      "abstract": "We describe a compression scheme for BUS files and an implementation of the algorithm in the BUStools software. Our compression algorithm yields smaller file sizes than gzip, at significantly faster compression and decompression speeds. We evaluated our algorithm on 533 BUS files from scRNA-seq experiments with a total size of 1TB. Our compression is 2.2x faster than the fastest gzip option, 35% slower than the fastest zstd option, and results in 1.5x smaller files than both methods. This amounts to an 8.3x reduction in the file size, resulting in a compressed size of 122GB for the dataset.",
      "events": [
        {
          "Methods/Approach": "ERROR",
          "Text": "We describe a compression scheme for BUS files and an implementation of the algorithm in the BUStools software.",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        },
        {
          "Results/Findings": "",
          "Text": "Our compression algorithm yields smaller file sizes than gzip, at significantly faster compression and decompression speeds. We evaluated our algorithm on 533 BUS files from scRNA-seq experiments with a total size of 1TB. Our compression is 2.2x faster than the fastest gzip option, 35% slower than the fastest zstd option, and results in 1.5x smaller files than both methods. This amounts to an 8.3x reduction in the file size, resulting in a compressed size of 122GB for the dataset.",
          "Main Action": "evaluated",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "our compression algorithm"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "BUS files from scRNA-seq experiments with a total size of 1TB"
            ],
            "Purpose": [
              "comparing our algorithm's performance against existing methods"
            ],
            "Method": [
              "evaluating our algorithm on 533 BUS files",
              "comparing compression speeds and file sizes"
            ],
            "Results": [
              "2.2x faster compression and decompression speeds",
              "1.5x smaller files than both gzip and zstd",
              "8.3x reduction in file size leading to a compressed size of 122GB"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}