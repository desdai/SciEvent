{
  "papers": [
    {
      "paper_code": "ACL_23_P_587",
      "abstract": "Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE). To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or, in a separate line of work, contrasts positive and negative sets to improve faithfulness. While effective, much of this work has focused on how to generate and optimize these sets. Less is known about why one setup is more effective than another. In this work, we uncover the underlying characteristics of effective sets. For each training instance, we form a large, diverse pool of candidates and systematically vary the subsets used for calibration fine-tuning. Each selection strategy targets distinct aspects of the sets, such as lexical diversity or the size of the gap between positive and negatives. On three diverse scientific long-form summarization datasets (spanning biomedical, clinical, and chemical domains), we find, among others, that faithfulness calibration is optimal when the negative sets are extractive and more likely to be generated, whereas for relevance calibration, the metric margin between candidates should be maximized and surprise–the disagreement between model and metric defined candidate rankings–minimized.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE). To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or, in a separate line of work, contrasts positive and negative sets to improve faithfulness. While effective, much of this work has focused on how to generate and optimize these sets. Less is known about why one setup is more effective than another.",
          "Main Action": "To address this",
          "Arguments": {
            "Agent": [
              "Recent work",
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "Poorly calibrated to quality metrics"
              ],
              "Secondary Object": [
                "Calibration step",
                "contrasts positive and negative sets"
              ]
            },
            "Context": [
              "Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE)"
            ],
            "Purpose": [
              "to improve relevance",
              "to improve faithfulness"
            ],
            "Method": [
              "exposes a model to its own ranked outputs",
              "contrasts positive and negative sets"
            ],
            "Results": [
              "While effective",
              "much of this work has focused on how to generate and optimize these sets"
            ],
            "Analysis": [
              "Less is known about why one setup is more effective than another"
            ],
            "Challenge": [
              "Understanding why one setup is more effective than another"
            ],
            "Ethical": [
              "No explicit ethical concerns mentioned"
            ],
            "Implications": [
              "Broader significance for AI-generated content enhancement"
            ],
            "Contradictions": [
              "No explicit contradictions mentioned"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we uncover the underlying characteristics of effective sets. For each training instance, we form a large, diverse pool of candidates and systematically vary the subsets used for calibration fine-tuning. Each selection strategy targets distinct aspects of the sets, such as lexical diversity or the size of the gap between positive and negatives.",
          "Main Action": "form a large, diverse pool of candidates and systematically vary the subsets used for calibration fine-tuning",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "training instances"
              ],
              "Secondary Object": [
                "subsets"
              ]
            },
            "Context": [
              "understanding the underlying characteristics of effective sets"
            ],
            "Purpose": [
              "exploring how different aspects of sets affect performance"
            ],
            "Method": [
              "using varied subsets for calibration fine-tuning"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "On three diverse scientific long-form summarization datasets (spanning biomedical, clinical, and chemical domains), we find, among others, that faithfulness calibration is optimal when the negative sets are extractive and more likely to be generated, whereas for relevance calibration, the metric margin between candidates should be maximized and surprise–the disagreement between model and metric defined candidate rankings–minimized.",
          "Main Action": "We find",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "three diverse scientific long-form summarization datasets"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "spanning biomedical, clinical, and chemical domains"
            ],
            "Purpose": [
              "to investigate how different calibration settings affect performance"
            ],
            "Method": [
              "comparing different calibration strategies (faithful vs relevant)",
              "measuring factors like margins and surprises"
            ],
            "Results": [
              "faithfulness calibration is optimal when the negative sets are extractive and more likely to be generated",
              "for relevance calibration, the metric margin between candidates should be maximized and surprise–the disagreement between model and metric defined candidate rankings–minimized"
            ],
            "Analysis": [
              "they interpret these results to suggest trade-offs between accuracy and diversity in models"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "These findings have broader impacts on NLP tasks relying on such calibrations, guiding future model design"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_299",
      "abstract": "The personalized dialogue explores the consistent relationship between dialogue generation and personality. Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories. However, sparse structured persona attributes are explicit but uninformative, dense persona texts contain rich persona descriptions with much noise, and dialogue history query is both noisy and uninformative for persona modeling. In this work, we combine the advantages of the three resources to obtain a richer and more accurate persona. We design a Contrastive Latent Variable-based model (CLV) that clusters the dense persona descriptions into sparse categories, which are combined with the history query to generate personalized responses. Experimental results on Chinese and English datasets demonstrate our model’s superiority in personalization.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "The personalized dialogue explores the consistent relationship between dialogue generation and personality. Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories. However, sparse structured persona attributes are explicit but uninformative, dense persona texts contain rich persona descriptions with much noise, and dialogue history query is both noisy and uninformative for persona modeling.",
          "Main Action": "Exploring the consistent relationship between dialogue generation and personality",
          "Arguments": {
            "Agent": [
              "Personalized dialogue"
            ],
            "Object": {
              "Primary Object": [
                "Dialogue generation and personality"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories"
            ],
            "Purpose": [
              "To address the limitations of existing methods"
            ],
            "Method": [
              "Trade-offs between different types of personas"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Sparse structured persona attributes are explicit but uninformative, dense persona texts contain rich persona descriptions with much noise, and dialogue history query is both noisy and uninformative for persona modeling"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Addressing these gaps will lead to improved personalization in dialogue systems"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this work, we combine the advantages of the three resources to obtain a richer and more accurate persona. We design a Contrastive Latent Variable-based model (CLV) that clusters the dense persona descriptions into sparse categories, which are combined with the history query to generate personalized responses.",
          "Main Action": "obtain",
          "Arguments": {
            "Agent": [
              "we"
            ],
            "Object": {
              "Primary Object": [
                "a richer and more accurate persona"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "three resources"
            ],
            "Purpose": [
              "enhancing the quality of personas"
            ],
            "Method": [
              "designing a Contrastive Latent Variable-based model (CLV)"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "improved personalization in AI responses"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experimental results on Chinese and English datasets demonstrate our model’s superiority in personalization.",
          "Main Action": "demonstrate",
          "Arguments": {
            "Agent": [
              "our model"
            ],
            "Object": {
              "Primary Object": [
                "Chinese and English datasets"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "Experimental results on Chinese and English datasets demonstrate our model’s superiority in personalization."
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}