{
  "papers": [
    {
      "paper_code": "ACL_23_P_554",
      "abstract": "Reasoning has been a central topic in artificial intelligence from the beginning. The recent progress made on distributed representation and neural networks continues to improve the state-of-the-art performance of natural language inference. However, it remains an open question whether the models perform real reasoning to reach their conclusions or rely on spurious correlations. Adversarial attacks have proven to be an important tool to help evaluate the Achilles’ heel of the victim models. In this study, we explore the fundamental problem of developing attack models based on logic formalism. We propose NatLogAttack to perform systematic attacks centring around natural logic, a classical logic formalism that is traceable back to Aristotle’s syllogism and has been closely developed for natural language inference. The proposed framework renders both label-preserving and label-flipping attacks. We show that compared to the existing attack models, NatLogAttack generates better adversarial examples with fewer visits to the victim models. The victim models are found to be more vulnerable under the label-flipping setting. NatLogAttack provides a tool to probe the existing and future NLI models’ capacity from a key viewpoint and we hope more logic-based attacks will be further explored for understanding the desired property of reasoning.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Reasoning has been a central topic in artificial intelligence from the beginning. The recent progress made on distributed representation and neural networks continues to improve the state-of-the-art performance of natural language inference. However, it remains an open question whether the models perform real reasoning to reach their conclusions or rely on spurious correlations. Adversarial attacks have proven to be an important tool to help evaluate the Achilles’ heel of the victim models.",
          "Main Action": "Adversarial attacks have proven to be an important tool to help evaluate",
          "Arguments": {
            "Agent": [
              "researchers"
            ],
            "Object": {
              "Primary Object": [
                "victim models"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "it remains an open question whether the models perform real reasoning to reach their conclusions or rely on spurious correlations"
            ],
            "Purpose": [
              "to assess the models' true capabilities vs. relying on luck"
            ],
            "Method": [
              "creating adversarial examples and testing how well the models perform under such conditions"
            ],
            "Results": [
              "adversarial attacks reveal vulnerabilities"
            ],
            "Analysis": [
              "interpreting these results suggests limitations in current models"
            ],
            "Challenge": [
              "complexity of generating effective adversarial inputs"
            ],
            "Ethical": [
              "ensuring fair comparisons"
            ],
            "Implications": [
              "better model design to avoid these issues"
            ],
            "Contradictions": [
              "previous assumptions about model reliability"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "In this study, we explore the fundamental problem of developing attack models based on logic formalism. We propose NatLogAttack to perform systematic attacks centring around natural logic, a classical logic formalism that is traceable back to Aristotle’s syllogism and has been closely developed for natural language inference. The proposed framework renders both label-preserving and label-flipping attacks.",
          "Main Action": "Proposing NatLogAttack",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "attack models"
              ],
              "Secondary Object": [
                "natural logic"
              ]
            },
            "Context": [
              "logic formalism",
              "Aristotle’s syllogism",
              "natural language inference"
            ],
            "Purpose": [
              "address the fundamental problem of developing attack models"
            ],
            "Method": [
              "NatLogAttack framework"
            ],
            "Results": [
              "label-preserving and label-flipping attacks"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "improving natural language processing tasks"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "We show that compared to the existing attack models, NatLogAttack generates better adversarial examples with fewer visits to the victim models. The victim models are found to be more vulnerable under the label-flipping setting.",
          "Main Action": "generates better adversarial examples",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "NatLogAttack"
              ],
              "Secondary Object": [
                "existing attack models"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "To compare and evaluate model performance"
            ],
            "Method": [
              "fewer visits to the victim models"
            ],
            "Results": [
              "Improved performance due to better adversarial examples"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "NatLogAttack provides a tool to probe the existing and future NLI models’ capacity from a key viewpoint and we hope more logic-based attacks will be further explored for understanding the desired property of reasoning.",
          "Main Action": "probes",
          "Arguments": {
            "Agent": [
              "NatLogAttack"
            ],
            "Object": {
              "Primary Object": [
                "tool"
              ],
              "Secondary Object": [
                "NLI models"
              ]
            },
            "Context": [
              "existing and future NLI models’ capacity from a key viewpoint"
            ],
            "Purpose": [
              "understanding the desired property of reasoning"
            ],
            "Method": [
              "using the tool to probe"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "further exploring logic-based attacks"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_606",
      "abstract": "Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses joint class-spread contrastive learning to extract structured representations. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training (CAT) strategy to learn more diverse features from context and enhance the model’s context robustness. Under the framework with CAT, we develop a sequence-based SACL-LSTM to learn label-consistent and context-robust features for ERC. Experiments on three datasets show that SACL-LSTM achieves state-of-the-art performance on ERC. Extended experiments prove the effectiveness of SACL and CAT.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC).",
          "Main Action": "<NONE>",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "Generalized and robust representations"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Emotion recognition in conversations (ERC)"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "<NONE>"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Extracting generalized and robust representations is a major challenge"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses joint class-spread contrastive learning to extract structured representations. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training (CAT) strategy to learn more diverse features from context and enhance the model’s context robustness. Under the framework with CAT, we develop a sequence-based SACL-LSTM to learn label-consistent and context-robust features for ERC.",
          "Main Action": "Propose a supervised adversarial contrastive learning (SACL) framework",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "Supervised adversarial contrastive learning (SACL) framework"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "context-dependent data",
              "label-level feature consistency"
            ],
            "Purpose": [
              "Avoiding the negative impact of adversarial perturbations on context-dependent data"
            ],
            "Method": [
              "contrast-aware adversarial training",
              "joint class-spread contrastive learning",
              "contextual adversarial training (CAT)",
              "sequence-based SACL-LSTM"
            ],
            "Results": [
              "generate worst-case samples",
              "extract structured representations",
              "learn label-consistent and context-robust features"
            ],
            "Analysis": [
              "utilizing label-level feature consistency",
              "retaining fine-grained intra-class features"
            ],
            "Challenge": [
              "Adversarial perturbations negatively impact context-dependent data"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "enhancing model's context robustness",
              "broader significance for future applications"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experiments on three datasets show that SACL-LSTM achieves state-of-the-art performance on ERC. Extended experiments prove the effectiveness of SACL and CAT.",
          "Main Action": "Experiments on three datasets show",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "three datasets",
                "effectiveness of SACL and CAT"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "SACL-LSTM model",
              "CAT framework"
            ],
            "Results": [
              "state-of-the-art performance on ERC",
              "proven effectiveness"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}