{
  "papers": [
    {
      "paper_code": "ACL_23_P_780",
      "abstract": "Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks’ theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset.",
      "events": [
        {
          "Background/Introduction": "ERROR",
          "Text": "Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision?",
          "Main Action": "ERROR",
          "Arguments": {
            "Agent": [
              "ERROR"
            ],
            "Object": {
              "Primary Object": [
                "ERROR"
              ],
              "Secondary Object": [
                "ERROR"
              ]
            },
            "Context": [
              "ERROR"
            ],
            "Purpose": [
              "ERROR"
            ],
            "Method": [
              "ERROR"
            ],
            "Results": [
              "ERROR"
            ],
            "Analysis": [
              "ERROR"
            ],
            "Challenge": [
              "ERROR"
            ],
            "Ethical": [
              "ERROR"
            ],
            "Implications": [
              "ERROR"
            ],
            "Contradictions": [
              "ERROR"
            ]
          },
          "Error_Type": "NO_JSON_FOUND"
        },
        {
          "Methods/Approach": "",
          "Text": "We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches.",
          "Main Action": "present",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "SymbolicToM"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation"
            ],
            "Purpose": [
              "allowing for more precise and interpretable reasoning than previous approaches"
            ],
            "Method": [
              "tracking each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations"
            ],
            "Results": [
              "more precise and interpretable reasoning"
            ],
            "Analysis": [
              "interpretable reasoning"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "potential for future applications/research"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks’ theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines.",
          "Main Action": "Empirical results...",
          "Arguments": {
            "Agent": [
              "SymbolicToM"
            ],
            "Object": {
              "Primary Object": [
                "off-the-shelf neural networks' theory of mind"
              ],
              "Secondary Object": [
                "zero-shot setting"
              ]
            },
            "Context": [
              "well-known ToMi benchmark (Le et al., 2019)"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "comparison to supervised baselines"
            ],
            "Results": [
              "showing robust out-of-distribution performance",
              "while demonstrating significant enhancements"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "<NONE>"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Conclusions/Implications": "",
          "Text": "Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset",
          "Main Action": "reveals",
          "Arguments": {
            "Agent": [
              "Our work"
            ],
            "Object": {
              "Primary Object": [
                "Spurious patterns"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "Existing theory of mind benchmarks"
            ],
            "Purpose": [
              "Highlighting issues with current benchmarks"
            ],
            "Method": [
              "Out-of-distribution evaluation",
              "Methods that do not overfit a particular dataset"
            ],
            "Results": [
              "Emphasis on OOD evaluation leads to seeing its necessity"
            ],
            "Analysis": [
              "Patterns suggest problems needing better evaluation methods"
            ],
            "Challenge": [
              "Over-reliance on specific datasets may cause issues"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Broader impacts on how models are tested"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    },
    {
      "paper_code": "ACL_23_P_88",
      "abstract": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples, exhibiting better performance than the existing definition generation method.",
      "events": [
        {
          "Background/Introduction": "",
          "Text": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words.",
          "Main Action": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depict the correct sense of the target word for the given context.",
          "Arguments": {
            "Agent": [
              "Researchers"
            ],
            "Object": {
              "Primary Object": [
                "target word"
              ],
              "Secondary Object": [
                "image"
              ]
            },
            "Context": [
              "Previously, image-text matching models often suffered from recognizing polysemous words."
            ],
            "Purpose": [
              "To improve image-text matching models by correctly interpreting polysemous words."
            ],
            "Method": [
              "Traditional methods were insufficient; new approaches needed."
            ],
            "Results": [
              "Current methods struggled with accuracy due to polysemy."
            ],
            "Analysis": [
              "Previous methods lacked ability to handle word meanings properly."
            ],
            "Challenge": [
              "Difficulty in mapping words to correct images due to polysemy."
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Better AI systems possible through improved disambiguation."
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Methods/Approach": "",
          "Text": "This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3.",
          "Main Action": "Introduces",
          "Arguments": {
            "Agent": [
              "We"
            ],
            "Object": {
              "Primary Object": [
                "Unsupervised VWSD approach"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "External lexical knowledge-base",
              "Sense definitions"
            ],
            "Purpose": [
              "To address the out-of-dictionary (OOD) issue"
            ],
            "Method": [
              "Bayesian inference",
              "Context-aware definition generation with GPT-3"
            ],
            "Results": [
              "<NONE>"
            ],
            "Analysis": [
              "<NONE>"
            ],
            "Challenge": [
              "Out-of-dictionary (OOD) issue"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Broadening NLP tasks' scope",
              "Improving handling of undefined terms"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        },
        {
          "Results/Findings": "",
          "Text": "Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples, exhibiting better performance than the existing definition generation method.",
          "Main Action": "Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach",
          "Arguments": {
            "Agent": [
              "<NONE>"
            ],
            "Object": {
              "Primary Object": [
                "VWSD performance"
              ],
              "Secondary Object": [
                "<NONE>"
              ]
            },
            "Context": [
              "<NONE>"
            ],
            "Purpose": [
              "<NONE>"
            ],
            "Method": [
              "Bayesian inference-based approach"
            ],
            "Results": [
              "Significant performance improvement",
              "Better performance than the existing definition generation method"
            ],
            "Analysis": [
              "The performance gains demonstrate notable advancements in the field",
              "The ability to perform well across diverse domains highlights the robustness of the approach"
            ],
            "Challenge": [
              "<NONE>"
            ],
            "Ethical": [
              "<NONE>"
            ],
            "Implications": [
              "Potential for future applications in various domains due to its broad applicability"
            ],
            "Contradictions": [
              "<NONE>"
            ]
          }
        }
      ]
    }
  ]
}